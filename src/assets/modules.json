{
  "path": "/modules",
  "name": "Modules",
  "description": "All operations are performed using dedicated modules. Typically, a module will perform one specific task, such as loading an image or measuring the shapes of objects.",
  "slug": "modules",
  "subCategories": [
    {
      "path": "/modules/core",
      "name": "Core",
      "description": "Required modules for input and output control.  Each workflow contains one copy of each module.",
      "slug": "core",
      "subCategories": [],
      "modules": [
        {
          "path": "/modules/core/input-control",
          "deprecated": false,
          "name": "Input control",
          "shortDescription": "Select which file(s) or folder(s) MIA will process through.",
          "fullDescription": "Select which file(s) or folder(s) MIA will process through.  If a file is selected, that file alone will be processed; however, selecting a folder will cause the system to iterate over all files and sub-folders within that folder.  Each file identified here will initialise its own workspace.  <br><br>It is possible to add filters to limit which files are used.  Multiple filters can be applied.<br><br>n.b. This module simply creates the workspace for subsequent analysis; no images are automatically loaded at this point.  To load image data to the workspace use the \"Load image\" module.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input path",
              "description": "The file or folder path to process.  If a file is selected, that file alone will be processed.  If a folder is selected, each file in that folder (and all sub-folders) passing the filters will be processed."
            },
            {
              "name": "Series mode",
              "description": "For multi-series files, select which series to process:<br><ul><li>\"All series\" will create a new workspace for each series in the file.<\/li><li>\"Series list\" allows a comma-separated list of series numbers to be specified.<\/li><\/ul>"
            },
            {
              "name": "Series list",
              "description": "Comma-separated list of series numbers to be processed."
            },
            {
              "name": "Only load first file per folder",
              "description": "Only load the (alphabetically) first file in each folder."
            },
            {
              "name": "Only load first matching group",
              "description": "Only load the (alphabetically) first file matching the group specified by the regular expression \"Pattern\" parameter.  Each candidate filename will be matched against the regular expression in \"Pattern\" and retained for analysis if the selected group hasn't been seen before.  Only one group (specified using standard regex parenthesis notation) can be used."
            },
            {
              "name": "Pattern",
              "description": "Regular expression pattern to use when \"Only load first matching group\" is enabled.  This pattern must contain at least one group (specified using standard regex parenthesis notation)."
            },
            {
              "name": "Spatial unit",
              "description": "Spatial units for calibrated measurements.  Assuming spatial calibration can be read from the input file when loaded, this will convert the input calibrated units to the units specified here."
            },
            {
              "name": "Temporal unit",
              "description": "Temporal units for calibrated measurements.  Assuming temporal calibration can be read from the input file when loaded, this will convert the input calibrated units to the units specified here."
            },
            {
              "name": "Ignore case",
              "description": ""
            },
            {
              "name": "Add filter",
              "subParameters": [
                {
                  "name": "Filter source",
                  "description": "Type of filter to add."
                },
                {
                  "name": "Filter value",
                  "description": "Value to filter filenames against."
                },
                {
                  "name": "Filter type",
                  "description": "Control how the present filter operates:<br><ul><li>\"Matches partially (include)\" will process an image if the filter value is partially present in the source (e.g. filename or extension).<\/li><li>\"Matches completely (include)\" will process an image if the filter value is exactly the same as the source.<\/li><li>\"Matches partially (exclude)\" will not process an image if the filter value is partially present in the source.<\/li><li>\"Matches completely (exclude)\" will not process an image if the filter value is exactly the same as the source.<\/li><\/ul>"
                }
              ],
              "description": "Add another filename filter.  All images to be processed will pass all filters."
            },
            {
              "name": "Simultaneous jobs",
              "description": "The number of images that will be processed simultaneously.  If this is set to \"1\" while processing a folder each valid file will still be processed, they will just complete one at a time.  For large images this is best left as \"1\" unless using a system with large amounts of RAM."
            }
          ],
          "slug": "input-control"
        },
        {
          "path": "/modules/core/output-control",
          "deprecated": false,
          "name": "Output control",
          "shortDescription": "Controls data export for each analysis run (job) as well as providing the option to run a final macro on all data.",
          "fullDescription": "Controls data export for each analysis run (job) as well as providing the option to run a final macro on all data.  If running a single file analysis, by default, the spreadsheet will be saved with the same name as the input file and stored at the same location.  Whereas, in batch mode (running multiple files from a folder) the spreadsheet will be saved in that folder with the same name as the input folder.  Using the \"Group save location\" parameter it's possible to redirect the output spreadsheet(s) to a specific location.<br><br>Data can be collated into a single Excel spreadsheet, or exported as one spreadsheet per analysis run (job).  Exported spreadsheets are separated into multiple sheets:<br><ul><li>\"Parameters\" An overview of the analysis setup (path to workflow file, date run, computer operating system) along with a list of all modules and parameters, their values and states.  The information here should be sufficient to reconstruct the analysis workflow in the absence of the original workflow file.<\/li><li>\"Log\" A list of any error messages presented to the user while the analysis was running.<\/li><li>\"Summary\" Each analysis run is summarised by a single line (or one line per timepoint or metadata value, if selected) containing metadata values, image measurements, the number of objects detected as well as statistics for object collections (e.g. mean of a particular measurement).  The summary sheet is intended to facilitate quick analysis; all data contained in this sheet (with the exception of image measurements) can be manually compiled from the individual object sheets.  This sheet can be enabled/disabled using the \"Export summary\" parameter.<\/li><li>\"[Object-specific sheets]\" Each object collection can export to a separate sheet.  These sheets contain one row per object in that collection and include metadata values, along with all measurements and relationships for that object.  This sheet can be enabled/disabled using the \"Export individual objects\" parameter.<\/li><\/ul><br>It's also possible to select the data to be exported for each sheet, including the metadata values (filename, series number, etc.), individual measurements and object collection statistics.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Add variable",
              "subParameters": [
                {
                  "name": "Variable type",
                  "description": "Controls the data type of the variable that will be assigned within the macro:<br><ul><li>\"Boolean\" Variable will be assigned a true/false value depending on whether the checkbox was selected/deselected (respectively).<\/li>.<li>\"Number\" Variable will be assigned a numeric value to which mathematical operations can be applied.<\/li>.<li>\"Text\" Variable will be assigned a text value.  Irrespective of whether the value contains only numeric characters this will be interpreted as text.<\/li><\/ul>."
                },
                {
                  "name": "Variable name",
                  "description": "The variable value can be accessed from within the macro by using this variable name."
                },
                {
                  "name": "Variable checkbox",
                  "description": "Boolean (true/false) value assigned to this variable if \"Variable type\" is set to \"Boolean\"."
                },
                {
                  "name": "Variable numeric value",
                  "description": "Numeric value assigned to this variable if \"Variable type\" is set to \"Number\"."
                },
                {
                  "name": "Variable text value",
                  "description": "Text value assigned to this variable if \"Variable type\" is set to \"Text\"."
                }
              ],
              "description": "Pre-define variables, which will be immediately accessible within the macro.  These can be used to provide user-controllable values to file-based macros or to prevent the need for editing macro code via the \"Output control\" panel."
            },
            {
              "name": "Run macro",
              "description": "When selected, a final macro can be run once all the analysis runs (jobs) have been completed.  By using the workspace handling macros (\"MIA_GetListOfWorkspaceIDs\" and \"MIA_SetActiveWorkspace\") it's possible to switch between workspaces, thus facilitating dataset-wide analyses.  This macro will be executed only once as part of the final data exporting phase of the analysis."
            },
            {
              "name": "Macro mode",
              "description": "Select the source for the macro code:<br><ul><li>\"Macro file\" Load the macro from the file specified by the \"Macro file\" parameter.<\/li><li>\"Macro text\" Macro code is written directly into the \"Macro text\" box.<\/li><\/ul>"
            },
            {
              "name": "Macro text",
              "description": "Macro code to be executed.  MIA macro commands are enabled using the \"run(\"Enable MIA Extensions\");\" command which is included by default.  This should always be the first line of a macro if these commands are needed."
            },
            {
              "name": "Macro file",
              "description": "Select a macro file (.ijm) to run once, after all analysis runs have completed."
            },
            {
              "name": "Export mode",
              "description": "Controls the number of results spreadsheets that are exported and what they contain:<br><ul><li>\"All together\" Results from all current analysis runs are grouped into a single spreadsheet.  The same sheets are used for all runs, so it's necessary to include a filename (and where necessary, a series name/number) metadata identifier.  Unless \"Save name mode\" is set to \"Specific name\", this file is named after the input file/folder.<\/li><li>\"Group by metadata\" Results are grouped and exported by a specific metadata item associated with each analysis run.  Unless \"Save name mode\" is set to \"Specific name\", the files are named in the format \"[input file/folder name]_[metadata name]-[metadata value]\".  The metadata item to group on is specified by the \"Metadata item for grouping\" parameter.<\/li><li>\"Individual files\" A separate results spreadsheet is saved for each analysis run.<\/li><li>\"None\" No results spreadsheets are exported.<\/li><\/ul>"
            },
            {
              "name": "Individual save location",
              "description": "If \"Export mode\" is set to \"Individual files\" this parameter controls where the individual results files are saved:<br><ul><li>\"Mirrored directory\" The files are saved in a mirrored directory structure.  This structure has the same folder layout as all subfolders of the specified input folder.  The root location of the mirrored structure is specified by the \"Mirrored directory root\" parameter.<\/li><li>\"Save with input file\" The files are saved in the same folder as the input file.  If a folder was selected as the input, the result files are saved directly inside that folder.<\/li><li>\"Specific location\" The files are all saved in a single folder specified by the \"File path\" parameter.<\/li><\/ul>"
            },
            {
              "name": "Group save location",
              "description": "If \"Export mode\" is set to \"Group by metadata\" this parameter controls where the grouped (by metadata) results files are saved:<br><ul><li>\"Save with input file\" The files are saved in the same folder as the input file.  If a folder was selected as the input, the result files are saved directly inside that folder.<\/li><li>\"Specific location\" The files are all saved in a single folder specified by the \"File path\" parameter.<\/li><\/ul>"
            },
            {
              "name": "File path",
              "description": "The path to the folder where results will be saved if using a specific folder path."
            },
            {
              "name": "Mirrored directory root",
              "description": "If using a mirrored directory structure for the results files, this parameter specifies the output structure root.  Subfolders will be created within this root folder that have identical structure to the subfolders of the input folder."
            },
            {
              "name": "Save name mode",
              "description": "Controls how the output results filename is generated:<br><ul><li>\"Match input file/folder name\" Results files are stored with the same name as the input file/folder (depending on the \"Export mode\" parameter).<\/li><li>\"Specific name\" Results files are stored with a specific name, specified by \"File name\".<\/li><\/ul>"
            },
            {
              "name": "File name",
              "description": "Name to save the results file with if saving results files with a specific name (\"Save name mode\" parameter)."
            },
            {
              "name": "Metadata item for grouping",
              "description": "If \"Export mode\" is set to \"Group by metadata\", results will be grouped and saved by the value of this metadata item associated with each analysis run.  There will be one results spreadsheet for each unique value of this metadata item."
            },
            {
              "name": "Continuous data export",
              "description": "When selected, the results spreadsheet(s) can be exported at intervals during a multi-analysis run.  They will be exported every N runs, where N is controlled by the \"Save every n files\" parameter.  The spreadsheet(s) will still be stored when all analysis runs have completed."
            },
            {
              "name": "Save every n files",
              "description": "If \"Continuous data export\" is enabled, the current version of the spreadsheet will be exported after every N analysis runs.  This means if the analysis fails or the computer crashes, results collected so far are not lost."
            },
            {
              "name": "Append date/time mode",
              "description": "Controls under what conditions the time and date will be appended on to the end of the results file filename.  This can be used to prevent accidental over-writing of results files from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
            },
            {
              "name": "Export summary",
              "description": "When selected, a summary sheet will be added to the results spreadsheet.  The summary sheet contains either (1) one row per input image file, (2) one row per timepoint per image file or (3) one row per metadata item.  The export mode is controlled by the \"Summary mode\" parameter.  This sheet is given the name \"SUMMARY\" and contains statistics for measurements (mean, min, max, standard deviation and sum), object counts and metadata items."
            },
            {
              "name": "Summary mode",
              "description": "Controls the form of the summary sheet (if \"Export summary\" is selected):<br><ul><li>\"Per timepoint per input file\" Each timepoint of each input image file is summarised in a separate row.<\/li><li>\"Group by metadata\" All files matching a specific metadata value are averaged and summarised in a separate row (i.e. one row per unique metadata item).<\/li><li>\"Per input file\" Each input image file is summarised in a separate row.<\/li><\/ul>"
            },
            {
              "name": "Metadata item for summary",
              "description": "The metadata item to group the rows of the summary sheet by if \"Summary mode\" is set to \"Group by metadata\"."
            },
            {
              "name": "Show object counts",
              "description": "When selected, the \"Summary\" results sheet displays columns reporting the number of objects per object collection."
            },
            {
              "name": "Export individual objects",
              "description": "When selected, individual results sheets will be created for each object collection.  In these sheets, each object in that collection is summarised per row.  The individual object sheets have the same name as their corresponding object collection."
            }
          ],
          "slug": "output-control"
        }
      ]
    },
    {
      "path": "/modules/images",
      "name": "Images",
      "description": "Modules dealing primarily with images.  These operations typically take images as inputs and may output new images or update existing ones.",
      "slug": "images",
      "subCategories": [
        {
          "path": "/modules/images/configure",
          "name": "Configure",
          "description": "Non-pixel operations such as setting display ranges or spatial calibrations.",
          "slug": "configure",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/images/configure/set-intensity-display-range",
              "deprecated": false,
              "name": "Set intensity display range",
              "shortDescription": "Set the minimum and maximum displayed intensities for a specified image from the workspace.",
              "fullDescription": "Set the minimum and maximum displayed intensities for a specified image from the workspace.  Any pixels with intensities outside the set displayed range will be rendered with the corresponding extreme value (i.e. any pixels with intensities less than the minimum display value will be shown with the same as the value at the minimum display value).  Display ranges can be calculated automatically or specified manually.  One or both extrema can be set at a time.<br><br>Note: Unlike the \"Normalise intensity\" module, pixel values are unchanged by this module.  The only change is to the way ImageJ/Fiji renders the image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace for which the updated intensity display range will be calculated."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if the new intensity display range should be applied directly to the input image, or if it should be applied to a duplicate image, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "If storing the processed image separately in the workspace, this is the name of the output image."
                },
                {
                  "name": "Calculation mode",
                  "description": "Controls how the display range is calculated:<br><ul><li>\"Fast\" All intensity values in the image are collected in a histogram.  As such, for 8 and 16-bit this is fast to calculate as there are a limited number of bins.  In this instance, the clip fraction corresponds to the fraction of bins.<\/li><li>\"Manual\" The minimum and maximum displayed intensities are manually specified with the \"Minimum range value\" and \"Maximum range value\" parameters.<\/li><li>\"Precise\" All intensity values are ordered by their intensity and the clip fraction corresponds to the fraction of pixels (rather than the fraction of unique intensities as with \"Fast\" mode.  As such, this method is more precise; however, can take a much longer time (especially for large images).<\/li><\/ul>"
                },
                {
                  "name": "Calculation source",
                  "description": "This parameter controls whether the display range (min, max) will be determined from the input image or another image:<br><ul><li>\"External\" The image for which the display range is calculated is different to the image that the final display will be applied to.  For example, this could be a single representative slice or substack.  Using this could significantly reduce run-time for large stacks, especially when \"Calculation mode\" is set to \"Precise\".<\/li><li>\"Internal\" The display range will be determined from the same image or stack onto which the display will be applied.<\/li><\/ul>"
                },
                {
                  "name": "External source",
                  "description": ""
                },
                {
                  "name": "Clipping fraction (min)",
                  "description": "Fraction of unique intensities (\"Precise\") or pixels (\"Fast\") that are clipped when setting the minimum displayed intensity.  Any values below this will be displayed equally with the minimum value of the LUT."
                },
                {
                  "name": "Clipping fraction (max)",
                  "description": "Fraction of unique intensities (\"Precise\") or pixels (\"Fast\") that are clipped when setting the maximum displayed intensity.  Any values above this will be displayed equally with the maximum value of the LUT."
                },
                {
                  "name": "Set minimum value",
                  "description": "When selected, the minimum displayed intensity will be updated.  Otherwise, the value will be unchanged."
                },
                {
                  "name": "Set maximum value",
                  "description": "When selected, the maximum displayed intensity will be updated.  Otherwise, the value will be unchanged."
                },
                {
                  "name": "Minimum range value",
                  "description": "If manually setting the minimum displayed intensity, this is the value that will be applied."
                },
                {
                  "name": "Maximum range value",
                  "description": "If manually setting the maximum displayed intensity, this is the value that will be applied."
                }
              ],
              "slug": "set-intensity-display-range"
            },
            {
              "path": "/modules/images/configure/set-lookup-table",
              "deprecated": false,
              "name": "Set lookup table",
              "shortDescription": "Set look-up table (LUT) for an image or a specific channel of an image.",
              "fullDescription": "Set look-up table (LUT) for an image or a specific channel of an image.  The look-up table determines what colour ImageJ will render each intensity value of an image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to set look-up table for."
                },
                {
                  "name": "Channel mode",
                  "description": "Control if the same look-up table is applied to all channels, or just one:<br><br>- \"All channels\" Apply the same look-up table to all channels of the input image.<br><br>- \"Specific channels\" Only apply the look-up table to the channel specified by the \"Channel\" parameter.  All other channels will remain unaffected.<br>"
                },
                {
                  "name": "Reference image",
                  "description": ""
                },
                {
                  "name": "Channel",
                  "description": "When in \"Specific channels\" mode, this is the channel the look-up table will be applied to.  Channel numbering starts at 1."
                },
                {
                  "name": "Lookup table",
                  "description": "Look-up table to apply to the relevant channels.  Choices are: Grey, Red, Green, Blue, Cyan, Magenta, Yellow, Fire, Ice, Jet, Physics, Spectrum, Thermal, Random."
                },
                {
                  "name": "Display mode",
                  "description": "Controls how the minimum value in the look-up table should be rendered:<br><br>- \"Full range\" Use the full colour range of the look-up table.  This is the default look-up table without modifications.<br><br>- \"Set zero to black\" Uses the standard look-up table, except for the lowest value, which is always set to black.  This is useful for cases where the background will be 0 or NaN and should be rendered as black.<br>"
                }
              ],
              "slug": "set-lookup-table"
            },
            {
              "path": "/modules/images/configure/set-spatial-calibration",
              "deprecated": false,
              "name": "Set spatial calibration",
              "shortDescription": "Update spatial calibration for XY and Z axes based on defined physical and corresponding image (pixel) distances.",
              "fullDescription": "Update spatial calibration for XY and Z axes based on defined physical and corresponding image (pixel) distances.  Both physical and image distances can be drawn from a variety of sources including image and object measurements, fixed values and user-defined values (specified at runtime).  Calibration can be applied to XY and Z axes simultaneously or independently.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to which spatial calibration specified in this module will be applied."
                },
                {
                  "name": "Axis mode",
                  "description": "Controls to which image axes the spatial calibration specified in this module will applied:<br><ul><li>\"XY\" Calibration will be applied to X and Y axes only.  Z axis will retain its existing calibration.<\/li><li>\"XY and Z\" Calibration will be applied equally to XY and Z axes.<\/li><li>\"Z\" Calibration will be applied to Z axis only.  X and Y axes will retain their existing calibrations.<\/li><\/ul>"
                },
                {
                  "name": "Physical distance (PD) source",
                  "description": "Source for physical distance measurement:<br><ul><li>\"First object measurement\" Distance is taken as the measurement (specified by \"Objects measurement (PD)\") for the first object in the collection \"Objects for measurement (PD)\".  The first object is taken since only one value can be used.<\/li><li>\"Fixed value\" Distance is the fixed value specified by the \"Fixed value (PD)\" parameter.<\/li><li>\"GUI selection (text box)\" At runtime, the user is presented with a text entry dialog box, into which they can type the physical distance measurement.<\/li><li>\"Image measurement\" Distance is taken as the measurement (specified by \"Image measurement (PD)\") for the image \"Image for measurement (PD)\".<\/li><\/ul>"
                },
                {
                  "name": "Objects for measurement (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"First object measurement\", this is the object collection from which the first measurement will be taken as the physical distance."
                },
                {
                  "name": "Objects measurement (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"First object measurement\", this is the measurement for the first object in the collection, \"Objects for measurement (PD)\", that willbe used as the physical distance."
                },
                {
                  "name": "Fixed value (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"Fixed value\", this is the fixed value that will be used as the physical distance."
                },
                {
                  "name": "Display image (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"GUI selection (text box)\", this image will be displayed to the user at the same time as the dialog box allowing the physical distance to be specified."
                },
                {
                  "name": "Store distance as measurement (PD)",
                  "description": "When selected (and if \"Physical distance (PD) source\" is set to \"GUI selection (text box)\"), the user-specified physical distance value will be added to the input image as a measurement.  This only applies for GUI-based distance specifications as the distance isn't recorded elsewhere (e.g. as an existing image or object measurement, or as a hard-coded parameter)."
                },
                {
                  "name": "Image for measurement (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"Image measurement\", this is the image from which the measurement will be taken as the physical distance."
                },
                {
                  "name": "Image measurement (PD)",
                  "description": "If \"Physical distance (PD) source\" is set to \"Image measurement\", this is the measurement for the image, \"Image for measurement (PD)\", that willbe used as the physical distance."
                },
                {
                  "name": "Image distance (ID) source",
                  "description": "Source for image (pixel) distance measurement:<br><ul><li>\"First object measurement\" Distance is taken as the measurement (specified by \"Objects measurement (ID)\") for the first object in the collection \"Objects for measurement (ID)\".  The first object is taken since only one value can be used.<\/li><li>\"Fixed value\" Distance is the fixed value specified by the \"Fixed value (ID)\" parameter.<\/li><li>\"GUI selection (text box)\" At runtime, the user is presented with a text entry dialog box, into which they can type the image (pixel) distance measurement.<\/li><li>\"Image measurement\" Distance is taken as the measurement (specified by \"Image measurement (ID)\") for the image \"Image for measurement (ID)\".<\/li><\/ul>"
                },
                {
                  "name": "Objects for measurement (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"First object measurement\", this is the object collection from which the first measurement will be taken as the image (pixel) distance."
                },
                {
                  "name": "Objects measurement (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"First object measurement\", this is the measurement for the first object in the collection, \"Objects for measurement (ID)\", that willbe used as the image (pixel) distance."
                },
                {
                  "name": "Fixed value (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"Fixed value\", this is the fixed value that will be used as the image (pixel) distance."
                },
                {
                  "name": "Display image (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"GUI selection (text box)\", this image will be displayed to the user at the same time as the dialog box allowing the image (pixel) distance to be specified."
                },
                {
                  "name": "Store distance as measurement (ID)",
                  "description": "When selected (and if \"Image distance (ID) source\" is set to \"GUI selection (text box)\"), the user-specified image (pixel) distance value will be added to the input image as a measurement.  This only applies for GUI-based distance specifications as the distance isn't recorded elsewhere (e.g. as an existing image or object measurement, or as a hard-coded parameter)."
                },
                {
                  "name": "Image for measurement (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"Image measurement\", this is the image from which the measurement will be taken as the image (pixel) distance."
                },
                {
                  "name": "Image measurement (ID)",
                  "description": "If \"Image distance (ID) source\" is set to \"Image measurement\", this is the measurement for the image, \"Image for measurement (ID)\", that willbe used as the image (pixel) distance."
                }
              ],
              "slug": "set-spatial-calibration"
            }
          ]
        },
        {
          "path": "/modules/images/measure",
          "name": "Measure",
          "description": "Operations making measurements on images. Measurements are associated with the input images for later use.",
          "slug": "measure",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/images/measure/create-2d-intensity-histogram",
              "deprecated": false,
              "name": "Create 2D intensity histogram",
              "shortDescription": "Creates a 2D intensity histogram for a pair of specified images.",
              "fullDescription": "Creates a 2D intensity histogram for a pair of specified images.  Intensities along the x-axis correspond to the first input image and those along the y-axis to the second input image.  Output histogram is saved to the workspace as an image.  Works for N-dimensional image stacks (must have the same dimensions).  Uses the ImgLib2 implementation ND intensity histograms.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image 1 (x-axis)",
                  "description": "First image from the workspace used in 2D intensity histogram plotting.  Intensities from this image are represented along te output histogram x-axis (columns).  For multidimensional stacks, all pixel values are compiled into a single intensity histogram."
                },
                {
                  "name": "Input image 2 (y-axis)",
                  "description": "Second image from the workspace used in 2D intensity histogram plotting.  Intensities from this image are represented along te output histogram y-axis (rows).  For multidimensional stacks, all pixel values are compiled into a single intensity histogram."
                },
                {
                  "name": "Output image",
                  "description": "Output 2D intensity histogram, which will be saved to the workspace."
                },
                {
                  "name": "Minimum bin (image 1)",
                  "description": "Minimum intensity bin for the image specified by \"Input image 1 (x-axis)\"."
                },
                {
                  "name": "Maximum bin (image 1)",
                  "description": "Maximum intensity bin for the image specified by \"Input image 1 (x-axis)\"."
                },
                {
                  "name": "Number of bins (image 1)",
                  "description": "Number of intensity bins to use for the image specified by \"Input image 1 (x-axis)\"."
                },
                {
                  "name": "Include tail bin (image 1)",
                  "description": "When selected, additional bins at the extremes of the histogram x-axis will be used for intensity values that fall outside the specified intensity range."
                },
                {
                  "name": "Minimum bin (image 2)",
                  "description": "Minimum intensity bin for the image specified by \"Input image 2 (y-axis)\"."
                },
                {
                  "name": "Maximum bin (image 2)",
                  "description": "Maximum intensity bin for the image specified by \"Input image 2 (y-axis)\"."
                },
                {
                  "name": "Number of bins (image 2)",
                  "description": "Number of intensity bins to use for the image specified by \"Input image 2 (y-axis)\"."
                },
                {
                  "name": "Include tail bin (image 2)",
                  "description": "When selected, additional bins at the extremes of the histogram y-axis will be used for intensity values that fall outside the specified intensity range."
                }
              ],
              "slug": "create-2d-intensity-histogram"
            },
            {
              "path": "/modules/images/measure/image-measurement-calculator",
              "deprecated": false,
              "name": "Image measurement calculator",
              "shortDescription": "Perform a mathematical operation on measurements associated with an image.",
              "fullDescription": "Perform a mathematical operation on measurements associated with an image.  The calculation can replace either or both input image measurements with fixed values.  The resulting measurement is associated with the input image as a new measurement.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace to perform the measurement calculation for."
                },
                {
                  "name": "Value mode 1",
                  "description": "Controls how the first value in the calculation is defined:<br><ul><li>\"Fixed\" A single, fixed value defined by \"Fixed value 1\"is used.<\/li><li>\"Measurement\" A measurement associated with the input image and defined by \"Measurement 1\" is used.<\/li><\/ul>"
                },
                {
                  "name": "Fixed value 1",
                  "description": "Fixed value to use in the calculation when \"Value mode 1\" is in \"Fixed\" mode."
                },
                {
                  "name": "Measurement 1",
                  "description": "Measurement associated with the input image to use in the calculation when \"Value mode 1\" is in \"Measurement\" mode."
                },
                {
                  "name": "Value mode 2",
                  "description": "Controls how the second value in the calculation is defined:<br><ul><li>\"Fixed\" A single, fixed value defined by \"Fixed value 2\"is used.<\/li><li>\"Measurement\" A measurement associated with the input image and defined by \"Measurement 2\" is used.<\/li><\/ul>"
                },
                {
                  "name": "Fixed value 2",
                  "description": "Fixed value to use in the calculation when \"Value mode 2\" is in \"Fixed\" mode."
                },
                {
                  "name": "Measurement 2",
                  "description": "Measurement associated with the input image to use in the calculation when \"Value mode 2\" is in \"Measurement\" mode."
                },
                {
                  "name": "Output measurement",
                  "description": "The value resulting from the calculation will be stored as a new measurement with this name.  This output measurement will be associated with the input image"
                },
                {
                  "name": "Calculation mode",
                  "description": "Calculation to perform.  Choices are: Add value 1 and value 2, Divide value 1 by value 2, Multiply value 1 and value 2, Subtract value 2 from value 1."
                }
              ],
              "slug": "image-measurement-calculator"
            },
            {
              "path": "/modules/images/measure/measure-gini-coefficient",
              "deprecated": false,
              "name": "Measure Gini coefficient",
              "shortDescription": "Measure the Gini coefficient for all pixels in the specified image.",
              "fullDescription": "Measure the Gini coefficient for all pixels in the specified image.  The Gini coefficient measures the inequality in intensity of pixels.  A coefficient of 0 indicates perfect intensity homogeneity (all pixels with the same value), while a value of 1 indicates the maximum possible inequality (all pixels are black, except for a single bright pixel).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Use mask",
                  "description": ""
                },
                {
                  "name": "Mask image",
                  "description": ""
                }
              ],
              "slug": "measure-gini-coefficient"
            },
            {
              "path": "/modules/images/measure/measure-greyscale-k-function",
              "deprecated": false,
              "name": "Measure greyscale K-function",
              "shortDescription": "Measure's Ripley's K-function for greyscale images.",
              "fullDescription": "Measure's Ripley's K-function for greyscale images.  This method is re-written from the publication \"Extending Ripley\u2019s K-Function to Quantify Aggregation in 2-D Grayscale Images\" by M. Amgad, et al. (doi: 10.1371/journal.pone.0144404).  Results are output to an Excel spreadsheet, with one file per input image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Save location",
                  "description": ""
                },
                {
                  "name": "Mirrored directory root",
                  "description": ""
                },
                {
                  "name": "File path",
                  "description": ""
                },
                {
                  "name": "File name (generic)",
                  "description": ""
                },
                {
                  "name": "Save name mode",
                  "description": ""
                },
                {
                  "name": "File name",
                  "description": ""
                },
                {
                  "name": "Append series mode",
                  "description": ""
                },
                {
                  "name": "Append date/time mode",
                  "description": ""
                },
                {
                  "name": "Add filename suffix",
                  "description": ""
                },
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Use mask",
                  "description": ""
                },
                {
                  "name": "Mask image",
                  "description": ""
                },
                {
                  "name": "Minimum radius (px)",
                  "description": ""
                },
                {
                  "name": "Maximum radius (px)",
                  "description": ""
                },
                {
                  "name": "Radius increment (px)",
                  "description": ""
                }
              ],
              "slug": "measure-greyscale-k-function"
            },
            {
              "path": "/modules/images/measure/measure-image-colocalisation",
              "deprecated": false,
              "name": "Measure image colocalisation",
              "shortDescription": "Calculates colocalisation of two input images.",
              "fullDescription": "Calculates colocalisation of two input images.  All measurements are associated with the first input image.  Measurements can be restricted to specific region using image or object-based masking.  To measure colocalisation on an object-by-object basis please use the \"Measure object colocalisation\" module.<br><br>All calculations are performed using the <a href=\"https://imagej.net/plugins/coloc-2\">Coloc2 plugin<\/a>.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image 1",
                  "description": "First image for which colocalisation will be calculated.  Measurements will be associated with this image."
                },
                {
                  "name": "Input image 2",
                  "description": "Second image for which colocalisation will be calculated."
                },
                {
                  "name": "Masking mode",
                  "description": "Controls which regions of the image will be evaluated for colocalisation:<br><ul><li>\"Mask using image\" A binary image (specified using \"Mask image\") determines which pixels are evaluated for colocalisation.  The \"Image mask logic\" parameter controls whether the pixels to be evaluated are black (0 intensity) or white (255 intensity).<\/li><li>\"Mask using objects\" An object collection (specified using \"Input objects\") determines which pixels are evaluated for colocalisation.  The \"Object mask logic\" parameter controls whether the pixels to be evaluated are inside or outside the objects.<\/li><li>\"None\" No mask will be applied.  All pixels in the image will be evaluated for colocalisation.<\/li><\/ul>"
                },
                {
                  "name": "Mask image",
                  "description": "If \"Masking mode\" is set to \"Mask using image\", this is the binary image which will control the pixels to be evaluated for colocalisation.  The \"Image mask logic\" parameter controls whether the pixels to be evaluated are black (0 intensity) or white (255 intensity)."
                },
                {
                  "name": "Image mask logic",
                  "description": "Controls whether colocalisation is measured for pixels coincident with black (0 intensity) or white (255 intensity) pixels in the mask image."
                },
                {
                  "name": "Input objects",
                  "description": "If \"Masking mode\" is set to \"Mask using objects\", this is the object collection which will control the pixels to be evaluated for colocalisation.  The \"Object mask logic\" parameter controls whether the pixels to be evaluated are inside or outside the objects."
                },
                {
                  "name": "Object mask logic",
                  "description": "Controls whether colocalisation is measured for pixels inside or outside objects in the masking object collection."
                },
                {
                  "name": "Thresholding mode",
                  "description": "Controls how the thresholds for measurements such as Manders' are set:<br><ul><li>\"Bisection (correlation)\" A faster method to calculate thresholds than the Costes approach.<\/li><li>\"Costes (correlation)\" The \"standard\" method to calculate thresholds for Manders' colocalisation measures.  This approach sets the thresholds for the two input images such that the pixels with intensities lower than their respective thresholds don't have any statistical correlation (i.e. have PCC values less than or equal to 0).  This is based on Costes' 2004 paper (Costes et al., <i>Biophys. J.<\/i> <b>86<\/b> (2004) 3993\u20134003.<\/li><li>\"Image measurements\" Thresholds for each image will be set equal to measurements associated with each object.<\/li><li>\"Manual\" Threshold values are manually set from user-defined values (\"Threshold (C1)\" and \"Threshold (C2)\" parameters).<\/li><li>\"None\" No threshold is set.  In this instance, Manders' metrics will only be calculated above zero intensity rather than both above zero and above the thresholds.  Similarly, Pearson's correlation coefficients will only be calculated for the entire region (after masking) rather than also for above and below the thresholds.<\/li><\/ul>"
                },
                {
                  "name": "Image measurement (C1)",
                  "description": "If \"Thresholding mode\" is set to \"Image measurements\", this is the measurement associated with \"Input image 1\" that will be applied to the first image."
                },
                {
                  "name": "Image measurement (C2)",
                  "description": "If \"Thresholding mode\" is set to \"Image measurements\", this is the measurement associated with \"Input image 2\" that will be applied to the second image."
                },
                {
                  "name": "Threshold (C1)",
                  "description": "If \"Thresholding mode\" is set to \"Manual\", this is the threshold that will be applied to the first image."
                },
                {
                  "name": "Threshold (C2)",
                  "description": "If \"Thresholding mode\" is set to \"Manual\", this is the threshold that will be applied to the second image."
                },
                {
                  "name": "PCC implementation",
                  "description": "Controls whether PCC should be calculated using the classic algorithm or using the Coloc2-default \"fast\" method."
                },
                {
                  "name": "Measure Kendall's Rank Correlation",
                  "description": "When selected, Kendall's rank correlation will be calculated.  This works in a similar manner to Pearson's PCC, except it's calculated on ranked data rather than raw pixel intensities."
                },
                {
                  "name": "Measure Li's ICQ",
                  "description": "When selected, Li's ICQ (intensity correlation quotient) will be calculated.  This measure reports the frequency with which both corresponding pixels for both channels are either both above or both below their respective means.  Values are scaled into the range -0.5 to +0.5, with values below 0 corresponding to anti-correlation and values above 0 indicating correlation."
                },
                {
                  "name": "Measure Manders' Correlation",
                  "description": "When selected, Manders' M1 and M2 coefficients will be calculated.  \"Proportional to the amount of fluorescence of the colocalizing pixels or voxels in each colour channel. You can get more details in Manders et al. Values range from 0 to 1, expressing the fraction of intensity in a channel that is located in pixels where there is above zero (or threshold) intensity in the other colour channel.\" Description taken from <a href=\"https://imagej.net/imaging/colocalization-analysis\">https://imagej.net/imaging/colocalization-analysis<\/a>"
                },
                {
                  "name": "Measure PCC",
                  "description": "When selected, Pearson's Correlation Coefficient (PCC) will be calculated.  \"It is not sensitive to differences in mean signal intensities or range, or a zero offset between the two components. The result is +1 for perfect correlation, 0 for no correlation, and -1 for perfect anti-correlation. Noise makes the value closer to 0 than it should be.\" Description taken from <a href=\"https://imagej.net/imaging/colocalization-analysis\">https://imagej.net/imaging/colocalization-analysis<\/a>"
                },
                {
                  "name": "Measure Spearman's Rank Correlation",
                  "description": "When selected, Spearman's rank correlation will be calculated.  Spearman's rho is calculated in a similar manner to Pearson's PCC, except the image intensities are replaced by their respective rank.  Spearman's correlation works with monotonic relationships.  As with PCC, values are in the range -1 to +1."
                }
              ],
              "slug": "measure-image-colocalisation"
            },
            {
              "path": "/modules/images/measure/measure-image-dimensions",
              "deprecated": false,
              "name": "Measure image dimensions",
              "shortDescription": "Measure dimensions of an image and store the values as measurements associated with that image.",
              "fullDescription": "Measure dimensions of an image and store the values as measurements associated with that image.",
              "version": "1.0.0",
              "parameters": [{
                "name": "Input image",
                "description": "Image to measure dimensions for."
              }],
              "slug": "measure-image-dimensions"
            },
            {
              "path": "/modules/images/measure/measure-image-intensity",
              "deprecated": false,
              "name": "Measure image intensity",
              "shortDescription": "Measure intensity statistics (mean, median, mode, minimum, maximum, sum and standard deviation) for an image in the workspace.",
              "fullDescription": "Measure intensity statistics (mean, median, mode, minimum, maximum, sum and standard deviation) for an image in the workspace.  Measurements are associated with the input image, so can be used later on or exported to the results spreadsheet.",
              "version": "1.0.0",
              "parameters": [{
                "name": "Input image",
                "description": "Image to measure intensity statistics for.  The resulting measurements will be associated with this image for use in subsequent modules."
              }],
              "slug": "measure-image-intensity"
            },
            {
              "path": "/modules/images/measure/measure-image-intensity-orientation",
              "deprecated": false,
              "name": "Measure image intensity orientation",
              "shortDescription": "Calculates the orientation of structures in an image.",
              "fullDescription": "Calculates the orientation of structures in an image.  This module uses the <a href=\"https://imagej.net/plugins/directionality\">Directionality_<\/a> plugin to calculate core measures.  Additional measurements, such as the Alignment Index [1] are also calculated.  All measurements are made for the entire image stack; that is, the individual slice histograms are merged and normalised prior to calculation of all measurements.  For multi-slice stacks, images can first be decomposed into whole-slice objects using the CreateWholeSliceObjects module, then processed on a per-object basis using the MeasureObjectIntensityOrientation module.<br><br>References:<br><ol><li>Sun, M., et al. \"Rapid Quantification of 3D Collagen Fiber Alignment and Fiber Intersection Correlations with High Sensitivity\" <i>PLOS ONE<\/i> (2015), doi: https://doi.org/10.1371/journal.pone.0131814<\/li><\/ol>",
              "version": "1.1.0",
              "parameters": [
                {
                  "name": "Save location",
                  "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
                },
                {
                  "name": "Mirrored directory root",
                  "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
                },
                {
                  "name": "File path",
                  "description": "Path to folder where images will be saved."
                },
                {
                  "name": "File name (generic)",
                  "description": ""
                },
                {
                  "name": "Save name mode",
                  "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
                },
                {
                  "name": "File name",
                  "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
                },
                {
                  "name": "Append series mode",
                  "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
                },
                {
                  "name": "Append date/time mode",
                  "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
                },
                {
                  "name": "Add filename suffix",
                  "description": "A custom suffix to be added to each filename."
                },
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Output orientation map",
                  "description": ""
                },
                {
                  "name": "Orientation map name",
                  "description": ""
                },
                {
                  "name": "Method",
                  "description": ""
                },
                {
                  "name": "Number of bins",
                  "description": ""
                },
                {
                  "name": "Histogram start ()",
                  "description": ""
                },
                {
                  "name": "Histogram end ()",
                  "description": ""
                },
                {
                  "name": "Include bin range in name",
                  "description": ""
                },
                {
                  "name": "Include number of bins in name",
                  "description": ""
                },
                {
                  "name": "Save histogram",
                  "description": ""
                }
              ],
              "slug": "measure-image-intensity-orientation"
            },
            {
              "path": "/modules/images/measure/measure-image-texture",
              "deprecated": false,
              "name": "Measure image texture",
              "shortDescription": "Calculates Haralick's texture features for an image.",
              "fullDescription": "Calculates Haralick's texture features for an image.  Each pixel in the image is compared to a corresponding pixel, a defined offset away (e.g. x-offset = 1, y-offset=0, z-offset=0 to compare to the pixel immediately right of each pixel).  The intensities of the pixel pairs are added to a 2D gray-level co-occurrence matrix (GLCM) from which measures of angular second moment, contrast, correlation and entropy can be calculated.<br><br>Robert M Haralick; K Shanmugam; Its'hak Dinstein, \"Textural Features for Image Classification\" <i>IEEE Transactions on Systems, Man, and Cybernetics. SMC-3<\/i> (1973) <b>6<\/b> 610\u2013621.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace for which texture metrics will be calculated.  Texture measurements will be assigned to this image."
                },
                {
                  "name": "X-offset",
                  "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the x-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                },
                {
                  "name": "Y-offset",
                  "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the y-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                },
                {
                  "name": "Z-offset",
                  "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the z-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                },
                {
                  "name": "Calibrated offset",
                  "description": "When selected, offsets are specified in calibrated units.  Otherwise, offsets are assumed to be in pixel units."
                }
              ],
              "slug": "measure-image-texture"
            }
          ]
        },
        {
          "path": "/modules/images/process",
          "name": "Process",
          "description": "Image processing operations which act on a pixel level, such as image filtering or thresholding.",
          "slug": "process",
          "subCategories": [
            {
              "path": "/modules/images/process/binary",
              "name": "Binary",
              "description": "Image operations which either act binary images.  Binary images are 8-bit with 255-intensity background and 0-intensity objects.",
              "slug": "binary",
              "subCategories": [],
              "modules": [
                {
                  "path": "/modules/images/process/binary/binary-operations-(legacy)",
                  "deprecated": true,
                  "name": "Binary operations (legacy)",
                  "shortDescription": "DEPRECATED: This Module has been superseeded by separate Modules for 2D and 3D binary operations.",
                  "fullDescription": "DEPRECATED: This Module has been superseeded by separate Modules for 2D and 3D binary operations.  It will be removed in a future release.<br><br>Applies stock binary operations to an image in the workspace.  This image must be 8-bit and have the logic black foreground (intensity 0) and white background (intensity 255).  Operations labelled \"2D\" are performed using the stock ImageJ implementations, while those labelled \"3D\" use the MorphoLibJ implementations.  If 2D operations are applied on higher dimensionality images the operations will be performed in a slice-by-slice manner.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply binary operation to.  This must be an 8-bit binary image (255 = background, 0 = foreground)."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Filter mode",
                      "description": "Controls which binary operation will be applied.  All operations assume the default ImageJ logic of black objects on a white background.  The 2D operations are described in full at <a href=\"https://imagej.nih.gov/ij/docs/guide/146-29.html\">https://imagej.nih.gov/ij/docs/guide/146-29.html<\/a>:<br><ul><li>\"Dilate 2D\" Change any foreground-connected background pixels to foreground.  This effectively expands objects by one pixel.  Uses ImageJ implementation.<\/li><li>\"Dilate 3D\" Change any foreground-connected background pixels to foreground.  This effectively expands objects by one pixel.  Uses MorphoLibJ implementation.<\/li><li>\"Distance map 3D\" Create a 32-bit greyscale image where the value of each foreground pixel is equal to its Euclidean distance to the nearest background pixel.  Uses MorphoLibJ implementation.<\/li><li>\"Erode 2D\" Change any background-connected foreground pixels to background.  This effectively shrinks objects by one pixel.  Uses ImageJ implementation.<\/li><li>\"Erode 3D\" Change any background-connected foreground pixels to background.  This effectively shrinks objects by one pixel.  Uses MorphoLibJ implementation.<\/li><li>\"Fill holes 2D\" Change all background pixels in a region which is fully enclosed by foreground pixels to foreground.  Uses ImageJ implementation.<\/li><li>\"Fill holes 3D\" Change all background pixels in a region which is fully enclosed by foreground pixels to foreground.  Uses MorphoLibJ implementation.<\/li><li>\"Outline 2D\" Convert all non-background-connected foreground pixels to background.  This effectively creates a fully-background image, except for the outer band of foreground pixels.  Uses ImageJ implementation.<\/li><li>\"Skeletonise 2D\" Repeatedly applies the erode process until each foreground region is a single pixel wide.  Uses ImageJ implementation.<\/li><li>\"Watershed 2D\" Peforms a distance-based watershed transform on the image.  This process is able to split separate regions of a single connected foreground region as long as the sub-regions are connected by narrow necks (e.g. snowman shape).  Background lines are drawn between each sub-region such that they are no longer connected.  Uses ImageJ implementation.<\/li><li>\"Watershed 3D\" Peforms a watershed transform on the image.  This process is able to split separate regions of a single connected foreground region as long as the sub-regions are connected by narrow necks (e.g. snowman shape).  Background lines are drawn between each sub-region such that they are no longer connected.  Unlike the 2D ImageJ implementation, this version can use specific markers and be run in either distance or intensity-based modes.  Uses MorphoLibJ implementation.<\/li><\/ul>"
                    },
                    {
                      "name": "Number of iterations",
                      "description": "Number of times the operation will be run on a single image.  For example, this allows objects to be eroded further than one pixel in a single step."
                    },
                    {
                      "name": "Use markers",
                      "description": "(3D watershed only) When selected, this option allows the use of markers to define the starting point of each region.  The marker image to use is specified using the \"Input marker image\" parameter.  If not selected, a distance map will be generated for the input binary image and extended minima created according to the dynamic specified by \"Dynamic\"."
                    },
                    {
                      "name": "Input marker image",
                      "description": "(3D watershed only) Marker image to be used if \"Use markers\" is selected.  This image must be of equal dimensions to the input image (to which the transform will be applied).  The image must be 8-bit binary with markers in black (intensity 0) on a white background (intensity 255)."
                    },
                    {
                      "name": "Intensity mode",
                      "description": "(3D watershed only) Controls the source for the intensity image against which the watershed transform will be computed.  Irrespective of mode, the image (raw image or object distance map) will act as a surface that the starting points will evolve up until adjacent regions come into contact (at which point creating a dividing line between the two):<br><ul><li>\"Distance\" A distance map will be created from the input binary image and used as the surface against which the watershed regions will evolve.<\/li><li>\"Input image intensity\" The watershed regions will evolve against an image from the workspace.  This image will be unaffected by this process.  The image should have lower intensity coincident with the markers, rising to higher intensity along the boundaries between regions. <\/li><\/ul>"
                    },
                    {
                      "name": "Intensity image",
                      "description": "(3D watershed only) If \"Intensity mode\" is set to \"Input image intensity\", this is the image from the workspace against which the watershed regions will evolve.  The image should have lower intensity coincident with the markers, rising to higher intensity along the boundaries between regions."
                    },
                    {
                      "name": "Dynamic",
                      "description": "(3D watershed only) If \"Use markers\" is not selected, the initial region markers will be created by generating a distance map for the input binary image and calculating the extended minima.  This parameter specifies the maximum permitted pixel intensity difference for a single marker.  Local intensity differences greater than this will result in creation of more markers.  The smaller the dynamic value is, the more the watershed transform will split the image."
                    },
                    {
                      "name": "Connectivity",
                      "description": "(3D watershed only) Controls which adjacent pixels are considered:<br><ul><li>\"6\" Only pixels immediately next to the active pixel are considered.  These are the pixels on the four \"cardinal\" directions plus the pixels immediately above and below the current pixel.  If working in 2D, 4-way connectivity is used.<\/li><li>\"26\" In addition to the core 6-pixels, all immediately diagonal pixels are used.  If working in 2D, 8-way connectivity is used.<\/li>"
                    },
                    {
                      "name": "Match Z to XY",
                      "description": "When selected, an image is interpolated in Z (so that all pixels are isotropic) prior to calculation of a distance map.  This prevents warping of the distance map along the Z-axis if XY and Z sampling aren't equal."
                    }
                  ],
                  "slug": "binary-operations-(legacy)"
                },
                {
                  "path": "/modules/images/process/binary/binary-operations-2d",
                  "deprecated": false,
                  "name": "Binary operations 2D",
                  "shortDescription": "Applies stock ImageJ binary operations to an image in the workspace.",
                  "fullDescription": "Applies stock ImageJ binary operations to an image in the workspace.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  All operations are performed in 2D, with higher dimensionality stacks being processed slice-by-slice.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply binary operation to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Filter mode",
                      "description": "Controls which binary operation will be applied.  The operations are described in full <a href=\"https://imagej.nih.gov/ij/docs/guide/146-29.html#toc-Subsection-29.8\">here<\/a>:<br><ul><li>\"Dilate\" Change any foreground-connected background pixels to foreground.  This effectively expands objects by one pixel.<\/li><li>\"Distance map\" Create a 32-bit greyscale image where the value of each foreground pixel is equal to its Euclidean distance to the nearest background pixel.<\/li><li>\"Erode\" Change any background-connected foreground pixels to background.  This effectively shrinks objects by one pixel.<\/li><li>\"Fill holes\" Change all background pixels in a region which is fully enclosed by foreground pixels to foreground.<\/li><li>\"Outline\" Convert all non-background-connected foreground pixels to background.  This effectively creates a fully-background image, except for the outer band of foreground pixels.<\/li><li>\"Skeletonise\" Repeatedly applies the erode process until each foreground region is a single pixel wide.<\/li><li>\"Ultimate points\" Repeatedly applies the erode process until each foreground is reduced to a single pixel.  The value of the remaining, isolated foreground pixels are equal to their equivalent, pre-erosion distance map values.  This process outputs a 32-bit greyscale image.<\/li><li>\"Voronoi\" Creates an image subdivided by lines such that all pixels contained within an enclosed region are closest to the same contiguous object in the input binary image.<\/li><li>\"Watershed\" Peforms a distance-based watershed transform on the image.  This process is able to split separate regions of a single connected foreground region as long as the sub-regions are connected by narrow necks (e.g. snowman shape).  Background lines are drawn between each sub-region such that they are no longer connected.<\/li><\/ul>"
                    },
                    {
                      "name": "Number of iterations",
                      "description": "Number of times the operation will be run on a single image.  For example, this allows objects to be eroded further than one pixel in a single step."
                    },
                    {
                      "name": "Count",
                      "description": "The minimum number of connected background or foreground for an erosion or dilation process to occur, respectively."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "binary-operations-2d"
                },
                {
                  "path": "/modules/images/process/binary/calculate-distance-map",
                  "deprecated": false,
                  "name": "Calculate distance map",
                  "shortDescription": "Creates a 32-bit greyscale image from an input binary image, where the value of each foreground pixel in the input image is equal to its Euclidean distance to the nearest background pixel.",
                  "fullDescription": "Creates a 32-bit greyscale image from an input binary image, where the value of each foreground pixel in the input image is equal to its Euclidean distance to the nearest background pixel.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  The output image will have pixel values of 0 coincident with background pixels in the input image and values greater than zero coincident with foreground pixels.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to calculate distance map for.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "The output distance map will be saved to the workspace with this name.  This image will be 32-bit format."
                    },
                    {
                      "name": "Weight modes",
                      "description": "The pre-defined set of weights that are used to compute the 3D distance transform using chamfer approximations of the euclidean metric (descriptions taken from <a href=\"https://ijpb.github.io/MorphoLibJ/javadoc/\">https://ijpb.github.io/MorphoLibJ/javadoc/<\/a>):<br><ul><li>\"Borgefors (3,4,5)\" Use weight values of 3 for orthogonal neighbors, 4 for diagonal neighbors and 5 for cube-diagonals (best approximation for 3-by-3-by-3 masks).<\/li><li>\"Chessboard (1,1,1)\" Use weight values of 1 for all neighbours.<\/li><li>\"City-Block (1,2,3)\" Use weight values of 1 for orthogonal neighbors, 2 for diagonal neighbors and 3 for cube-diagonals.<\/li><li>\"Svensson (3,4,5,7)\" Use weight values of 3 for orthogonal neighbors, 4 for diagonal neighbors, 5 for cube-diagonals and 7 for (2,1,1) shifts. Good approximation using only four weights, and keeping low value of orthogonal weight.<\/li><\/ul>"
                    },
                    {
                      "name": "Match Z to XY",
                      "description": "When selected, an image is interpolated in Z (so that all pixels are isotropic) prior to calculation of the distance map.  This prevents warping of the distance map along the Z-axis if XY and Z sampling aren't equal."
                    },
                    {
                      "name": "Spatial units mode",
                      "description": "Controls whether spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "calculate-distance-map"
                },
                {
                  "path": "/modules/images/process/binary/dilate-and-erode",
                  "deprecated": false,
                  "name": "Dilate and erode",
                  "shortDescription": "Applies binary dilate or erode operations to an image in the workspace.",
                  "fullDescription": "Applies binary dilate or erode operations to an image in the workspace.  Dilate will expand all foreground-labelled regions by a specified number of pixels, while erode will shrink all foreground-labelled regions by the same ammount.<br><br>This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  If 2D operations are applied on higher dimensionality images the operations will be performed in a slice-by-slice manner.  All operations (both 2D and 3D) use the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply dilate or erode operation to."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Filter mode",
                      "description": "Controls what sort of dilate or erode operation is performed on the input image:<br><ul><li>\"Dilate 2D\" Change any foreground-connected background pixels to foreground.  This effectively expands objects by one pixel.  Uses ImageJ implementation.<\/li><li>\"Dilate 3D\" Change any foreground-connected background pixels to foreground.  This effectively expands objects by one pixel.  Uses MorphoLibJ implementation.<\/li><li>\"Erode 2D\" Change any background-connected foreground pixels to background.  This effectively shrinks objects by one pixel.  Uses ImageJ implementation.<\/li><li>\"Erode 3D\" Change any background-connected foreground pixels to background.  This effectively shrinks objects by one pixel.  Uses MorphoLibJ implementation.<\/li><\/ul>"
                    },
                    {
                      "name": "Number of iterations",
                      "description": "Number of times the operation will be run on a single image.  Effectively, this allows objects to be dilated or eroded by a specific number of pixels."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "dilate-and-erode"
                },
                {
                  "path": "/modules/images/process/binary/extended-minimamaxima",
                  "deprecated": false,
                  "name": "Extended minima/maxima",
                  "shortDescription": "Detects extended minima or maxima in a specified input image.",
                  "fullDescription": "Detects extended minima or maxima in a specified input image.<br><br>As described in the <a href=\"https://imagej.net/imagej-wiki-static/MorphoLibJ\">MorphoLibJ documentation<\/a>: \"Extended maxima are defined as a connected region containing elements such that the difference of the value of each element within the region with the maximal value within the region is lower than the tolerance, and such that the neighbors of the regions all have values smaller than the maximum within the region minus the tolerance. This definition allows the identification of larger extrema, that better takes into account the noise within the image. The extended minima are defined in a similar way, and are efficiently used as pre-processing step for watershed segmentation.\".<br><br>This module uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply extended minima operation to."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Minima/maxima",
                      "description": "Controls whether the module will detect minima or maxima in the input intensity image"
                    },
                    {
                      "name": "Dynamic",
                      "description": "This parameter specifies the maximum permitted pixel intensity difference for a single minima.  Local intensity differences greater than this will result in creation of more minima.  The smaller the dynamic value is, the more minima will be created.  As the dynamic value increases, minima will increase in size."
                    },
                    {
                      "name": "Connectivity",
                      "description": "Controls which adjacent pixels are considered:<br><ul><li>\"6\" Only pixels immediately next to the active pixel are considered.  These are the pixels on the four \"cardinal\" directions plus the pixels immediately above and below the current pixel.  If working in 2D, 4-way connectivity is used.<\/li><li>\"26\" In addition to the core 6-pixels, all immediately diagonal pixels are used.  If working in 2D, 8-way connectivity is used.<\/li>"
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    },
                    {
                      "name": "Enable multithreading",
                      "description": "Process multiple 3D stacks simultaneously.  Since the extended minima operation is applied on a single 3D stack at a time, multithreading only works for images with multiple channels or timepoints (other stacks will still work, but won't see a speed improvement).  This can provide a speed improvement when working on a computer with a multi-core CPU."
                    }
                  ],
                  "slug": "extended-minimamaxima"
                },
                {
                  "path": "/modules/images/process/binary/fill-holes",
                  "deprecated": false,
                  "name": "Fill holes",
                  "shortDescription": "Performs a 3D fill holes operation on an input binary image.",
                  "fullDescription": "Performs a 3D fill holes operation on an input binary image.  This operation will change all background pixels in a region which is fully enclosed by foreground pixels to foreground.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply fill holes operation to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "fill-holes"
                },
                {
                  "path": "/modules/images/process/binary/fill-holes-by-volume",
                  "deprecated": false,
                  "name": "Fill holes by volume",
                  "shortDescription": "Performs a volume-limited 3D fill holes operation on an input binary image.",
                  "fullDescription": "Performs a volume-limited 3D fill holes operation on an input binary image.  This operation will change all background pixels in a region which is fully enclosed by foreground pixels to foreground.  The volume of holes to be filled can be restricted with both minimum and maximum permissible holes.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply fill holes operation to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Set minimum volume",
                      "description": "When selected, a minimum permitted volume for binary holes can be set.  Any holes with volumes smaller than this value will be removed."
                    },
                    {
                      "name": "Minimum permitted volume",
                      "description": "If \"Set minimum volume\" is selected, this is the minimum volume each hole must have for it to be retained.  Volumes are specified in units controlled by the \"Calibrated units\" parameter."
                    },
                    {
                      "name": "Set maximum volume",
                      "description": "When selected, a maximum permitted volume for binary holes can be set.  Any holes with volumes larger than this value will be removed."
                    },
                    {
                      "name": "Maximum permitted volume",
                      "description": "If \"Set maximum volume\" is selected, this is the maximum volume each hole must have for it to be retained.  Volumes are specified in units controlled by the \"Calibrated units\" parameter."
                    },
                    {
                      "name": "Calibrated units",
                      "description": "When selected, hole size limits are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                    },
                    {
                      "name": "Connectivity",
                      "description": "Controls which adjacent pixels are considered:<br><ul><li>\"6\" Only pixels immediately next to the active pixel are considered.  These are the pixels on the four \"cardinal\" directions plus the pixels immediately above and below the current pixel.  If working in 2D, 4-way connectivity is used.<\/li><li>\"26\" In addition to the core 6-pixels, all immediately diagonal pixels are used.  If working in 2D, 8-way connectivity is used.<\/li>"
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    },
                    {
                      "name": "Enable multithreading",
                      "description": "Break the image down into strips, each one processed on a separate CPU thread.  The overhead required to do this means it's best for large multi-core CPUs, but should be left disabled for small images or on CPUs with few cores."
                    },
                    {
                      "name": "Minimum strip width (px)",
                      "description": "Minimum width of each strip to be processed on a separate CPU thread.  Measured in pixel units."
                    }
                  ],
                  "slug": "fill-holes-by-volume"
                },
                {
                  "path": "/modules/images/process/binary/fix-skeleton-breaks",
                  "deprecated": false,
                  "name": "Fix skeleton breaks",
                  "shortDescription": "Fixes breaks (gaps) in binary skeleton images.",
                  "fullDescription": "Fixes breaks (gaps) in binary skeleton images.  This considers each end point of the skeleton and tests it against multiple distance and orientation criteria to see if it can be linked to any other ends (or even midpoints) of the skeleton.  The path between linked ends is added to the binary image as a straight line.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image to apply break correction to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "Select if the correction should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                    },
                    {
                      "name": "Output image",
                      "description": "Name of the output image created during processing.  This image will be added to the workspace."
                    },
                    {
                      "name": "Number of end pixels to fit",
                      "description": "Number of pixels at the end of a branch to be used for determination of branch orientation."
                    },
                    {
                      "name": "Maximum linking distance",
                      "description": "Maximum break distance that can be bridged.  Specified in pixels unless \"Calibrated units\" is enabled."
                    },
                    {
                      "name": "Calibrated units",
                      "description": "Select whether \"Maximum linking distance\" should be specified in pixel (false) or calibrated (true) units."
                    },
                    {
                      "name": "Maximum linking angle (degrees)",
                      "description": "Maximum angular deviation of linking region relative to orientation of existing branch end.  Specified in degrees."
                    },
                    {
                      "name": "Only link ends",
                      "description": "Only remove breaks between pixels at branch ends.  When disabled, an end can link into the middle of another branch."
                    },
                    {
                      "name": "Angle weight",
                      "description": "Weight applied to orientation mismatch of ends.  This controls how important orientation mismatches are when considering multiple candidate fixes.  The larger this is, the more likely ends need to be well aligned to be chosen for linking."
                    },
                    {
                      "name": "Distance weight",
                      "description": "Weight applied to distance between candidate ends.  This controls how important minimising the distance between candidate ends is when multiple candidate fixes are available.  The larger than is, the more likely ends will need to be in close proximity to be chosen for linking."
                    },
                    {
                      "name": "End weight",
                      "description": "Weight applied to preference for linking end points. The larger this is, the more likely the points chosen for linking will be ends of the skeleton (rather than mid-points)."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "fix-skeleton-breaks"
                },
                {
                  "path": "/modules/images/process/binary/skeletonise",
                  "deprecated": false,
                  "name": "Skeletonise",
                  "shortDescription": "Creates an skeletonised representation of a specific binary image in the workspace.",
                  "fullDescription": "Creates an skeletonised representation of a specific binary image in the workspace.  The input and output images will be 8-bit with binary logic determined by the \"Binary logic\" parameter.  Each minima will show the lowest local intensity region within a specific dynamic range.  Local variation greater than this dynamic will result in the creation of more minima.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply 3D skeletonisation operation to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "skeletonise"
                },
                {
                  "path": "/modules/images/process/binary/watershed-transform",
                  "deprecated": false,
                  "name": "Watershed transform",
                  "shortDescription": "Peforms a watershed transform on a specified input image.",
                  "fullDescription": "Peforms a watershed transform on a specified input image.  This process is able to split separate regions of a single connected foreground region as long as the sub-regions are connected by narrow necks (e.g. snowman shape).  Background lines are drawn between each sub-region such that they are no longer connected.  This can use specific markers and be run in either distance or intensity-based modes.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from workspace to apply watershed transform to.  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                    },
                    {
                      "name": "Output image",
                      "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                    },
                    {
                      "name": "Use markers",
                      "description": "When selected, this option allows the use of markers to define the starting point of each region.  The marker image to use is specified using the \"Input marker image\" parameter.  If not selected, a distance map will be generated for the input binary image and extended minima created according to the dynamic specified by \"Dynamic\"."
                    },
                    {
                      "name": "Input marker image",
                      "description": "Marker image to be used if \"Use markers\" is selected.  This image must be of equal dimensions to the input image (to which the transform will be applied).  This image will be 8-bit with binary logic determined by the \"Binary logic\" parameter."
                    },
                    {
                      "name": "Intensity mode",
                      "description": "Controls the source for the intensity image against which the watershed transform will be computed.  Irrespective of mode, the image (raw image or object distance map) will act as a surface that the starting points will evolve up until adjacent regions come into contact (at which point creating a dividing line between the two):<br><ul><li>\"Distance\" A distance map will be created from the input binary image and used as the surface against which the watershed regions will evolve.<\/li><li>\"Input image intensity\" The watershed regions will evolve against an image from the workspace.  This image will be unaffected by this process.  The image should have lower intensity coincident with the markers, rising to higher intensity along the boundaries between regions. <\/li><\/ul>"
                    },
                    {
                      "name": "Intensity image",
                      "description": "If \"Intensity mode\" is set to \"Input image intensity\", this is the image from the workspace against which the watershed regions will evolve.  The image should have lower intensity coincident with the markers, rising to higher intensity along the boundaries between regions."
                    },
                    {
                      "name": "Dynamic",
                      "description": "If \"Use markers\" is not selected, the initial region markers will be created by generating a distance map for the input binary image and calculating the extended minima.  This parameter specifies the maximum permitted pixel intensity difference for a single marker.  Local intensity differences greater than this will result in creation of more markers.  The smaller the dynamic value is, the more the watershed transform will split the image."
                    },
                    {
                      "name": "Connectivity",
                      "description": "Controls which adjacent pixels are considered:<br><ul><li>\"6\" Only pixels immediately next to the active pixel are considered.  These are the pixels on the four \"cardinal\" directions plus the pixels immediately above and below the current pixel.  If working in 2D, 4-way connectivity is used.<\/li><li>\"26\" In addition to the core 6-pixels, all immediately diagonal pixels are used.  If working in 2D, 8-way connectivity is used.<\/li>"
                    },
                    {
                      "name": "Match Z to XY",
                      "description": "When selected, an image is interpolated in Z (so that all pixels are isotropic) prior to calculation of a distance map.  This prevents warping of the distance map along the Z-axis if XY and Z sampling aren't equal."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    },
                    {
                      "name": "Enable multithreading",
                      "description": "Process multiple 3D stacks simultaneously.  Since the watershed transform is applied on a single 3D stack at a time, multithreading only works for images with multiple channels or timepoints (other stacks will still work, but won't see a speed improvement).  This can provide a speed improvement when working on a computer with a multi-core CPU."
                    }
                  ],
                  "slug": "watershed-transform"
                }
              ]
            },
            {
              "path": "/modules/images/process/threshold",
              "name": "Threshold",
              "description": "Operations binarising images from the workspace.  Thresholds can be calculated automatically or applied manually. ",
              "slug": "threshold",
              "subCategories": [],
              "modules": [
                {
                  "path": "/modules/images/process/threshold/global-auto-threshold",
                  "deprecated": false,
                  "name": "Global auto-threshold",
                  "shortDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.",
                  "fullDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.  Uses the built-in ImageJ global <a href=\"https://imagej.net/Auto_Threshold\">auto-thresholding algorithms<\/a>.<br><br>Note: Currently only works on 8-bit images.  Images with other bit depths will be automatically converted to 8-bit based on the \"Fill target range (normalise)\" scaling method from the \"Image type converter\" module.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image to apply threshold to."
                    },
                    {
                      "name": "Output mode",
                      "description": "Controls if the threshold is applied to the input image or only calculated and stored as a measurement:<br><ul><li>\"Calculate and apply\" Calculate the threshold and apply it to the input image.  Whether the binarised image updates the input image or is saved as a separate image to the workspace is controlled by the \"Apply to input image\" parameter.  In this mode the calculated threshold is still stored as a measurement of the input image.<\/li><li>\"Calculate only\" Calculate the threshold, but do not apply it to the input image.  The calculated threshold is only stored as a measurement of the input image.<\/li>"
                    },
                    {
                      "name": "Apply to input image",
                      "description": "Select if the threshold should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                    },
                    {
                      "name": "Output image",
                      "description": "Name of the output image created during the thresholding process.  This image will be added to the workspace."
                    },
                    {
                      "name": "Algorithm",
                      "description": "Global thresholding algorithm to use.  Choices are: Huang, Intermodes, IsoData, Li, MaxEntropy, Mean, MinError, Minimum, Moments, Otsu, Percentile, RenyiEntropy, Shanbhag, Triangle, Yen."
                    },
                    {
                      "name": "Threshold multiplier",
                      "description": "Prior to application of automatically-calculated thresholds the threshold value is multiplied by this value.  This allows the threshold to be systematically increased or decreased.  For example, a \"Threshold multiplier\" of 0.9 applied to an automatically-calculated threshold of 200 will see the image thresholded at the level 180."
                    },
                    {
                      "name": "Use lower threshold limit",
                      "description": "Limit the lowest threshold that can be applied to the image.  This is used to prevent unintentional segmentation of an image containing only background (i.e. no features present)."
                    },
                    {
                      "name": "Lower threshold limit",
                      "description": "Lowest absolute threshold value that can be applied."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    },
                    {
                      "name": "Measure on objects",
                      "description": ""
                    },
                    {
                      "name": "Input objects",
                      "description": ""
                    }
                  ],
                  "slug": "global-auto-threshold"
                },
                {
                  "path": "/modules/images/process/threshold/live-manual-threshold",
                  "deprecated": false,
                  "name": "Live manual threshold",
                  "shortDescription": "",
                  "fullDescription": "",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": ""
                    },
                    {
                      "name": "Apply to input image",
                      "description": ""
                    },
                    {
                      "name": "Output image",
                      "description": ""
                    },
                    {
                      "name": "Threshold value",
                      "description": ""
                    },
                    {
                      "name": "Binary logic",
                      "description": ""
                    },
                    {
                      "name": "Live selection",
                      "description": ""
                    }
                  ],
                  "slug": "live-manual-threshold"
                },
                {
                  "path": "/modules/images/process/threshold/local-auto-threshold",
                  "deprecated": false,
                  "name": "Local auto-threshold",
                  "shortDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.",
                  "fullDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.  Uses the built-in ImageJ global and 2D local auto-thresholding algorithms.<br><br>Note: Currently only works on 8-bit images.  Images with other bit depths will be automatically converted to 8-bit based on the \"Fill target range (normalise)\" scaling method from the \"Image type converter\" module.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image to apply threshold to."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "Select if the threshold should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                    },
                    {
                      "name": "Output image",
                      "description": "Name of the output image created during the thresholding process.  This image will be added to the workspace."
                    },
                    {
                      "name": "Threshold mode",
                      "description": "Local thresholding algorithm mode to use.<br><ul><li>\"Slice-by-slice\" Local thresholds are applied to a multidimensional image stack one 2D image at a time.  Images are processed independently.<\/li><li>\"3D\" Local threshold algorithms are calculated in 3D and applied to all slices of an image in a single run.  This is more computationally expensive.<\/li><\/ul>"
                    },
                    {
                      "name": "Algorithm (slice-by-slice)",
                      "description": "Algorithms available for calculating local threshold on a slice-by-slice basis.  These are described at <a href=\"https://imagej.net/Auto_Local_Threshold\">https://imagej.net/Auto_Local_Threshold<\/a>.  Algorithms available: Bernsen, Contrast, Mean, Median, MidGrey, Niblack, Otsu, Phansalkar, Sauvola"
                    },
                    {
                      "name": "Algorithm (3D)",
                      "description": "Algorithms available for calculating local threshold on 3D stack in a single run (all slices processed together).  These are 3D modifications of the algorithms described at <a href=\"https://imagej.net/Auto_Local_Threshold\">https://imagej.net/Auto_Local_Threshold<\/a>.  Algorithms available: Bernsen, Contrast, Mean, Median, Phansalkar"
                    },
                    {
                      "name": "Threshold multiplier",
                      "description": "Prior to application of automatically-calculated thresholds the threshold value is multiplied by this value.  This allows the threshold to be systematically increased or decreased.  For example, a \"Threshold multiplier\" of 0.9 applied to an automatically-calculated threshold of 200 will see the image thresholded at the level 180."
                    },
                    {
                      "name": "Use lower threshold limit",
                      "description": "Limit the lowest threshold that can be applied to the image.  This is used to prevent unintentional segmentation of an image containing only background (i.e. no features present)."
                    },
                    {
                      "name": "Lower threshold limit",
                      "description": "Lowest absolute threshold value that can be applied."
                    },
                    {
                      "name": "Local radius",
                      "description": "Radius of region to be used when calculating local intensity thresholds.  Units controlled by \"Spatial units mode\" control."
                    },
                    {
                      "name": "Spatial units mode",
                      "description": "Controls whether spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                    },
                    {
                      "name": "Use full Z-range (\"Global Z\")",
                      "description": "When performing 3D local thresholding, this takes all z-values at a location into account.  If disabled, pixels will be sampled in z according to the \"Local radius\" setting."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "local-auto-threshold"
                },
                {
                  "path": "/modules/images/process/threshold/manual-threshold",
                  "deprecated": false,
                  "name": "Manual threshold",
                  "shortDescription": "Binarises an image (or image stack) using a fixed intensity threshold.",
                  "fullDescription": "Binarises an image (or image stack) using a fixed intensity threshold.  The input threshold can be a single value (same for all images) or taken from a measurement associated with the image to be binarised.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image to apply threshold to."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "Select if the threshold should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                    },
                    {
                      "name": "Output image",
                      "description": "Name of the output image created during the thresholding process.  This image will be added to the workspace."
                    },
                    {
                      "name": "Threshold source",
                      "description": "Source for the threshold value:<br><ul><li>\"Fixed value\" Uses a single, fixed value for all images.  This value is specified using the \"Threshold value\" parameter.<\/li><li>\"Image measurement\" Threshold is set to the value of a measurement assoiated with the image to be binarised.  Measurement selected by \"Measurement\" parameter.  In this mode a different threshold can be applied to each image.<\/li><\/ul>"
                    },
                    {
                      "name": "Threshold value",
                      "description": "Absolute manual threshold value that will be applied to all pixels."
                    },
                    {
                      "name": "Measurement",
                      "description": "Measurement to act as threshold value when in \"Image measurement\" mode."
                    },
                    {
                      "name": "Binary logic",
                      "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                    }
                  ],
                  "slug": "manual-threshold"
                },
                {
                  "path": "/modules/images/process/threshold/threshold-image",
                  "deprecated": true,
                  "name": "Threshold image",
                  "shortDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.",
                  "fullDescription": "Binarise an image in the workspace such that the output only has pixel values of 0 and 255.  Uses the built-in ImageJ global and 2D local auto-thresholding algorithms.<br><br>Note: Currently only works on 8-bit images.  Images with other bit depths will be automatically converted to 8-bit based on the \"Fill target range (normalise)\" scaling method from the \"Image type converter\" module.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image to apply threshold to."
                    },
                    {
                      "name": "Apply to input image",
                      "description": "Select if the threshold should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                    },
                    {
                      "name": "Output image",
                      "description": "Name of the output image created during the thresholding process.  This image will be added to the workspace."
                    },
                    {
                      "name": "Threshold type",
                      "description": "Class of threshold to be applied.<br><br> - \"Global\" (default) will apply a constant, automatically-determined threshold value to all pixels in the image.  This is best when the image is uniformly illuminated.<br><br> - \" Local\" will apply a variable threshold to each pixel in the image based on the local intensity around that pixel.  This is best when one region of the image is brighter than another, for example, due to heterogeneous illumination.  The size of the local region is determined by the user.<br><br> - \" Manual\" will apply a fixed threshold value to all pixels in the image.<br>"
                    },
                    {
                      "name": "Global threshold algorithm",
                      "description": "Global thresholding algorithm to use."
                    },
                    {
                      "name": "Local threshold algorithm",
                      "description": "Local thresholding algorithm to use."
                    },
                    {
                      "name": "Threshold multiplier",
                      "description": "Prior to application of automatically-calculated thresholds the threshold value is multiplied by this value.  This allows the threshold to be systematically increased or decreased.  For example, a \"Threshold multiplier\" of 0.9 applied to an automatically-calculated threshold of 200 will see the image thresholded at the level 180."
                    },
                    {
                      "name": "Use lower threshold limit",
                      "description": "Limit the lowest threshold that can be applied to the image.  This is used to prevent unintentional segmentation of an image containing only background (i.e. no features present)."
                    },
                    {
                      "name": "Lower threshold limit",
                      "description": "Lowest absolute threshold value that can be applied."
                    },
                    {
                      "name": "Local radius",
                      "description": "Radius of region to be used when calculating local intensity thresholds.  Units controlled by \"Spatial units mode\" control."
                    },
                    {
                      "name": "Spatial units mode",
                      "description": "Controls whether spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                    },
                    {
                      "name": "Threshold value",
                      "description": "Absolute manual threshold value that will be applied to all pixels."
                    },
                    {
                      "name": "Use full Z-range (\"Global Z\")",
                      "description": "When performing 3D local thresholding, this takes all z-values at a location into account.  If disabled, pixels will be sampled in z according to the \"Local radius\" setting."
                    },
                    {
                      "name": "Black objects/white background",
                      "description": "Controls the logic of the output image in terms of what is considered foreground and background."
                    }
                  ],
                  "slug": "threshold-image"
                }
              ]
            }
          ],
          "modules": [
            {
              "path": "/modules/images/process/apply-clahe",
              "deprecated": false,
              "name": "Apply CLAHE",
              "shortDescription": "Applies the MPICBG implementation of CLAHE (Contrast Limited Adaptive Histogram Equalization).",
              "fullDescription": "Applies the MPICBG implementation of CLAHE (Contrast Limited Adaptive Histogram Equalization).  This module runs the Image \"<a href=\"https://imagej.net/Enhance_Local_Contrast_(CLAHE)\">CLAHE<\/a>\" plugin.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to apply CLAHE to."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if CLAHE should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output image created during the CLAHE process.  This image will be added to the workspace."
                },
                {
                  "name": "Blocksize",
                  "description": "\"The size of the local region around a pixel for which the histogram is equalized. This size should be larger than the size of features to be preserved\".  Description taken from <a href=\"https://imagej.net/plugins/clahe\">https://imagej.net/plugins/clahe<\/a>"
                },
                {
                  "name": "Calibrated units",
                  "description": "Choose if block size is specified in pixel (set to \"false\") or calibrated (set to \"true\") units.  What units are used are controlled from \"Input control\"."
                },
                {
                  "name": "Histogram bins",
                  "description": "\"The number of histogram bins used for histogram equalization. The implementation internally works with byte resolution, so values larger than 256 are not meaningful. This value also limits the quantification of the output when processing 8bit gray or 24bit RGB images. The number of histogram bins should be smaller than the number of pixels in a block\".  Description taken from <a href=\"https://imagej.net/plugins/clahe\">https://imagej.net/plugins/clahe<\/a>"
                },
                {
                  "name": "Maximum slope",
                  "description": "\"Limits the contrast stretch in the intensity transfer function. Very large values will let the histogram equalization do whatever it wants to do, that is result in maximal local contrast. The value 1 will result in the original image\".  Description taken from <a href=\"https://imagej.net/plugins/clahe\">https://imagej.net/plugins/clahe<\/a>"
                },
                {
                  "name": "Use mask",
                  "description": "When selected, only pixels coincident with the white (255 intensity) part of the mask image (specified by the \"Mask image\" parameter) are processed.  All other pixels will retain their initial value"
                },
                {
                  "name": "Mask image",
                  "description": "Image to use for masking when \"Use mask\" is selected.  Only pixels in the input image coincident with white (255 intensity) pixels in this mask will be processed."
                },
                {
                  "name": "Fast (less accurate)",
                  "description": "\"Use the fast but less accurate version of the filter. The fast version does not evaluate the intensity transfer function for each pixel independently but for a grid of adjacent boxes of the given block size only and interpolates for locations in between\".  Description taken from <a href=\"https://imagej.net/plugins/clahe\">https://imagej.net/plugins/clahe<\/a>"
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple slices independently.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "apply-clahe"
            },
            {
              "path": "/modules/images/process/apply-deepimagej-model",
              "deprecated": false,
              "name": "Apply DeepImageJ model",
              "shortDescription": "Uses DeepImageJ to run Tensorflow and Pytorch models from the BioImage Model Zoo.",
              "fullDescription": "Uses DeepImageJ to run Tensorflow and Pytorch models from the BioImage Model Zoo.  This module will detect and run any models already installed in the active copy of Fiji.  For more information on DeepImageJ and the BioImage Model Zoo, please go to <a href=\"https://deepimagej.github.io/deepimagej/\">DeepImageJ<\/a> and <a href=\"https://bioimage.io/#/\">BioImage Model Zoo<\/a>.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace to apply deep learning model to."
                },
                {
                  "name": "Output image",
                  "description": "Final image generated by model, which will be stored in the workspace with this name."
                },
                {
                  "name": "Model",
                  "description": "Model to apply to input image.  This can be any model currently installed in MIA.  When using MIA's GUI, the available modules will automatically appear as options."
                },
                {
                  "name": "Use postprocessing",
                  "description": "If post-processing routines are available for the chosen model this option will be visible.  Note: If pre-processing routines are available, these will always be applied."
                },
                {
                  "name": "Patch size",
                  "description": ""
                }
              ],
              "slug": "apply-deepimagej-model"
            },
            {
              "path": "/modules/images/process/bleaching-correction",
              "deprecated": false,
              "name": "Bleaching correction",
              "shortDescription": "Apply bleaching correction to a specified image.",
              "fullDescription": "Apply bleaching correction to a specified image.  This adjusts intensities in all frames (after the first) to match the histogram distribution of the first frame.  It is intended to account for any fluorophore bleaching that occurs during acquisition of a timecourse.<br><br>This macro runs the Fiji bleaching correction plugin, \"<a href=\"https://imagej.net/Bleach_Correction\">Bleach Correction<\/a>\".",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from workspace to apply bleaching correction process to."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                },
                {
                  "name": "Output image",
                  "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                },
                {
                  "name": "Correction mode",
                  "description": "Controls the bleach correction algorithm to use:<br><ul><li>\"Exponential fit\" Assumes the bleaching process is controlled by a mono-exponential decay.  Will fail if the signal does not decay over time.  Calculation can be performed using a single ROI for all frames.<\/li><li>\"Histogram matching\" Adjusts image intensities so that the histograms match that from the first frame.<\/li><li>\"Simple ratio\" Normalises images to have the same mean intensity.  Calculation can be performed using a single ROI for all frames.<\/li><\/ul>"
                },
                {
                  "name": "Use ROI objects",
                  "description": "When selected, the bleaching and associated intensity correction will be calculated based on the pixels within a region of interest (specified as the objects of collection \"ROI objects\").  A single ROI is used for all frames (i.e. the region can't be different from frame to frame)."
                },
                {
                  "name": "ROI objects",
                  "description": "If \"Use ROI objects\" is selected, this is the object collection which will act as the region of interest for calculating the bleaching.  Since only a single ROI can be used, all objects in this collection are reduced down into a single frame and timepoint."
                }
              ],
              "slug": "bleaching-correction"
            },
            {
              "path": "/modules/images/process/colour-deconvolution",
              "deprecated": false,
              "name": "Colour deconvolution",
              "shortDescription": "Applies the <a href=\"https://imagej.",
              "fullDescription": "Applies the <a href=\"https://imagej.net/plugins/colour-deconvolution\">Colour Deconvolution<\/a> plugin to unmix an RGB image (stored in the workspace) into up to three separate channels, which are output as separate images.  The input image can be stored as either an RGB or composite image.  This process is only applicable to images created via subtractive mixing (e.g. histological staining), not to additive methods (e.g. fluorescence).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace to apply unmixing to.  This can be stored as either an RGB or composite image."
                },
                {
                  "name": "Output image 1",
                  "description": "When selected, the first stain in the stain matrix will be output to the workspace with the name specified by \"Output image 1 name\""
                },
                {
                  "name": "Output image 1 name",
                  "description": "Name to assign to first stain image, if output to the workspace."
                },
                {
                  "name": "Output image 2",
                  "description": "When selected, the second stain in the stain matrix will be output to the workspace with the name specified by \"Output image 2 name\""
                },
                {
                  "name": "Output image 2 name",
                  "description": "Name to assign to second stain image, if output to the workspace."
                },
                {
                  "name": "Output image 3",
                  "description": "When selected, the third stain in the stain matrix will be output to the workspace with the name specified by \"Output image 3 name\""
                },
                {
                  "name": "Output image 3 name",
                  "description": "Name to assign to third stain image, if output to the workspace."
                },
                {
                  "name": "Stain model",
                  "description": "Stain models to apply to input image.  If set to \"Custom (user values)\" the individual RGB components for each channel can be specified.  Model choices are: Alcian blue & H,Azan-Mallory,Brilliant_Blue,CMY,FastRed FastBlue DAB,Feulgen Light Green,Giemsa,H AEC,H&E,H&E 2,H&E DAB,H DAB,H PAS,Masson Trichrome,Methyl Green DAB,RGB,Custom (user values)"
                },
                {
                  "name": "Stain 1 (red)",
                  "description": "Red component of stain 1.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 1 (green)",
                  "description": "Green component of stain 1.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 1 (blue)",
                  "description": "Blue component of stain 1.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 2 (red)",
                  "description": "Red component of stain 2.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 2 (green)",
                  "description": "Green component of stain 2.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 2 (blue)",
                  "description": "Blue component of stain 2.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 3 (red)",
                  "description": "Red component of stain 3.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 3 (green)",
                  "description": "Green component of stain 3.  Value specified in range 0-1."
                },
                {
                  "name": "Stain 3 (blue)",
                  "description": "Blue component of stain 3.  Value specified in range 0-1."
                }
              ],
              "slug": "colour-deconvolution"
            },
            {
              "path": "/modules/images/process/combing-correction",
              "deprecated": false,
              "name": "Combing correction",
              "shortDescription": "Applies an integer pixel row shift to every other row (starting with top-most row).",
              "fullDescription": "Applies an integer pixel row shift to every other row (starting with top-most row).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to which the correction will be applied."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the input image will be updated to contain the corrected image.  Otherwise, the corrected image will be stored separately in the workspace (name controlled by \"Output image\" parameter)."
                },
                {
                  "name": "Output image",
                  "description": "Output image with correction applied."
                },
                {
                  "name": "Offset (px)",
                  "description": "Pixel offset to be applied to every other row."
                }
              ],
              "slug": "combing-correction"
            },
            {
              "path": "/modules/images/process/filter-image",
              "deprecated": false,
              "name": "Filter image",
              "shortDescription": "Apply intensity filters to an image (or image stack) in the workspace.",
              "fullDescription": "Apply intensity filters to an image (or image stack) in the workspace.  Filters are applied to each Z-stack independently (i.e. channels and timepoints do not interact with each other).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to apply filter to."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if the filter should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output image created during the filtering process.  This image will be added to the workspace."
                },
                {
                  "name": "Filter mode",
                  "description": "Filter to be applied to the image.<br><ul><li>\"DoG 2D\" Difference of Gaussian filter in 2D.<\/li><li>\"Gaussian 2D\" <\/li><li>\"Gaussian 3D\" <\/li><li>\"Gradient 2D\" <\/li><li>\"LoG 2D (approximation)\" Approximation of a Laplacian of Gaussian filter in 2D.  This is used to enhance spot-like features of sizes similar to the setting for \"Filter radius\".<\/li><li>\"Maximum 2D\" <\/li><li>\"Maximum 3D\" <\/li><li>\"Mean 2D\" <\/li><li>\"Mean 3D\" <\/li><li>\"Median 2D\" <\/li><li>\"Median 3D\" <\/li><li>\"Minimum 2D\" <\/li><li>\"Minimum 3D\" <\/li><li>\"Ridge enhancement 2D\" Uses initial image processing steps from \"Ridge Detection\" plugin to enhance ridge-like structures.<\/li><li>\"Rolling frame\" Filters the image at each frame based on frames before and/after.  The frame window over which the statistics are calculated is user-controllable<\/li><li>\"Variance 2D\" <\/li><li>\"Variance 3D\" <\/li><\/ul>"
                },
                {
                  "name": "Filter radius",
                  "description": "Range the filter is calculated over.  Often also referred to as \"sigma\".  Value specified in pixel units, unless \"calibrated units\" is enabled."
                },
                {
                  "name": "Filter radius 2",
                  "description": ""
                },
                {
                  "name": "Calibrated units",
                  "description": "Choose if filter radius is specified in pixel (set to \"false\") or calibrated (set to \"true\") units.  What units are used are controlled from \"Input control\"."
                },
                {
                  "name": "Rolling filter method",
                  "description": "Statistic to apply for rolling frame filtering."
                },
                {
                  "name": "Window indices",
                  "description": "When \"Filter mode\" is set to \"Rolling frame\", the rolling frame statistic will be calculated for each frame using these relative frames (i.e. with indices set to \"-1,1\" the statistic would be calculated based on the frames immediately before and after)."
                },
                {
                  "name": "Contour contrast",
                  "description": "When \"Filter mode\" is set to \"Ridge enhancement 2D\", this parameter controls whether the ridges to be enhanced are bright (brighter than the background) or dark (darker than the background)."
                }
              ],
              "slug": "filter-image"
            },
            {
              "path": "/modules/images/process/image-calculator",
              "deprecated": false,
              "name": "Image calculator",
              "shortDescription": "Apply pixel-wise intensity calculations for two images of matching dimensions.",
              "fullDescription": "Apply pixel-wise intensity calculations for two images of matching dimensions.<br><br>Note: Images to be processed must have matching spatial dimensions and intensity bit-depths.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image 1",
                  "description": "First image to be processed as part of calculation."
                },
                {
                  "name": "Input image 2",
                  "description": "Second image to be processed as part of calculation."
                },
                {
                  "name": "Overwrite mode",
                  "description": "Controls how the resultant image should be output:<br><ul><li>\"Create new image\" (default) will create a new image and save it to the workspace.<\/li><li>\"Overwrite image 1\" will overwrite the first input image with the output image.  The output image will retain all measurements from the first input image.<\/li><li>\"Overwrite image 2\" will overwrite the second input image with the output image.  The output image will retain all measurements from the second input image.<\/li><\/ul>"
                },
                {
                  "name": "Output image",
                  "description": "Name of the output image created during the image calculation.  This image will be added to the workspace."
                },
                {
                  "name": "Output 32-bit image",
                  "description": "When enabled, the calculation will be performed on 32-bit float values.  This is useful if the calculation is likely to create negative or decimal values.  The output image will also be stored in the workspace as a 32-bit float image."
                },
                {
                  "name": "Calculation method",
                  "description": "The calculation to apply to the two input images."
                },
                {
                  "name": "Image 2 relative contribution",
                  "description": ""
                },
                {
                  "name": "Set NaN values to zero",
                  "description": "If input images are 32-bit (or are being converted to 32-bit via \"Output 32-bit image\" option) the output image can contain NaN (not a number) values in place of any zeros."
                }
              ],
              "slug": "image-calculator"
            },
            {
              "path": "/modules/images/process/image-math",
              "deprecated": false,
              "name": "Image math",
              "shortDescription": "Applies a mathematical operation to all pixels of the input image stack.",
              "fullDescription": "Applies a mathematical operation to all pixels of the input image stack.  Operations that can be performed are: Absolute, Add, Divide, Multiply, Square, Squareroot, Subtract",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from workspace to apply calculation to.  This image can be of any bit depth."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                },
                {
                  "name": "Output image",
                  "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                },
                {
                  "name": "Output 32-bit image",
                  "description": "When enabled, the calculation will be performed on 32-bit float values.  This is useful if the calculation is likely to create negative or decimal values.  The output image will also be stored in the workspace as a 32-bit float image."
                },
                {
                  "name": "Calculation",
                  "description": "Controls the mathematical operation being applied to all pixels of this image.  Choices are: Absolute, Add, Divide, Multiply, Square, Squareroot, Subtract"
                },
                {
                  "name": "Value source",
                  "description": "For calculations that require a specific value (i.e. addition, subtraction, etc.) this parameter controls how the value is defined:<br><ul><li>\"Fixed value\" A fixed value is specified using the \"Value\" parameter.  This value is the same for all images processed by this module..<\/li><li>\"Image measurement value\" The value is taken from a measurement associated with the input image.  Values obtained in this way can be different from image to image.<\/li><\/ul>"
                },
                {
                  "name": "Image for measurement",
                  "description": "If \"Value source\" is set to \"Image measurement value\", this is the image that the measurement will be taken from.  It can be any image in the workspace, not necessarily the image to which the math operation is being applied."
                },
                {
                  "name": "Measurement",
                  "description": "If \"Value source\" is set to \"Image measurement value\", this is the measurement associated with the image specified by \"Image for measurement\" that will be used in the calculation."
                },
                {
                  "name": "Value",
                  "description": "If \"Value source\" is set to \"Fixed value\", this is the value that will be used in the calculation."
                }
              ],
              "slug": "image-math"
            },
            {
              "path": "/modules/images/process/image-type-converter",
              "deprecated": false,
              "name": "Image type converter",
              "shortDescription": "Change the bit-depth of an image stack.",
              "fullDescription": "Change the bit-depth of an image stack.  This module provides multiple ways to handle the intensity transformation from one bit-depth to another.<br><br>Note: Different scaling modes currently only apply when reducing the bit-depth of an image.  As such, converting from 8-bit to 16-bit will always result in direct conversion of intensities.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input image to be converted to another bit-depth."
                },
                {
                  "name": "Apply to input image",
                  "description": "If selected, the converted image will replace the input image in the workspace.  All measurements associated with the input image will be transferred to the converted image."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output converted image."
                },
                {
                  "name": "Output image type",
                  "description": "Target bit-depth to convert the image to.  Pixel intensities will lie within the following ranges for each bit-depth: 8-bit (0-255), 16-bit (0-65535), 32-bit (floating point precision)."
                },
                {
                  "name": "Scaling mode",
                  "description": "Method for calculating the intensity transformation between the input and output bit-depths:<br><ul><li>\"Clip (direct conversion)\" Will convert directly from the input to output bit-depth without performing any intensity scaling.  As such, any input intensities outside the available range of the output bit-depth will be clipped to the closest possible value.  For example, an input 16-bit image with intensity range 234-34563 converted to 8-bit will have an output range of 234-255.<\/li><li>\"Fill target range (normalise)\" Will stretch the input intensity range to fill the available output intensity range.  For example, an input 16-bit image with intensity range 234-34563 converted to 8-bit will have an output range of 0-255.  Images converted to 32-bit will be scaled to the range 0-1.<\/li><li>\"Scale proportionally\" Will proportionately scale intensities such that the input and output intensity ranges fill their respective bit-depths by equal amounts.  For example, an input 16-bit image with intensity range 234-34563 converted to 8-bit will have an output range of 1-135.<\/li><\/ul>"
                }
              ],
              "slug": "image-type-converter"
            },
            {
              "path": "/modules/images/process/invert-image-intensity",
              "deprecated": false,
              "name": "Invert image intensity",
              "shortDescription": "Invert intensity of each pixel.",
              "fullDescription": "Invert intensity of each pixel.  This uses the stock ImageJ intensity inversion function (\"Edit > Invert\")",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to be inverted."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the input image will be replaced by the inverted image in the workspace.  If disabled, the inverted image will be stored as a new image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "If \"Apply to input image\" is not selected, the inverted image will be stored as a new image in the workspace.  This is the name of the output inverted image."
                }
              ],
              "slug": "invert-image-intensity"
            },
            {
              "path": "/modules/images/process/manually-edit-image",
              "deprecated": false,
              "name": "Manually edit image",
              "shortDescription": "Displays an image from the workspace, allowing the user to make manual edits, before saving it back into the workspace.",
              "fullDescription": "Displays an image from the workspace, allowing the user to make manual edits, before saving it back into the workspace.  Edited images can either be stored as a new image in the workspace or overwrite the input image.  Once the image has been displayed, the user makes edits until clicking \"OK\" in the dialog box that appears.  As such, this module is only suitable for running in GUI mode.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from workspace to manually edit.  When this module executes the image will be displayed along with a dialog box allowing the user to identify when editing is complete.  Depending on the \"Apply to input image\" parameter, the edits will either be applied directly to this input image or stored in a separate image in the workspace."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the edited image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                },
                {
                  "name": "Output image",
                  "description": "If \"Apply to input image\" is not selected, the edited image will be saved to the workspace with this name."
                },
                {
                  "name": "Channel mode",
                  "description": "Select whether multi-channel images should be displayed as composites (show all channels overlaid) or individually (the displayed channel is controlled by the \"C\" slider at the bottom of the image window)."
                }
              ],
              "slug": "manually-edit-image"
            },
            {
              "path": "/modules/images/process/normalise-intensity",
              "deprecated": false,
              "name": "Normalise intensity",
              "shortDescription": "Sets the intensity to maximise the dynamic range of the image.",
              "fullDescription": "Sets the intensity to maximise the dynamic range of the image.\n\"Clipping fraction\" is the fraction of pixels at either end of the range that gets clipped.The \"Per object\" region mode will normalise all pixels within each object.<br><br>Note: This module will change pixel intensities.  To set the display range without altering pixel intensities, use the \"Set intensity display range\" module",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace for which intensity normalisation will be calculated."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if the normalisation should be applied directly to the input image, or if it should be applied to a duplicate image, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "If storing the processed image separately in the workspace, this is the name of the output image."
                },
                {
                  "name": "Region mode",
                  "description": "Controls whether the intensities are normalised across the entire image stack (\"Entire image\"), across the image slice-by-slice (\"Per slice\") or separately within each object (\"Per slice\")"
                },
                {
                  "name": "Input objects",
                  "description": "If normalising intensities on an object-by-object basis (\"Region mode\" set to \"Per object\"), these are the objects that will be used."
                },
                {
                  "name": "Calculation mode",
                  "description": "Controls how the normalisation is calculated.  In each case, the minimum and maximum intensities are calculated and all values in the output image linearly interpolated between them:<br><ul><li>\"Fast\" All intensity values in the image are collected in a histogram.  As such, for 8 and 16-bit this is fast to calculate as there are a limited number of bins.  In this instance, the clip fraction corresponds to the fraction of bins.<\/li><li>\"Manual\" The minimum and maximum intensities in the final image are manually specified with the \"Minimum range value\" and \"Maximum range value\" parameters.<\/li><li>\"Precise\" All intensity values are ordered by their intensity and the clip fraction corresponds to the fraction of pixels (rather than the fraction of unique intensities as with \"Fast\" mode.  As such, this method is more precise; however, can take a much longer time (especially for large images).<\/li><\/ul>"
                },
                {
                  "name": "Calculation source",
                  "description": "When applying a single normalisation to the entire image (\"Region mode\" set to \"Entire image\"), this parameter controls whether the normalisation range (min, max) will be determined from the input image or another image:<br><ul><li>\"External\" The image for which the normalisation range is calculated is different to the image that the final normalisation will be applied to.  For example, this could be a single representative slice or substack.  Using this could significantly reduce run-time for large stacks, especially when \"Calculation mode\" is set to \"Precise\".<\/li><li>\"Internal\" The normalisation range will be determined from the same image or stack onto which the normalisation will be applied.<\/li><\/ul>"
                },
                {
                  "name": "External source",
                  "description": "If using a separate image to determine the normalisation range (\"Calculation source\" set to \"External\"), this is the image that will be used for that calculation."
                },
                {
                  "name": "Clipping fraction (min)",
                  "description": "Fraction of unique intensities (\"Precise\") or pixels (\"Fast\") that are clipped when setting the minimum intensity.  Any values below this will be set to the calculated minimum intensity."
                },
                {
                  "name": "Clipping fraction (max)",
                  "description": "Fraction of unique intensities (\"Precise\") or pixels (\"Fast\") that are clipped when setting the maximum intensity.  Any values above this will be set to the calculated maximum intensity."
                },
                {
                  "name": "Minimum range value",
                  "description": "If manually setting the minimum intensity, this is the value that will be applied."
                },
                {
                  "name": "Maximum range value",
                  "description": "If manually setting the maximum intensity, this is the value that will be applied."
                }
              ],
              "slug": "normalise-intensity"
            },
            {
              "path": "/modules/images/process/weka-pixel-classification",
              "deprecated": false,
              "name": "Weka pixel classification",
              "shortDescription": "Performs pixel classification using the WEKA Trainable Segmentation plugin.",
              "fullDescription": "Performs pixel classification using the WEKA Trainable Segmentation plugin.<br><br>This module loads a previously-saved WEKA classifier model and applies it to the input image.  It then returns the multi-channel probability map.<br><br>Image stacks are processed in 2D, one slice at a time.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to apply pixel classification to."
                },
                {
                  "name": "Convert to RGB",
                  "description": "Converts a composite image to RGB format.  This should be set to match the image-type used for generation of the model."
                },
                {
                  "name": "Output mode",
                  "description": "Controls whether the output image is a probability map or single channel classified image.  For probabiliy maps, each class is assigned its own channel with floating point values in the range 0-1 depending on the likelihood of that pixel belonging to that class.  With classified images the pixel value corresponds to the most probable class at that position (class numbering starts at 0)."
                },
                {
                  "name": "Output image",
                  "description": "Output image, which can be either a probability map or pre-assigned class image."
                },
                {
                  "name": "Output bit depth",
                  "description": "By default images will be saved as floating point 32-bit (probabilities in the range 0-1); however, they can be converted to 8-bit (probabilities in the range 0-255) or 16-bit (probabilities in the range 0-65535).  This is useful for saving memory or if the output probability map will be passed to image threshold module."
                },
                {
                  "name": "Output single class",
                  "description": "Allows a single class (image channel) to be output.  This is another feature for reducing memory usage."
                },
                {
                  "name": "Output class",
                  "description": "Class (image channel) to be output.  Channel numbering starts at 1."
                },
                {
                  "name": "Path type",
                  "description": "Method to use for generation of the classifier filename:<br><ul><li>\"Matching format\" Will generate a name from metadata values stored in the current workspace.  This is useful if the classifier varies from input file to input file.<\/li><li>\"Specific file\" Will load the classifier file at a specific location.  This is useful if the same file is to be used for all input files.<\/li><\/ul>"
                },
                {
                  "name": "Generic format",
                  "description": "Format for a generic filename.  Plain text can be mixed with global variables or metadata values currently stored in the workspace.  Global variables are specified using the \"V{name}\" notation, where \"name\" is the name of the variable to insert.  Similarly, metadata values are specified with the \"M{name}\" notation."
                },
                {
                  "name": "Available metadata fields",
                  "description": "List of the currently-available metadata values for this workspace.  These can be used when compiling a generic filename."
                },
                {
                  "name": "Classifier file path",
                  "description": "Path to the classifier file (.model extension).  This file needs to be created manually using the WEKA Trainable Segmentation plugin included with Fiji."
                },
                {
                  "name": "Simultaneous slices",
                  "description": "Number of image slices to process at any given time.  This reduces the memory footprint of the module, but can slow down processing."
                },
                {
                  "name": "Tile factor",
                  "description": "Number of tiles per dimension each image will be subdivided into for processing.  For example, a tile factor of 2 will divide the image into a 2x2 grid of tiles.  This reduces the memory footprint of the module."
                }
              ],
              "slug": "weka-pixel-classification"
            },
            {
              "path": "/modules/images/process/white-balance-correction",
              "deprecated": false,
              "name": "White balance correction",
              "shortDescription": "Apply whitebalance correction to an image based on a reference region (specified as an object).",
              "fullDescription": "Apply whitebalance correction to an image based on a reference region (specified as an object).<br><br>Method based on the <a href=\"https://github.com/pmascalchi/ImageJ_Auto-white-balance-correction\">macro<\/a> by Patrice Mascalchi ().",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to apply white balance correction to."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if the white balance correction should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output image created during the correction process.  This image will be added to the workspace."
                },
                {
                  "name": "Reference object(s)",
                  "description": "Object to use as background reference.  Relative channel brightness will be corrected against the pixels contained within this object.  If more than one object is present in the object collection the largest object will be used."
                }
              ],
              "slug": "white-balance-correction"
            }
          ]
        },
        {
          "path": "/modules/images/transform",
          "name": "Transform",
          "description": "Image processing operations leading to changes in the whole stack layout.  For example, substack extraction, cropping or drift correction.",
          "slug": "transform",
          "subCategories": [{
            "path": "/modules/images/transform/registration",
            "name": "Registration",
            "description": "Modules performing alignment of images within a stack to account for sample drift or other motion.",
            "slug": "registration",
            "subCategories": [],
            "modules": [
              {
                "path": "/modules/images/transform/registration/affine-(mops)",
                "deprecated": false,
                "name": "Affine (MOPS)",
                "shortDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.",
                "fullDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be \"linked\" (all frames given the same registration) or \"independent\" (each stack registered separately).<br><br>This module uses the <a href=\"https://imagej.net/Feature_Extraction\">Feature Extraction<\/a> plugin and associated MPICBG tools to detect MOPS (\"Multi-Scale Oriented Patches\") features from the input images and calculate and apply the necessary 2D affine transforms.<br><br>References:<ul><li>Brown, Matthew & Szeliski, Richard \"Multi-image feature matching using multi-scale oriented patches\". US Patent 7,382,897 (June 3, 2008). Asignee: Microsoft Corporation.<\/li><\/ul>",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine (trans., rot., scale, shear)\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid (trans., rot.)\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity (trans., rot., iso-scale)\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Test flip (mirror image)",
                    "description": "When selected, alignment will be tested for both the \"normal\" and \"flipped\" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained."
                  },
                  {
                    "name": "Independent rotation",
                    "description": "When selected, the image will be rotated multiple times, with registration optimised at each orientation.  The orientation with the best score will be retained.  This is useful for algorithms which perform poorly with rotated features (e.g. block matching).  The increment between rotations is controlled by \"Orientation increment (degs)\"."
                  },
                  {
                    "name": "Orientation increment (degs)",
                    "description": "If \"Independent rotation\" is enabled, this is the angular increment between rotations.  The increment is specified in degree units."
                  },
                  {
                    "name": "Show transformation(s)",
                    "description": "When selected, the affine transform will be displayed in the results table.  Fixed affine transform values such as these can be applied using the \"Affine (fixed transform)\" module."
                  },
                  {
                    "name": "Clear between images",
                    "description": "If \"Show transformation(s)\" is enabled, this parameter can be used to reset the displayed affine transform in the results table.  If this option isn't selected, the new transform will be added to the bottom of the results table."
                  },
                  {
                    "name": "Initial Gaussian blur (px)",
                    "description": "\"Accurate localization of keypoints requires initial smoothing of the image. If your images are blurred already, you might lower the initial blur 0 slightly to get more but eventually less stable keypoints. Increasing 0 increases the computational cost for Gaussian blur, setting it to 0=3.2px is equivalent to keep 0=1.6px and use half maximum image size. Tip: Keep the default value 0=1.6px as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Steps per scale",
                    "description": "\"Keypoint candidates are extracted at all scales between maximum image size and minimum image size. This Scale Space is represented in octaves each covering a fixed number of discrete scale steps from 0 to 20. More steps result in more but eventually less stable keypoint candidates. Tip: Keep 3 as suggested by Lowe (2004) and do not use more than 10.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Minimum image size (px)",
                    "description": "\"The Scale Space stops if the size of the octave would be smaller than minimum image size. Tip: Increase the minimum size to discard large features (i.e. those extracted from looking at an image from far, such as the overall shape).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximum image size (px)",
                    "description": "\"The Scale Space starts with the first octave equal or smaller than the maximum image size. Tip: By reducing the size, fine scaled features will be discarded. Increasing the size beyond that of the actual images has no effect.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor size",
                    "description": "\"The MOPS-descriptor is simply a nxn intensity patch with normalized intensities. Brown (2005) suggests n=8.  We found larger descriptors with n>16 perform better for Transmission Electron Micrographs from serial sections.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor orientation bins",
                    "description": "\"For SIFT-descriptors, this is the number of orientation bins b per 44px block as described above. Tip: Keep the default value b=8 as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Closest/next closest ratio",
                    "description": "\"Correspondence candidates from local descriptor matching are accepted only if the Euclidean distance to the nearest neighbour is significantly smaller than that to the next nearest neighbour. Lowe (2004) suggests a ratio of r=0.8 which requires some increase when matching things that appear significantly distorted.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximal alignment error (px)",
                    "description": "\"Matching local descriptors gives many false positives, but true positives are consistent with respect to a common transformation while false positives are not. This consistent set and the underlying transformation are identified using RANSAC. This value is the maximal allowed transfer error of a match to be counted as a good one. Tip: Set this to about 10% of the image size.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Inlier ratio",
                    "description": "\"The ratio of the number of true matches to the number of all matches including both true and false used by RANSAC. 0.05 means that minimally 5% of all matches are expected to be good while 0.9 requires that 90% of the matches were good. Only transformations with this minimal ratio of true consent matches are accepted. Tip: Do not go below 0.05 (and only if 5% is more than about 7 matches) except with a very small maximal alignment error to avoid wrong solutions.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  }
                ],
                "slug": "affine-(mops)"
              },
              {
                "path": "/modules/images/transform/registration/affine-(sift)",
                "deprecated": false,
                "name": "Affine (SIFT)",
                "shortDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.",
                "fullDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be \"linked\" (all frames given the same registration) or \"independent\" (each stack registered separately).<br><br>This module uses the <a href=\"https://imagej.net/Feature_Extraction\">Feature Extraction<\/a> plugin and associated MPICBG tools to detect SIFT (\"Scale Invariant Feature Transform\") features from the input images and calculate and apply the necessary 2D affine transforms.<br><br>Note: The SIFT-algorithm is protected by U.S. Patent 6,711,293: Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image by the University of British Columbia. That is, for commercial applications the permission of the author is required. Anything else is published under the terms of the GPL, so feel free to use it for academic or personal purposes.<br><br>References:<ul><li>Lowe, David G. \"Object recognition from local scale-invariant features\". <i>Proceedings of the International Conference on Computer Vision<\/i> <b>2<\/b> (1999) 1150\u20131157.<\/li><li>Lowe, David G. \"Distinctive Image Features from Scale-Invariant Keypoints\". <i>International Journal of Computer Vision<\/i> <b>60<\/b> (2004) 91\u2013110.<\/li><\/ul>",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine (trans., rot., scale, shear)\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid (trans., rot.)\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity (trans., rot., iso-scale)\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Test flip (mirror image)",
                    "description": "When selected, alignment will be tested for both the \"normal\" and \"flipped\" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained."
                  },
                  {
                    "name": "Independent rotation",
                    "description": "When selected, the image will be rotated multiple times, with registration optimised at each orientation.  The orientation with the best score will be retained.  This is useful for algorithms which perform poorly with rotated features (e.g. block matching).  The increment between rotations is controlled by \"Orientation increment (degs)\"."
                  },
                  {
                    "name": "Orientation increment (degs)",
                    "description": "If \"Independent rotation\" is enabled, this is the angular increment between rotations.  The increment is specified in degree units."
                  },
                  {
                    "name": "Show transformation(s)",
                    "description": "When selected, the affine transform will be displayed in the results table.  Fixed affine transform values such as these can be applied using the \"Affine (fixed transform)\" module."
                  },
                  {
                    "name": "Clear between images",
                    "description": "If \"Show transformation(s)\" is enabled, this parameter can be used to reset the displayed affine transform in the results table.  If this option isn't selected, the new transform will be added to the bottom of the results table."
                  },
                  {
                    "name": "Initial Gaussian blur (px)",
                    "description": "\"Accurate localization of keypoints requires initial smoothing of the image. If your images are blurred already, you might lower the initial blur 0 slightly to get more but eventually less stable keypoints. Increasing 0 increases the computational cost for Gaussian blur, setting it to 0=3.2px is equivalent to keep 0=1.6px and use half maximum image size. Tip: Keep the default value 0=1.6px as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Steps per scale",
                    "description": "\"Keypoint candidates are extracted at all scales between maximum image size and minimum image size. This Scale Space is represented in octaves each covering a fixed number of discrete scale steps from 0 to 20. More steps result in more but eventually less stable keypoint candidates. Tip: Keep 3 as suggested by Lowe (2004) and do not use more than 10.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Minimum image size (px)",
                    "description": "\"The Scale Space stops if the size of the octave would be smaller than minimum image size. Tip: Increase the minimum size to discard large features (i.e. those extracted from looking at an image from far, such as the overall shape).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximum image size (px)",
                    "description": "\"The Scale Space starts with the first octave equal or smaller than the maximum image size. Tip: By reducing the size, fine scaled features will be discarded. Increasing the size beyond that of the actual images has no effect.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor size",
                    "description": "\"The SIFT-descriptor consists of nn gradient histograms, each from a 44px block. n is this value. Lowe (2004) uses n=4. We found larger descriptors with n=8 perform better for Transmission Electron Micrographs from serial sections.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor orientation bins",
                    "description": "\"For SIFT-descriptors, this is the number of orientation bins b per 44px block as described above. Tip: Keep the default value b=8 as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Closest/next closest ratio",
                    "description": "\"Correspondence candidates from local descriptor matching are accepted only if the Euclidean distance to the nearest neighbour is significantly smaller than that to the next nearest neighbour. Lowe (2004) suggests a ratio of r=0.8 which requires some increase when matching things that appear significantly distorted.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximal alignment error (px)",
                    "description": "\"Matching local descriptors gives many false positives, but true positives are consistent with respect to a common transformation while false positives are not. This consistent set and the underlying transformation are identified using RANSAC. This value is the maximal allowed transfer error of a match to be counted as a good one. Tip: Set this to about 10% of the image size.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Inlier ratio",
                    "description": "\"The ratio of the number of true matches to the number of all matches including both true and false used by RANSAC. 0.05 means that minimally 5% of all matches are expected to be good while 0.9 requires that 90% of the matches were good. Only transformations with this minimal ratio of true consent matches are accepted. Tip: Do not go below 0.05 (and only if 5% is more than about 7 matches) except with a very small maximal alignment error to avoid wrong solutions.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  }
                ],
                "slug": "affine-(sift)"
              },
              {
                "path": "/modules/images/transform/registration/affine-(block-matching)",
                "deprecated": false,
                "name": "Affine (block matching)",
                "shortDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.",
                "fullDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be \"linked\" (all frames given the same registration) or \"independent\" (each stack registered separately).<br><br>This module uses the <a href=\"https://github.com/fiji/blockmatching\">Block Matching<\/a> plugin and associated MPICBG tools to detect matching regions from the input images and calculate and apply the necessary 2D affine transforms.",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine (trans., rot., scale, shear)\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid (trans., rot.)\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity (trans., rot., iso-scale)\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Test flip (mirror image)",
                    "description": "When selected, alignment will be tested for both the \"normal\" and \"flipped\" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained."
                  },
                  {
                    "name": "Independent rotation",
                    "description": "When selected, the image will be rotated multiple times, with registration optimised at each orientation.  The orientation with the best score will be retained.  This is useful for algorithms which perform poorly with rotated features (e.g. block matching).  The increment between rotations is controlled by \"Orientation increment (degs)\"."
                  },
                  {
                    "name": "Orientation increment (degs)",
                    "description": "If \"Independent rotation\" is enabled, this is the angular increment between rotations.  The increment is specified in degree units."
                  },
                  {
                    "name": "Show transformation(s)",
                    "description": "When selected, the affine transform will be displayed in the results table.  Fixed affine transform values such as these can be applied using the \"Affine (fixed transform)\" module."
                  },
                  {
                    "name": "Clear between images",
                    "description": "If \"Show transformation(s)\" is enabled, this parameter can be used to reset the displayed affine transform in the results table.  If this option isn't selected, the new transform will be added to the bottom of the results table."
                  },
                  {
                    "name": "Layer scale",
                    "description": "Scale factor applied to input image prior to alignment of blocks.  This can be used to reduce the computational cost of performing the registration.  For example, a scale of 0.5 will reduce the size of the image used in the alignment process.  Note: The final registration will be applied to the original size images."
                  },
                  {
                    "name": "Search radius (px)",
                    "description": "The maximum range a single block can move from its original position whilst searching for the highest quality alignment."
                  },
                  {
                    "name": "Block radius (px)",
                    "description": "Size of the individual blocks that will be used in searching for the highest quality alignment.  These need to be sufficiently large to include recognisable features in a single block."
                  },
                  {
                    "name": "Resolution",
                    "description": "The number of vertices in the spring mesh, with higher numbers giving smoother results."
                  },
                  {
                    "name": "Minimal PMCC r",
                    "description": "\"The PMCC coefficent <i>r<\/i> of a patch around the vertex and the overlapping patch in the other image is used as the quality measure for a match.  The threshold for minimal PMCC <i>r<\/i> can be higher for aligning the same signal than for aligning changing signals. Higher values will lead to more matches rejected and thus less false positives.\".  Description taken from <a href=\"https://imagej.net/Elastic_Alignment_and_Montage.html\">https://imagej.net/Elastic_Alignment_and_Montage.html<\/a>"
                  },
                  {
                    "name": "Maximal curvature ratio",
                    "description": "\"The maximal curvature ratio is the threshold for edge responses. The value must be >1.0. Higher values will accept more matches alongside elongated structures and thus lead to potentially more false positives.\".  Description taken from <a href=\"https://imagej.net/Elastic_Alignment_and_Montage.html\">https://imagej.net/Elastic_Alignment_and_Montage.html<\/a>"
                  },
                  {
                    "name": "Closest/next closest ratio",
                    "description": "\"Correspondence candidates from local descriptor matching are accepted only if the Euclidean distance to the nearest neighbour is significantly smaller than that to the next nearest neighbour. Lowe (2004) suggests a ratio of r=0.8 which requires some increase when matching things that appear significantly distorted.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Local region sigma",
                    "description": "\"The local smoothness filter inspects each match and compares how well the estimated translational offset agrees with all other matches weighted by their distance to the inspected match. To that end, a local linear transformation (typically rigid) is calculated using weighted least squares. The weight for each match is defined by a Gaussian radial distribution function (RDF) centered at the reference match.  This parameter controls sigma for this RDF. A match is rejected if its transfer error relative to the estimated linear transformation is larger than an absolute threshold or larger than k the average transfer error of all weighted matches (k is specified in the relative field).  \"Description taken from <a href=\"https://imagej.net/Elastic_Alignment_and_Montage.html\">https://imagej.net/Elastic_Alignment_and_Montage.html<\/a>"
                  },
                  {
                    "name": "Maximal absolute local displacement (px)",
                    "description": "See description for \"Local region sigma\"."
                  },
                  {
                    "name": "Maximal relative local displacement (px)",
                    "description": "See description for \"Local region sigma\"."
                  }
                ],
                "slug": "affine-(block-matching)"
              },
              {
                "path": "/modules/images/transform/registration/affine-(fixed-transform)",
                "deprecated": false,
                "name": "Affine (fixed transform)",
                "shortDescription": "",
                "fullDescription": "",
                "version": "1.0.0",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": ""
                  },
                  {
                    "name": "Apply to input image",
                    "description": ""
                  },
                  {
                    "name": "Output image",
                    "description": ""
                  },
                  {
                    "name": "Transformation",
                    "description": ""
                  },
                  {
                    "name": "Fill mode",
                    "description": ""
                  },
                  {
                    "name": "Enable multithreading",
                    "description": ""
                  }
                ],
                "slug": "affine-(fixed-transform)"
              },
              {
                "path": "/modules/images/transform/registration/affine-(manual)",
                "deprecated": false,
                "name": "Affine (manual)",
                "shortDescription": "Apply 2D affine transforms to align images from the workspace to other images from the workspace using manually-selected reference points.",
                "fullDescription": "Apply 2D affine transforms to align images from the workspace to other images from the workspace using manually-selected reference points.  When the module runs, the input and reference images are displayed. The user then selects matching points on each image and clicks \"Add pair(s)\". Points must be added in the same order on each image (ID numbers next to each point provide a reference). Points are shown in the control window and can be deleted by highlighting the relevant entry and clicking \"Remove pair\". Finally, the alignment is accepted by clicking \"Finish adding pairs\", at which point the images are closed and the transform is applied.  If multiple slices/timepoints are to be aligned, the next image pair will immediately be displayed and the processes is repeated.  The transformed input image can either overwrite the input image in the workspace, or be saved to the workspace with a new name.Alignments are calculated using the <a href=\"https://github.com/axtimwalde/mpicbg\">MPICBG<\/a> image transformation library.",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine (trans., rot., scale, shear)\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid (trans., rot.)\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity (trans., rot., iso-scale)\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Test flip (mirror image)",
                    "description": "When selected, alignment will be tested for both the \"normal\" and \"flipped\" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained."
                  },
                  {
                    "name": "Independent rotation",
                    "description": "When selected, the image will be rotated multiple times, with registration optimised at each orientation.  The orientation with the best score will be retained.  This is useful for algorithms which perform poorly with rotated features (e.g. block matching).  The increment between rotations is controlled by \"Orientation increment (degs)\"."
                  },
                  {
                    "name": "Orientation increment (degs)",
                    "description": "If \"Independent rotation\" is enabled, this is the angular increment between rotations.  The increment is specified in degree units."
                  },
                  {
                    "name": "Show transformation(s)",
                    "description": "When selected, the affine transform will be displayed in the results table.  Fixed affine transform values such as these can be applied using the \"Affine (fixed transform)\" module."
                  },
                  {
                    "name": "Clear between images",
                    "description": "If \"Show transformation(s)\" is enabled, this parameter can be used to reset the displayed affine transform in the results table.  If this option isn't selected, the new transform will be added to the bottom of the results table."
                  },
                  {
                    "name": "Point selection mode",
                    "description": "The source for points to be used in calculation of image registration:<br><ul><li>\"Pre-selected points\" Points have been previously-selected on the input images as multi-point ROIs.  These points are passed directly into the registration calculation.  This negates the need for user-interaction at runtime.<\/li><li>\"Select at runtime\" Points must be manually-selected by the user at analysis runtime.  The two images to be aligned are displayed and a dialog box opens to allow selection of point pairs.  Point pairs must be added in the same order on each image.  For images where multiple slices/timepoints need to be registered, image pairs will be opened sequentially, with the point selections from the previous slice/timepoint being pre-selected for convenience.<\/li><\/ul>"
                  }
                ],
                "slug": "affine-(manual)"
              },
              {
                "path": "/modules/images/transform/registration/affine-(object-centroids)",
                "deprecated": false,
                "name": "Affine (object centroids)",
                "shortDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.",
                "fullDescription": "Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be \"linked\" (all frames given the same registration) or \"independent\" (each stack registered separately).<br><br>This module uses centroids of previously-detected objects as the reference points for image alignment",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine (trans., rot., scale, shear)\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid (trans., rot.)\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity (trans., rot., iso-scale)\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Test flip (mirror image)",
                    "description": "When selected, alignment will be tested for both the \"normal\" and \"flipped\" (mirror) states of the image.  The state yielding the lower cost to alignment will be retained."
                  },
                  {
                    "name": "Independent rotation",
                    "description": "When selected, the image will be rotated multiple times, with registration optimised at each orientation.  The orientation with the best score will be retained.  This is useful for algorithms which perform poorly with rotated features (e.g. block matching).  The increment between rotations is controlled by \"Orientation increment (degs)\"."
                  },
                  {
                    "name": "Orientation increment (degs)",
                    "description": "If \"Independent rotation\" is enabled, this is the angular increment between rotations.  The increment is specified in degree units."
                  },
                  {
                    "name": "Show transformation(s)",
                    "description": "When selected, the affine transform will be displayed in the results table.  Fixed affine transform values such as these can be applied using the \"Affine (fixed transform)\" module."
                  },
                  {
                    "name": "Clear between images",
                    "description": "If \"Show transformation(s)\" is enabled, this parameter can be used to reset the displayed affine transform in the results table.  If this option isn't selected, the new transform will be added to the bottom of the results table."
                  },
                  {
                    "name": "Input objects",
                    "description": "Centroids for these objects will be used as the references for image alignment."
                  },
                  {
                    "name": "Maximum separation (px)",
                    "description": "Maximum spatial separation between object centroids for them to be linked and used in the alignment."
                  },
                  {
                    "name": "Maximal alignment error (px)",
                    "description": "\"Matching local descriptors gives many false positives, but true positives are consistent with respect to a common transformation while false positives are not. This consistent set and the underlying transformation are identified using RANSAC. This value is the maximal allowed transfer error of a match to be counted as a good one. Tip: Set this to about 10% of the image size.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Inlier ratio",
                    "description": "\"The ratio of the number of true matches to the number of all matches including both true and false used by RANSAC. 0.05 means that minimally 5% of all matches are expected to be good while 0.9 requires that 90% of the matches were good. Only transformations with this minimal ratio of true consent matches are accepted. Tip: Do not go below 0.05 (and only if 5% is more than about 7 matches) except with a very small maximal alignment error to avoid wrong solutions.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  }
                ],
                "slug": "affine-(object-centroids)"
              },
              {
                "path": "/modules/images/transform/registration/register-images",
                "deprecated": true,
                "name": "Register images",
                "shortDescription": "DEPRECATED: Please use separate automatic and manual modules instead.",
                "fullDescription": "DEPRECATED: Please use separate automatic and manual modules instead.<br><br>Apply slice-by-slice (2D) affine-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration is performed along the time axes and applied equally to all Z-slices.  For greater control (including registration along Z) please use separate automatic and manual modules instead.<br><br>This module uses the <a href=\"https://imagej.net/Feature_Extraction\">Feature Extraction<\/a> and <a href=\"https://imagej.net/Linear_Stack_Alignment_with_SIFT\">Linear Stack Alignment with SIFT<\/a> plugins to detect SIFT (\"Scale Invariant Feature Transform\") features from the input images and calculate and apply the necessary 2D affine transforms.<br><br>Note: The SIFT-algorithm is protected by U.S. Patent 6,711,293: Method and apparatus for identifying scale invariant features in an image and use of same for locating an object in an image by the University of British Columbia. That is, for commercial applications the permission of the author is required. Anything else is published under the terms of the GPL, so feel free to use it for academic or personal purposes.<br><br>References:<ul><li>Lowe, David G. \"Object recognition from local scale-invariant features\". <i>Proceedings of the International Conference on Computer Vision<\/i> <b>2<\/b> (1999) 1150\u20131157.<\/li><li>Lowe, David G. \"Distinctive Image Features from Scale-Invariant Keypoints\". <i>International Journal of Computer Vision<\/i> <b>60<\/b> (2004) 91\u2013110.<\/li><\/ul>",
                "version": "1.0.0",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Transformation mode",
                    "description": "Controls the type of registration being applied:<br><ul><li>\"Affine\" Applies the full affine transformation, whereby the input image can undergo translation, rotation, reflection, scaling and shear.<\/li><li>\"Rigid\" Applies only translation and rotation to the input image.  As such, all features should remain the same size.<\/li><li>\"Similarity\" Applies translation, rotating and linear scaling to the input image.<\/li><li>\"Translation\" Applies only translation (motion within the 2D plane) to the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "Alignment mode",
                    "description": "Controls whether the registration is determined automatically through SIFT feature extraction or manually, by a user selecting reference points on a pair of images."
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Relative mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous frame\" Each image will be compared to the frame (or slice when in Z-axis mode) immediately before it.  This copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Rolling correction",
                    "description": "Controls whether the entire stack is moved at specific intervals.  When enabled (\"Every nth frame\"), any remaining unregistered images will be moved to match the present transform.  This is only available when registering relative to the previous frame and is intended to prevent the difference between the previous frame (registered) and unregistered images becoming too large.  The frame interval at which this transformation is applied to the unregistered images is specified using \"Correction interval\".  Note: This can lead to images becoming increasingly blurry as they are passed through multiple interpolation steps."
                  },
                  {
                    "name": "Correction interval",
                    "description": "If applying rolling correction, this is the frame interval at which the transformation will be applied to the unregistered images."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Relative mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  The external image must be the same length along the registration axis.  The non-registration axis will have a maximum intensity projection applied prior to calculation of transform.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Initial Gaussian blur (px)",
                    "description": "\"Accurate localization of keypoints requires initial smoothing of the image. If your images are blurred already, you might lower the initial blur 0 slightly to get more but eventually less stable keypoints. Increasing 0 increases the computational cost for Gaussian blur, setting it to 0=3.2px is equivalent to keep 0=1.6px and use half maximum image size. Tip: Keep the default value 0=1.6px as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Steps per scale",
                    "description": "\"Keypoint candidates are extracted at all scales between maximum image size and minimum image size. This Scale Space is represented in octaves each covering a fixed number of discrete scale steps from 0 to 20. More steps result in more but eventually less stable keypoint candidates. Tip: Keep 3 as suggested by Lowe (2004) and do not use more than 10.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Minimum image size (px)",
                    "description": "\"The Scale Space stops if the size of the octave would be smaller than minimum image size. Tip: Increase the minimum size to discard large features (i.e. those extracted from looking at an image from far, such as the overall shape).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximum image size (px)",
                    "description": "\"The Scale Space starts with the first octave equal or smaller than the maximum image size. Tip: By reducing the size, fine scaled features will be discarded. Increasing the size beyond that of the actual images has no effect.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor size",
                    "description": "\"The SIFT-descriptor consists of nn gradient histograms, each from a 44px block. n is this value. Lowe (2004) uses n=4. We found larger descriptors with n=8 perform better for Transmission Electron Micrographs from serial sections.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Feature descriptor orientation bins",
                    "description": "\"For SIFT-descriptors, this is the number of orientation bins b per 44px block as described above. Tip: Keep the default value b=8 as suggested by Lowe (2004).\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Closest/next closest ratio",
                    "description": "\"Correspondence candidates from local descriptor matching are accepted only if the Euclidean distance to the nearest neighbour is significantly smaller than that to the next nearest neighbour. Lowe (2004) suggests a ratio of r=0.8 which requires some increase when matching things that appear significantly distorted.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Maximal alignment error (px)",
                    "description": "\"Matching local descriptors gives many false positives, but true positives are consistent with respect to a common transformation while false positives are not. This consistent set and the underlying transformation are identified using RANSAC. This value is the maximal allowed transfer error of a match to be counted as a good one. Tip: Set this to about 10% of the image size.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  },
                  {
                    "name": "Inlier ratio",
                    "description": "\"The ratio of the number of true matches to the number of all matches including both true and false used by RANSAC. 0.05 means that minimally 5% of all matches are expected to be good while 0.9 requires that 90% of the matches were good. Only transformations with this minimal ratio of true consent matches are accepted. Tip: Do not go below 0.05 (and only if 5% is more than about 7 matches) except with a very small maximal alignment error to avoid wrong solutions.\".  Description taken from <a href=\"https://imagej.net/Feature_Extraction\">https://imagej.net/Feature_Extraction<\/a>"
                  }
                ],
                "slug": "register-images"
              },
              {
                "path": "/modules/images/transform/registration/unwarp-(automatic)",
                "deprecated": false,
                "name": "Unwarp (automatic)",
                "shortDescription": "Apply slice-by-slice (2D) B-spline unwarping-based image registration to a multi-dimensional stack.",
                "fullDescription": "Apply slice-by-slice (2D) B-spline unwarping-based image registration to a multi-dimensional stack.  Images can be aligned relative to the first frame in the stack, the previous frame or a separate image in the workspace.  The registration transform can also be calculated from a separate stack to the one that it will be applied to.  Registration can be performed along either the time or Z axes.  The non-registered axis (e.g. time axis when registering in Z) can be \"linked\" (all frames given the same registration) or \"independent\" (each stack registered separately).<br><br>This module uses the <a href=\"https://imagej.net/BUnwarpJ\">BUnwarpJ<\/a> plugin to calculate and apply the necessary 2D transforms.  Detailed information about how the BUnwarpJ process works can be found at <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>.",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Registration mode",
                    "description": "\"The registration mode can be \"Accurate\", \"Fast\" and \"Mono\". The registration mode \"Mono\" makes the program to perform only unidirectional registration, i.e. from source to target. The two registration modes \"Accurate\" and \"Fast\" involve performing bidirectional registration and affect the stopping criteria internally used by the program.\"  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Subsample factor",
                    "description": "\"The registration will be calculated using subsampled versions of the images but the results will be applied to the original ones. The image subsampling parameter can be chosen between 0 and 7, i.e. the image dimensions can be reduced by a factor of 2^0 = 1 to 2^7 = 128. This is very useful when registering large images.\"  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Initial deformation mode",
                    "description": "\"Determines the level of detail of the initial deformation. In bUnwarpJ this is defined by the number of B-splines used to represent the deformations:<br><br><table style=\"font:sans-serif;font-size:12\"><tr><th>Deformation<\/th><th>Num. intervals in grid<\/th><\/tr><tr><td>Very coarse<\/td><td>1x1<\/td><\/tr><tr><td>Coarse<\/td><td>2x2<\/td><\/tr><tr><td>Fine<\/td><td>3x3<\/td><\/tr><tr><td>Very fine<\/td><td>8x8<\/td><\/tr><tr><td>Super fine<\/td><td>16x16<\/td><\/tr><\/table><font face=\"sans-serif\" size=\"3\"><br>If images start very far away from the right alignment, it is usually a good idea to go from \"Very Coarse\" to \"Very Fine\". If they start close to the right alignment, using a very coarse initial deformation could cause the algorithm to fail. So, in that case, it would be enough to set initial deformation to \"Fine\" and final deformation to \"Very Fine\". Use \"Super Fine\" only when you need a very high level of accuracy, because it makes the algorithm quite slower depending on the image sizes.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Final deformation mode",
                    "description": "See description for \"Initial deformation mode\""
                  },
                  {
                    "name": "Divergence weight",
                    "description": "Regularizes the deformation by penalizing the divergence of the deformation vector field.  If you see that your transformations get too rough, it is a good idea to use this parameter.  A value of 0.1 is usually good if there's no prior knowledge about the deformation shape.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Curl weight",
                    "description": "Regularizes the deformation by penalizing the curl of the deformation vector field.  If you see that your transformations get too rough, it is a good idea to use this parameter.  A value of 0.1 is usually good if there's no prior knowledge about the deformation shape.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Landmark weight",
                    "description": "Forces the deformations to fit the landmark points.  Set it to 1.0 unless you're not using landmarks.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Image weight",
                    "description": "The weight to control the pixel values difference.  Leave it at 1.0 unless you want to do, for instance, landmark-only registration.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Consistency weight",
                    "description": "Forces the resulting deformations to be one (source to target) as close as possible to the inverse of the other one (target to source). Values between 10.0 and 30.0 usually work fine. It is only taken into account for registration modes \"Fast\" or \"Accurate\".  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Stop threshold",
                    "description": "Stops the optimization process at each multiresolution level when the error relative change is not larger than this threshold.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  }
                ],
                "slug": "unwarp-(automatic)"
              },
              {
                "path": "/modules/images/transform/registration/unwarp-(manual)",
                "deprecated": false,
                "name": "Unwarp (manual)",
                "shortDescription": "Apply 2D B-spline unwarping transforms to align images from the workspace to other images from the workspace using manually-selected reference points.",
                "fullDescription": "Apply 2D B-spline unwarping transforms to align images from the workspace to other images from the workspace using manually-selected reference points.  When the module runs, the input and reference images are displayed. The user then selects matching points on each image and clicks \"Add pair(s)\". Points must be added in the same order on each image (ID numbers next to each point provide a reference). Points are shown in the control window and can be deleted by highlighting the relevant entry and clicking \"Remove pair\". Finally, the alignment is accepted by clicking \"Finish adding pairs\", at which point the images are closed and the transform is applied.  If multiple slices/timepoints are to be aligned, the next image pair will immediately be displayed and the processes is repeated.  The transformed input image can either overwrite the input image in the workspace, or be saved to the workspace with a new name.Alignments are calculated using the <a href=\"https://imagej.net/BUnwarpJ\">BUnwarpJ<\/a> image transformation library.",
                "version": "1.0.1",
                "parameters": [
                  {
                    "name": "Input image",
                    "description": "Image from workspace to apply registration to."
                  },
                  {
                    "name": "Apply to input image",
                    "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                  },
                  {
                    "name": "Output image",
                    "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                  },
                  {
                    "name": "Registration axis",
                    "description": "Controls which stack axis the registration will be applied in.  For example, when \"Time\" is selected, all images along the time axis will be aligned.  Choices are: Time, Z."
                  },
                  {
                    "name": "Other axis mode",
                    "description": "For stacks with non-registration axis lengths longer than 1 (e.g. the \"Z\" axis when registering in time) the behaviour of this other axis is controlled by this parameter:<br><ul><li>\"Independent\" Each non-registration axis is registered independently.  For example, applying separate Z-registrations for each timepoint of a 4D stack.<\/li><li>\"Linked\" All elements of the non-registration axis are registered with a single transform.  For example, applying the same registration at a timepoint to all slices of a 4D stack.<\/li><\/ul>"
                  },
                  {
                    "name": "Fill mode",
                    "description": "Controls what intensity any border pixels will have.  \"Borders\" in this case correspond to strips/wedges at the image edge corresponding to regions outside the initial image (e.g. the right-side of an output image when the input was translated to the left).   Choices are: Black, White."
                  },
                  {
                    "name": "Show detected points",
                    "description": "When enabled, the points used for calculation of the registration will be added as an overlay to the input image and displayed."
                  },
                  {
                    "name": "Enable multithreading",
                    "description": "When selected, certain parts of the registration process will be run on multiple threads of the CPU.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                  },
                  {
                    "name": "Reference mode",
                    "description": "Controls what reference image each image will be compared to:<br><ul><li>\"First frame\" All images will be compared to the first frame (or slice when in Z-axis mode).  For image sequences which continuously evolve over time (e.g. cells dividing) this can lead to reduced likelihood of successfully calculating the transform over time.<\/li><li>\"Previous N frames\" Each image will be compared to the N frames (or slice when in Z-axis mode) immediately before it (number of frames specified by \"Number of previous frames\").  These reference frames are consolidated into a single reference image using a projection based on the statistic specified by \"Previous frames statistic\".  This mode copes better with image sequences which continuously evolve over time, but can also lead to compounding errors over time (errors in registration get propagated to all remaining slices).<\/li><li>\"Specific image\" All images will be compared to a separate 2D image from the workspace.  The image to compare to is selected using the \"Reference image\" parameter.<\/li><\/ul>"
                  },
                  {
                    "name": "Number of previous frames",
                    "description": "Number of previous frames (or slices) to use as reference image when \"Reference mode\" is set to \"Previous N frames\".  If there are insufficient previous frames (e.g. towards the beginning of the stack) the maximum available frames will be used.  Irrespective of the number of frames used, the images will be projected into a single reference image using the statistic specified by \"Previous frames statistic\"."
                  },
                  {
                    "name": "Previous frames statistic",
                    "description": "Statistic to use when combining multiple previous frames as a reference (\"Reference mode\" set to \"Previous N frames\")."
                  },
                  {
                    "name": "Reference image",
                    "description": "If \"Reference mode\" is set to \"Specific image\" mode, all input images will be registered relative to this image.  This image must only have a single channel, slice and timepoint."
                  },
                  {
                    "name": "Calculation source",
                    "description": "Controls whether the input image will be used to calculate the registration transform or whether it will be determined from a separate image:<br><ul><li>\"External\" The transform is calculated from a separate image from the workspace (specified using \"External source\").  This could be an image with enhanced contrast (to enable better feature extraction), but where the enhancements are not desired in the output registered image.  When \"Other axis mode\" is set to \"Linked\", the external image must be the same length along the registration axis and have single-valued length along the non-registration axis.  However, when set to \"Independent\", the external image must have the same axis lengths for both the registration and non-registration axes.<\/li><li>\"Internal\" The transform is calculated from the input image.<\/li><\/ul>"
                  },
                  {
                    "name": "External source",
                    "description": "If \"Calculation source\" is set to \"External\", registration transforms will be calculated using this image from the workspace.  This image will be unaffected by the process."
                  },
                  {
                    "name": "Calculation channel",
                    "description": "If calculating the registration transform from a multi-channel image stack, the transform will be determined from this channel only.  Irrespectively, for multi-channel image stacks, the calculated transform will be applied equally to all channels."
                  },
                  {
                    "name": "Registration mode",
                    "description": "\"The registration mode can be \"Accurate\", \"Fast\" and \"Mono\". The registration mode \"Mono\" makes the program to perform only unidirectional registration, i.e. from source to target. The two registration modes \"Accurate\" and \"Fast\" involve performing bidirectional registration and affect the stopping criteria internally used by the program.\"  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Subsample factor",
                    "description": "\"The registration will be calculated using subsampled versions of the images but the results will be applied to the original ones. The image subsampling parameter can be chosen between 0 and 7, i.e. the image dimensions can be reduced by a factor of 2^0 = 1 to 2^7 = 128. This is very useful when registering large images.\"  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Initial deformation mode",
                    "description": "\"Determines the level of detail of the initial deformation. In bUnwarpJ this is defined by the number of B-splines used to represent the deformations:<br><br><table style=\"font:sans-serif;font-size:12\"><tr><th>Deformation<\/th><th>Num. intervals in grid<\/th><\/tr><tr><td>Very coarse<\/td><td>1x1<\/td><\/tr><tr><td>Coarse<\/td><td>2x2<\/td><\/tr><tr><td>Fine<\/td><td>3x3<\/td><\/tr><tr><td>Very fine<\/td><td>8x8<\/td><\/tr><tr><td>Super fine<\/td><td>16x16<\/td><\/tr><\/table><font face=\"sans-serif\" size=\"3\"><br>If images start very far away from the right alignment, it is usually a good idea to go from \"Very Coarse\" to \"Very Fine\". If they start close to the right alignment, using a very coarse initial deformation could cause the algorithm to fail. So, in that case, it would be enough to set initial deformation to \"Fine\" and final deformation to \"Very Fine\". Use \"Super Fine\" only when you need a very high level of accuracy, because it makes the algorithm quite slower depending on the image sizes.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Final deformation mode",
                    "description": "See description for \"Initial deformation mode\""
                  },
                  {
                    "name": "Divergence weight",
                    "description": "Regularizes the deformation by penalizing the divergence of the deformation vector field.  If you see that your transformations get too rough, it is a good idea to use this parameter.  A value of 0.1 is usually good if there's no prior knowledge about the deformation shape.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Curl weight",
                    "description": "Regularizes the deformation by penalizing the curl of the deformation vector field.  If you see that your transformations get too rough, it is a good idea to use this parameter.  A value of 0.1 is usually good if there's no prior knowledge about the deformation shape.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Landmark weight",
                    "description": "Forces the deformations to fit the landmark points.  Set it to 1.0 unless you're not using landmarks.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Image weight",
                    "description": "The weight to control the pixel values difference.  Leave it at 1.0 unless you want to do, for instance, landmark-only registration.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Consistency weight",
                    "description": "Forces the resulting deformations to be one (source to target) as close as possible to the inverse of the other one (target to source). Values between 10.0 and 30.0 usually work fine. It is only taken into account for registration modes \"Fast\" or \"Accurate\".  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Stop threshold",
                    "description": "Stops the optimization process at each multiresolution level when the error relative change is not larger than this threshold.  Description taken from <a href=\"https://imagej.net/BUnwarpJ\">https://imagej.net/BUnwarpJ<\/a>"
                  },
                  {
                    "name": "Point selection mode",
                    "description": "The source for points to be used in calculation of image registration:<br><ul><li>\"Pre-selected points\" Points have been previously-selected on the input images as multi-point ROIs.  These points are passed directly into the registration calculation.  This negates the need for user-interaction at runtime.<\/li><li>\"Select at runtime\" Points must be manually-selected by the user at analysis runtime.  The two images to be aligned are displayed and a dialog box opens to allow selection of point pairs.  Point pairs must be added in the same order on each image.  For images where multiple slices/timepoints need to be registered, image pairs will be opened sequentially, with the point selections from the previous slice/timepoint being pre-selected for convenience.<\/li><\/ul>"
                  }
                ],
                "slug": "unwarp-(manual)"
              }
            ]
          }],
          "modules": [
            {
              "path": "/modules/images/transform/channel-extractor",
              "deprecated": true,
              "name": "Channel extractor",
              "shortDescription": "DEPRECATED: Please use ExtractSubstack module.",
              "fullDescription": "DEPRECATED: Please use ExtractSubstack module.<br><br> Extracts a single channel from a stack and stores it as a new image in the workspace.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Multi-channel image to extract a channel from.  This image is unaffected by the extraction process (i.e. its channel count doesn't decrease by 1)."
                },
                {
                  "name": "Output image",
                  "description": "The extracted channel will be stored in the workspace under this name."
                },
                {
                  "name": "Channel to extract (>= 1)",
                  "description": "Channel index to extract.  Index numbering starts at 1 and must be specified as a single integer value."
                }
              ],
              "slug": "channel-extractor"
            },
            {
              "path": "/modules/images/transform/concatenate-stacks",
              "deprecated": false,
              "name": "Concatenate stacks",
              "shortDescription": "Combine two or more image stacks into a single stack.",
              "fullDescription": "Combine two or more image stacks into a single stack.  This module allows images to be combined along any of the axes X,Y,C,Z or T.<br><br>Note: Image stack dimensions and bit-depths must be compatible.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Add image",
                  "subParameters": [{
                    "name": "Input image",
                    "description": "Image for concatenation."
                  }],
                  "description": "Add another image for concatenation."
                },
                {
                  "name": "Allow missing images",
                  "description": "If enabled, the moduule can ignore any images specified for inclusion that aren't present in the workspace.  This is useful if an image's existence is dependent on optional modules."
                },
                {
                  "name": "Output image",
                  "description": "The resultant image of concatenation to be added to the workspace."
                },
                {
                  "name": "Axis mode",
                  "description": "Axis along which to concatenate input images."
                }
              ],
              "slug": "concatenate-stacks"
            },
            {
              "path": "/modules/images/transform/convert-3d-stack-(switch-z-and-t)",
              "deprecated": false,
              "name": "Convert 3D stack (switch Z and T)",
              "shortDescription": "Emsures 3D stacks (or 4D with multiple channels) are of the expected type (timeseries or Z-stack).",
              "fullDescription": "Emsures 3D stacks (or 4D with multiple channels) are of the expected type (timeseries or Z-stack).  This module verifies the singular dimension of a 3D stack is correct for the specified output type (e.g. single slice when dealing with timeseries).  Any stacks which are not in the expected order have their T and Z axes swapped.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from workspace to test T and Z inversion of."
                },
                {
                  "name": "Apply to input image",
                  "description": "When selected, the post-operation image will overwrite the input image in the workspace.  Otherwise, the image will be saved to the workspace with the name specified by the \"Output image\" parameter."
                },
                {
                  "name": "Output image",
                  "description": "If \"Apply to input image\" is not selected, the post-operation image will be saved to the workspace with this name."
                },
                {
                  "name": "Mode",
                  "description": "Controls the expected stack output type.  Any input stacks which do not match this format are updated to give the expected order.  Choices are: Output timeseries, Output Z-stack."
                }
              ],
              "slug": "convert-3d-stack-(switch-z-and-t)"
            },
            {
              "path": "/modules/images/transform/crop-image",
              "deprecated": false,
              "name": "Crop image",
              "shortDescription": "Crop an image in X and Y using pre-defined limits or limits based on the extents of objects in a collection.",
              "fullDescription": "Crop an image in X and Y using pre-defined limits or limits based on the extents of objects in a collection.  Any pixels outside the specified limits are discarded.<br><br>Note: The x-min, y-min, width and height limits used here are in the same order and format as those output by ImageJ's default rectangle region of interest tool (i.e. displayed in the status bar of the ImageJ control panel).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from workspace to apply crop process to."
                },
                {
                  "name": "Apply to input image",
                  "description": "Select if the crop should be applied directly to the input image, or if it should be applied to a duplicate, then stored as a different image in the workspace."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output image created during the cropping process if storing the cropped image as a new image in the workspace (\"Apply to input image\" parameter)."
                },
                {
                  "name": "Limits mode",
                  "description": "Controls how the limits for the cropped region are specified:<br><ul><li>\"Fixed values\" The input image will be cropped to the region specified by the fixed values, \"Left coordinate\", \"Top coordinate\", \"Width\" and \"Height\".<\/li><li>\"Object collection limits\" The input image will be cropped to the region corresponding to the limits of the object collection specified by \"Input objects\"<\/li><\/ul>"
                },
                {
                  "name": "Left coordinate",
                  "description": "Left crop coordinate.  All pixels with x-coordinates lower than this will be removed.  Specified in pixel units with indexing starting at 0."
                },
                {
                  "name": "Top coordinate",
                  "description": "Top crop coordinate.  All pixels with y-coordinates lower than this will be removed.  Specified in pixel units with indexing starting at 0."
                },
                {
                  "name": "Width",
                  "description": "Width (number of columns) of the output cropped region.  Specified in pixel units."
                },
                {
                  "name": "Height",
                  "description": "Height (number of rows) of the output cropped region.  Specified in pixel units."
                },
                {
                  "name": "Input objects",
                  "description": "When \"Limits mode\" is set to \"Object collection limits\", these are the objects that will be used to define the cropped region."
                }
              ],
              "slug": "crop-image"
            },
            {
              "path": "/modules/images/transform/extract-substack",
              "deprecated": false,
              "name": "Extract substack",
              "shortDescription": "Extract a substack from the specified input image in terms of channels, slices and frames.",
              "fullDescription": "Extract a substack from the specified input image in terms of channels, slices and frames.  The output image is saved to the workspace for use later on in the workflow.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from which the substack will be taken."
                },
                {
                  "name": "Output image",
                  "description": "Output substack image."
                },
                {
                  "name": "Selection mode",
                  "description": "Method for selection of substack dimension ranges.<br><br>- \"Fixed\" (default) will apply the pre-specified dimension ranges to the input image.<br><br>- \"Manual\" will display a dialog asking the user to select the dimension ranges at runtime.  Each dimension (channel, slice or frame) can be fixed (i.e. not presented as an option to the user) using the \"enable\" toggles."
                },
                {
                  "name": "Channels",
                  "description": "Channel range to be extracted from the input image.  If \"Selection mode\" is set to \"Fixed\" this will be applied to each image.  If \"Selection mode\" is set to \"Manual\" this will be the default value present to the user, but can be changed for the final extraction."
                },
                {
                  "name": "Slices",
                  "description": "Slice range to be extracted from the input image.  If \"Selection mode\" is set to \"Fixed\" this will be applied to each image.  If \"Selection mode\" is set to \"Manual\" this will be the default value present to the user, but can be changed for the final extraction."
                },
                {
                  "name": "Frames",
                  "description": "Frame range to be extracted from the input image.  If \"Selection mode\" is set to \"Fixed\" this will be applied to each image.  If \"Selection mode\" is set to \"Manual\" this will be the default value present to the user, but can be changed for the final extraction."
                },
                {
                  "name": "Enable channels selection",
                  "description": "If enabled, the user will be able to specify the final channel range to be used in the substack extraction."
                },
                {
                  "name": "Enable slices selection",
                  "description": "If enabled, the user will be able to specify the final slice range to be used in the substack extraction."
                },
                {
                  "name": "Enable frames selection",
                  "description": "If enabled, the user will be able to specify the final frame range to be used in the substack extraction."
                }
              ],
              "slug": "extract-substack"
            },
            {
              "path": "/modules/images/transform/flip-stack",
              "deprecated": false,
              "name": "Flip stack",
              "shortDescription": "Flips the order of slices in stack.",
              "fullDescription": "Flips the order of slices in stack.  This operation can be performed on the channel, time or Z axis.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to process."
                },
                {
                  "name": "Apply to input image",
                  "description": "If selected, the flipped image will replace the input image in the workspace.  All measurements associated with the input image will be transferred to the flipped image."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output flipped image."
                },
                {
                  "name": "Axis mode",
                  "description": "Axis along which to flip the image."
                }
              ],
              "slug": "flip-stack"
            },
            {
              "path": "/modules/images/transform/focus-stack-(global)",
              "deprecated": false,
              "name": "Focus stack (global)",
              "shortDescription": "Extract a Z-substack from an input stack based on either manually-selected slices, or an automatically-calculated best-focus slice.",
              "fullDescription": "Extract a Z-substack from an input stack based on either manually-selected slices, or an automatically-calculated best-focus slice.  For automated methods, best focus is determined using intensity statistics (e.g. largest variance) of all pixels in each slice.  When in manual mode, only the best focus slice for the first and last timepoints need be specified (all others will be estimated using polynomial spline interpolation); however, more frames can be specified if preferred.<br><br>Irrespective of the calculation method (manual or automatic), it's possible to extract a fixed number of slices above and below the determined best-focus slice.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to extract substack from."
                },
                {
                  "name": "Output mode",
                  "description": "Controls whether the best focus positions are calculated and applied (creating a new image) or simply calculated.  In both cases, statistics for the best focus position (mean, median, minimum, maximum and standard deviation of slices) are stored as measurements associated with the input image."
                },
                {
                  "name": "Output image",
                  "description": "Substack image to be added to the current workspace."
                },
                {
                  "name": "Best-focus calculation",
                  "description": "Method for determining the best-focus slice.<br><ul><li>\"Manual\" Displays a control window, allowing the user to specify reference slices.  These slices should be at the same true z-plane.  Once complete, a substack will be extracted a specific number of slices above and below the reference plane (defined by \"Relative start slice\" and \"Relative end slice\".  If references aren't specific for all timepoints, the missing frames will be estimated using polynomial spline interpolation.<\/li><li>\"Smallest mean intensity\" The reference slice is taken as the slice with the minimum mean intensity.<\/li><li>\"Largest mean intensity\" The reference slice is taken as the slice with the maximum mean intensity.<\/li><li>\"Smallest standard deviation\" The reference slice is taken as the slice with the minimum intensity standard deviation.<\/li><li>\"Largest standard deviation\" The reference slice is taken as the slice with the maximum intensity standard deviation.<\/li><\/ul>"
                },
                {
                  "name": "Relative start slice",
                  "description": "Index of start slice relative to determined best-focus slice (i.e. -5 is 5 slices below the best-focus)."
                },
                {
                  "name": "Relative end slice",
                  "description": "Index of end slice relative to determined best-focus slice (i.e. 5 is 5 slices above the best-focus)."
                },
                {
                  "name": "Reference image",
                  "description": "If using manual selection of best focus slices, this is the image that will be shown to the user.  While it doesn't need to be the input image (the one the output substack will be generated from), it must have the same number of slices and timepoints as the input."
                },
                {
                  "name": "Calculation source",
                  "description": "When using automatic best focus slice determination this controls the image source:<br><ul><li>\"External\" The image for which intensity statistics are calculated is different to the image that the final substack will be created from.  For example, this could be an filtered version of the input image to enhance structures when in focus.<\/li><li>\"Internal\" The same image will be used for determination of the best slice and generation of the output substack.<\/li><\/ul>"
                },
                {
                  "name": "External source",
                  "description": "If using a separate image to determine the best focus slice (\"Calculation source\" set to \"External\"), this is the image that will be used for that calculation."
                },
                {
                  "name": "Channel mode",
                  "description": "How many channels to use when calculating the best-focus slice.  \"Use all channels\" will use all channels, whereas \"Use single channel\" will base the calculation on a single, user-defined channel."
                },
                {
                  "name": "Channel",
                  "description": "Channel to base the best-focus calculation on."
                },
                {
                  "name": "Smooth timeseries",
                  "description": "Apply median filter to best focus slice index over time.  This should smooth the transitions over time (prevent large jumps between frames)."
                },
                {
                  "name": "Smoothing range (odd numbers)",
                  "description": "Number of frames over which to calculate the median.  If the specified number is even it will be increased by 1."
                }
              ],
              "slug": "focus-stack-(global)"
            },
            {
              "path": "/modules/images/transform/focus-stack-(local)",
              "deprecated": false,
              "name": "Focus stack (local)",
              "shortDescription": "Focuses a Z-stack into a single plane using the StackFocuser ImageJ plugin.",
              "fullDescription": "Focuses a Z-stack into a single plane using the StackFocuser ImageJ plugin.  Best focus position is determined at each 2D pixel location, with the final image being comprised of the pixels from the slice with the best focus at that location.  Each channel and timepoint is focused separately.  Prior to application, the focus map can be median filtered to remove outliers.  Height maps can be stored and used in additional \"Focus stack (local)\" instances, thus allowing height maps to be edited prior to use.<br><br>Uses the <a href=\"https://imagej.nih.gov/ij/plugins/download/Stack_Focuser_.java\">StackFocuser<\/a> plugin created by Mikhail Umorin (source code downloaded on 06-June-2018).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image stack from the workspace which will be focused into a single plane."
                },
                {
                  "name": "Output mode",
                  "description": ""
                },
                {
                  "name": "Output focused image",
                  "description": "Output focused image which will be added to the workspace.  This image will have the same number of channels and timepoints as the input image, but will always only have a single Z-slice."
                },
                {
                  "name": "Output height image",
                  "description": ""
                },
                {
                  "name": "Use existing height image",
                  "description": "When selected, the height map image will be loaded from the workspace (\"Input height image\" parameter) rather than being calculated based on the input image."
                },
                {
                  "name": "Input height image",
                  "description": "The name of the height map image in the workspace if the height map has been pre-determined."
                },
                {
                  "name": "Range",
                  "description": "If calculating a new height image (\"Use existing height image\" parameter isn't selected), the best focus slice at each pixel will be based on pixel intensities within this range (specified in pixel units)."
                },
                {
                  "name": "Smooth height map",
                  "description": "When selected, the height map will be passed through a 2D median filter (range specified by \"Range\" parameter) to remove outliers."
                }
              ],
              "slug": "focus-stack-(local)"
            },
            {
              "path": "/modules/images/transform/interpolate-z-axis",
              "deprecated": false,
              "name": "Interpolate Z axis",
              "shortDescription": "Interpolates Z-axis of image to match XY spatial calibration",
              "fullDescription": "Interpolates Z-axis of image to match XY spatial calibration",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input image to which the Z-axis interpolation will be applied."
                },
                {
                  "name": "Output image",
                  "description": "Output image with Z-axis interpolation applied.  This image will be stored in the workspace and be accessible using this name."
                },
                {
                  "name": "Interpolation mode",
                  "description": "Controls how interpolated pixel values are calculated."
                }
              ],
              "slug": "interpolate-z-axis"
            },
            {
              "path": "/modules/images/transform/merge-channels",
              "deprecated": true,
              "name": "Merge channels",
              "shortDescription": "DEPRECATED: This Module has been superseeded by the more generalised \"Concatenate stacks\" Module.",
              "fullDescription": "DEPRECATED: This Module has been superseeded by the more generalised \"Concatenate stacks\" Module.  It will be removed in a future release.<br><br>Combines image stacks as different channels.  Output is automatically converted to a composite image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Add image",
                  "subParameters": [{
                    "name": "Input image",
                    "description": "Image from workspace to add to output merged image."
                  }],
                  "description": "Add another image to be included in output merged image.  All added images must have the same X,Y,Z and T dimensions."
                },
                {
                  "name": "Overwrite mode",
                  "description": "Controls where the output image is stored:<br><ul><li>\"Create new image\" Stores the merged image as a new image in the workspace with the name specified by \"Output image\".<\/li><li>\"Overwrite image\" Overwrite the image specified by the index \"Image index to overwrite (>= 1)\" in the workspace with the merged image.<\/li><\/ul>"
                },
                {
                  "name": "Output image",
                  "description": "Name for the output merged image to be stored in the workspace with."
                },
                {
                  "name": "Image index to overwrite (>= 1)",
                  "description": "If overwriting one of the input images, the image specified by this index (numbering starting at 1) will be overwritten."
                }
              ],
              "slug": "merge-channels"
            },
            {
              "path": "/modules/images/transform/project-image",
              "deprecated": false,
              "name": "Project image",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Output image",
                  "description": ""
                },
                {
                  "name": "Projection axis",
                  "description": ""
                },
                {
                  "name": "Projection mode",
                  "description": ""
                }
              ],
              "slug": "project-image"
            },
            {
              "path": "/modules/images/transform/replace-image",
              "deprecated": false,
              "name": "Replace image",
              "shortDescription": "This module duplicates an image into another, existing, image.",
              "fullDescription": "This module duplicates an image into another, existing, image.  \nThis is useful when dealing with optional modules, where a specific input is required later on.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image 1 (to be replaced)",
                  "description": "Pixel intensities for this image will be replaced."
                },
                {
                  "name": "Input image 2",
                  "description": "The image to copy pixel intensities from"
                }
              ],
              "slug": "replace-image"
            },
            {
              "path": "/modules/images/transform/scale-stack",
              "deprecated": false,
              "name": "Scale stack",
              "shortDescription": "Applies independent X,Y and Z-axis scaling to an input image.",
              "fullDescription": "Applies independent X,Y and Z-axis scaling to an input image.  Output dimensions can be specified explicitly, matched to another image in the workspace or calculated with a scaling factor.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image to process."
                },
                {
                  "name": "Output image",
                  "description": "Name of the output scaled image."
                },
                {
                  "name": "Interpolation mode",
                  "description": "Controls how interpolated pixel values are calculated.  Choices are: None,Bicubic,Bilinear"
                },
                {
                  "name": "Scale mode (x-axis)",
                  "description": "Controls how the output x-axis resolution is calculated:<br><ul><li>\"Fixed resolution\" The output image will have the specific resolution defined by the \"Resolution (x-axis)\" parameter.<\/li><li>\"Match image\" The output image will have the same resolution as the image specified by the \"Image (x-axis)\" parameter.<\/li><li>\"Scale factor\" The output image will have an equal resolution to the input resolution multiplied by the scaling factor, \"Scale factor (x-axis)\".  The output resolution will be rounded to the closest whole number.<\/li><\/ul>"
                },
                {
                  "name": "Resolution (x-axis)",
                  "description": "If \"Scale mode (x-axis)\" is set to \"Fixed resolution\", this is the resolution in the output image."
                },
                {
                  "name": "Image (x-axis)",
                  "description": "If \"Scale mode (x-axis)\" is set to \"Match image\", the output image will have the same resolution as this image."
                },
                {
                  "name": "Adopt calibration (x-axis)",
                  "description": "If \"Scale mode (x-axis)\" is set to \"Match image\", and this is selected, the output image's x-axis calibration will be copied from the input image."
                },
                {
                  "name": "Scale factor (x-axis)",
                  "description": "If \"Scale mode (x-axis)\" is set to \"Scale factor\", the output image will have a resolution equal to the input resolution multiplied by this scale factor.  The applied resolution will be rounded to the closest whole number"
                },
                {
                  "name": "Scale mode (y-axis)",
                  "description": "Controls how the output y-axis resolution is calculated:<br><ul><li>\"Fixed resolution\" The output image will have the specific resolution defined by the \"Resolution (y-axis)\" parameter.<\/li><li>\"Match image\" The output image will have the same resolution as the image specified by the \"Image (y-axis)\" parameter.<\/li><li>\"Scale factor\" The output image will have an equal resolution to the input resolution multiplied by the scaling factor, \"Scale factor (y-axis)\".  The output resolution will be rounded to the closest whole number.<\/li><\/ul>"
                },
                {
                  "name": "Resolution (y-axis)",
                  "description": "If \"Scale mode (y-axis)\" is set to \"Fixed resolution\", this is the resolution in the output image."
                },
                {
                  "name": "Image (y-axis)",
                  "description": "If \"Scale mode (y-axis)\" is set to \"Match image\", the output image will have the same resolution as this image."
                },
                {
                  "name": "Adopt calibration (y-axis)",
                  "description": "If \"Scale mode (y-axis)\" is set to \"Match image\", and this is selected, the output image's y-axis calibration will be copied from the input image."
                },
                {
                  "name": "Scale factor (y-axis)",
                  "description": "If \"Scale mode (y-axis)\" is set to \"Scale factor\", the output image will have a resolution equal to the input resolution multiplied by this scale factor.  The applied resolution will be rounded to the closest whole number"
                },
                {
                  "name": "Scale mode (z-axis)",
                  "description": "Controls how the output z-axis resolution (number of slices) is calculated:<br><ul><li>\"Fixed resolution\" The output image will have the specific number of slices defined by the \"Resolution (z-axis)\" parameter.<\/li><li>\"Match image\" The output image will have the same number of slices as the image specified by the \"Image (z-axis)\" parameter.<\/li><li>\"Scale factor\" The output image will have an equal number of slices to the input number of slices multiplied by the scaling factor, \"Scale factor (z-axis)\".  The output number of slices will be rounded to the closest whole number.<\/li><\/ul>"
                },
                {
                  "name": "Resolution (z-axis)",
                  "description": "If \"Scale mode (z-axis)\" is set to \"Fixed resolution\", this is the number of slices in the output image."
                },
                {
                  "name": "Image (z-axis)",
                  "description": "If \"Scale mode (z-axis)\" is set to \"Match image\", the output image will have the same number of slices as there are in this image."
                },
                {
                  "name": "Adopt calibration (z-axis)",
                  "description": "If \"Scale mode (z-axis)\" is set to \"Match image\", and this is selected, the output image's z-axis calibration will be copied from the input image."
                },
                {
                  "name": "Scale factor (z-axis)",
                  "description": "If \"Scale mode (z-axis)\" is set to \"Scale factor\", the output image will have a number of slices equal to the input number of slices multiplied by this scale factor.  The applied number of slices will be rounded to the closest whole number"
                }
              ],
              "slug": "scale-stack"
            }
          ]
        }
      ],
      "modules": []
    },
    {
      "path": "/modules/input-output",
      "name": "Input output",
      "description": "Modules loading or saving files to the filesystem.",
      "slug": "input-output",
      "subCategories": [],
      "modules": [
        {
          "path": "/modules/input-output/export-acc-dataset",
          "deprecated": false,
          "name": "Export ACC dataset",
          "shortDescription": "Exports objects and associated measurements in the Advanced Cell Classifier (ACC) format.",
          "fullDescription": "Exports objects and associated measurements in the Advanced Cell Classifier (ACC) format.  The output dataset can be loaded into ACC, where machine learning can be applied to classify objects in a GUI-based environment.  This module allows specific measurements to be exported from all those associated with the input objects.  For more information on the format, please visit the <a href=\"https://www.cellclassifier.org/\">Advanced Cell Classifier documentation<\/a>.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input objects",
              "description": "Objects for which an ACC dataset will be generated.  A summary of each object is output along with the specified measurements in a format that can be loaded into ACC."
            },
            {
              "name": "Input raw image",
              "description": "Raw image from which objects were ultimately detected.  This image doesn't need to show the object selections, but will be displayed in ACC with an optional overlay variant (specified by \"Input overlay image\").  As such, the objects themselves should be visible.  Typically this would be a fluorescence or brightfield image."
            },
            {
              "name": "Input overlay image",
              "description": "Equivalent image to that specified by \"Input raw image\", but with an overlay showing the input objects.  The overlay will be automatically flattened onto this image prior to saving."
            },
            {
              "name": "Root dataset folder",
              "description": "Root folder where the output ACC dataset will be stored."
            },
            {
              "name": "Plate name",
              "description": "Metadata item associated with the input image that corresponds to the plate name from which the image was taken.  Note: The ACC format requires objects and images to be organised in a plate-based format; however, for single image samples, this text value could be set to the filename."
            },
            {
              "name": "Row letter",
              "description": "Metadata item associated with the input image that corresponds to the row letter for the plate well from which the image was taken.  Note: The ACC format requires objects and images to be organised in a plate-based format; however, for single image samples, this text value could be set to the series number."
            },
            {
              "name": "Column number",
              "description": "Metadata item associated with the input image that corresponds to the column number for the plate well from which the image was taken.  Note: The ACC format requires objects and images to be organised in a plate-based format; however, for single image samples, this numeric value could be set to the series number."
            },
            {
              "name": "Show measurement selection",
              "description": "When selected, the available measurements are displayed in the MIA interface.  This allows only the relevant measurements associated with the objects to be used in ACC."
            },
            {
              "name": "Measurements",
              "description": "If \"Show measurement selection\" is selected, all measurements associated with the input objects will be shown in the MIA interface, along with tickboxes that can be used to mark those for inclusion in the ACC dataset."
            }
          ],
          "slug": "export-acc-dataset"
        },
        {
          "path": "/modules/input-output/extract-metadata",
          "deprecated": false,
          "name": "Extract metadata",
          "shortDescription": "Metadata values can be extracted from a variety of sources and assigned to the current workspace.",
          "fullDescription": "Metadata values can be extracted from a variety of sources and assigned to the current workspace.  These metadata values can subsequently be accessed in the form M{[NAME]}, where [NAME] is the metadata name.  Some common file and foldername formats are included as pre-defined metadata extraction methods, while other forms can be constructed using regular expressions.",
          "version": "1.0.1",
          "parameters": [
            {
              "name": "Extractor mode",
              "description": "Data source for metadata extraction:<br><ul><li>\"Filename\" Metadata taken from filename (not including folder path)<\/li><li>\"Foldername\" Metadata taken from parent foldername (incuding full system path to that folder)<\/li><li>\"Metadata file\" Metadata taken from a separate file, specified by the \"Metadata file\" parameter<\/li><li>\"Series name\" Metadata taken from seriesname (if not from a multi-series file, this is just the filename)<\/li><\/ul>"
            },
            {
              "name": "Filename extractor",
              "description": "The format of the filename to be converted to metadata values:<br><ul><li>\"Generic\" Name format is compiled using regular expressions defined in the \"Pattern\" parameter.  Each group (pattern enclosed in parenthesis) specified in the pattern is assigned to a metadata value.  Metadata value names are defined by the comma-separated list defined in \"Groups (comma separated)\".<\/li><li>\"CV1000 filename\" The Yokogawa CellVoyager CV1000 format (e.g. W1F001T0001Z00C1.tif)<\/li><li>\"CV7000 filename\" The Yokogawa CellVoyager CV7000 format (e.g. AssayPlate_Greiner_#655090_C02_T0001F001L01A01Z01C01.tif)<\/li><li>\"IncuCyte long filename\" The Incucyte long format, where each timepoint is stored as a separate image file and accordingly the filename records the time of acquisition (e.g. MySample1_A1_1_2021y08m06d_11h39m.tif)<\/li><li>\"IncuCyte short filename\" The Incucyte short format, where all timepoints are stored in a single file and the filename only records the well and field (e.g. MySample1_A1_1.tif)<\/li><li>\"Opera filename\" The Perkin Elmer Opera LX name format, which specifies row, column and field (e.g. 001001001.flex)<\/li><\/ul>"
            },
            {
              "name": "Foldername extractor",
              "description": "The format of the foldername to be converted to metadata values:<br><ul><li>\"<li>\"Generic\" Name format is compiled using regular expressions defined in the \"Pattern\" parameter.  Each group (pattern enclosed in parenthesis) specified in the pattern is assigned to a metadata value.  Metadata value names are defined by the comma-separated list defined in \"Groups (comma separated)\".<\/li><li>\"CV1000 foldername\" The Yokogawa CV1000 format (e.g. 20210806T113905_10x_K01_MySample1)<\/li><li>\"Opera measurement foldername\" The Perkin Elmer Opera LX foldername format (e.g. Meas_01(2021-08-06_11-39-05))<\/li><\/ul>"
            },
            {
              "name": "Metadata file extractor",
              "description": "The format of the metadata file to be converted to metadata values:<br><ul><li>\"CSV file\" Metadata values stored in a two-column CSV file, where the first column defines an identifier to the current file being processed (e.g. filename, or series name) and the second column defines a value that will be assigned as metadata.  Optionally, this value can be split into multiple metadata values using a regular expression.<\/li><li>\"Opera file (.flex)\" Specifically extracts the \"Area name\" property from the current .flex file.<\/li><\/ul>"
            },
            {
              "name": "Input source",
              "description": "If extracting metadata from a CSV file, this controls whether a single, static CSV file is used or whether there is one provided (with a fixed name) in the current folder (i.e where the current image file is loaded from)."
            },
            {
              "name": "Metadata file",
              "description": "If extracting metadata from a static CSV file (i.e. the same file for all processed images), this is the path to that metadata CSV file."
            },
            {
              "name": "Metadata file name",
              "description": "If extracting metadata from a dynamic CSV file (i.e. the file is in the current image folder), this is the name of that metadata CSV file (it must be the same name in all folders)."
            },
            {
              "name": "Metadata item to match",
              "description": "For CSV-based metadata extraction, the first column of the CSV file identifies which row to read.  This parameter defines the source (e.g. filename, series name, etc.).  Choices are: File in input folder,Static file"
            },
            {
              "name": "Pattern",
              "description": "Regular expression pattern to use when interpreting generic metadata formats.  This pattern must contain at least one group (specified using standard regex parenthesis notation)."
            },
            {
              "name": "Groups (comma separated)",
              "description": "When interpreting generic metadata formats, these group names will be assigned to each group matched using regular expressions.  The metadata values will subsequently be accessed via these names in the form M{[NAME]}, where [NAME] is the group name."
            },
            {
              "name": "Case insensitive",
              "description": "When selected regular expression matches will be found irrespective of case."
            },
            {
              "name": "Show pattern matching test",
              "description": "When selected (and constructing a regular expression extractor), an example string can be provided and the identified groups displayed.  This allows for regular expression forms to be tested during workflow assembly."
            },
            {
              "name": "Example string",
              "description": "If testing a regular expression form (\"Show pattern matching test\" selected), this is the example string which will be processed and broken down into its individual metadata values."
            },
            {
              "name": "Identified groups",
              "description": "If testing a regular expression form (\"Show pattern matching test\" selected), the extracted metadata values will be displayed here."
            },
            {
              "name": "Split using regular expressions",
              "description": "When selected, the metadata value taken from a CSV file will itself be broken down into multiple metadata values using a regular expressions"
            },
            {
              "name": "Metadata value name",
              "description": "If a metadata value loaded from CSV is not to be sub-divided into multiple metadata values, the entire value loaded will be stored as a metadata item with this name."
            }
          ],
          "slug": "extract-metadata"
        },
        {
          "path": "/modules/input-output/load-image",
          "deprecated": false,
          "name": "Load image",
          "shortDescription": "Load image into MIA workspace.",
          "fullDescription": "Load image into MIA workspace.  This module can be configured to import images from a variety of locations (selected using the \"Import mode\" control).",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Output image",
              "description": "Name assigned to the image."
            },
            {
              "name": "Import mode",
              "description": "Controls where the image will be loaded from:<br><ul><li>\"Current file\" (default option) will import the current root-file for the workspace (this is the file specified in the \"Input control\" module).<\/li><li>\"From ImageJ\" will load the active image fromm ImageJ.<\/li><li>\"Image sequence (alphabetical)\" will load a series of images matching a specified name format in alphabetical order.  The format of the names to be loaded is specified by the \"Sequence root name\" parameter.<\/li><li>\"Image sequence (zero-based)\" will load a series of images with numbered elements.  The format of the names to be loaded is specified by the \"Sequence root name\" parameter.<\/li><li>\"Matching format\" will load the image matching a filename based on the root-file for the workspace and a series of rules.<\/li><li>\"Specific file\" will load the image at the location specified by \"File path\".<\/li><\/ul>"
            },
            {
              "name": "Reader",
              "description": "Set the reader for importing the image:<br><ul><li>\"Bio-Formats\" will use the Bio-Formats plugin.  This is best for most cases (especially proprietary formats).<\/li><li>\"ImageJ\" will use the stock ImageJ file reader.<\/li><\/ul>"
            },
            {
              "name": "Sequence root name",
              "description": "Template filename for loading multiple image sequence files (those with names in the format \"image0001.tif\", \"image0002.tif\", \"image0003.tif\",etc.).  Template filenames are constructed in a generic manner, whereby metadata values stored in the workspace can be inserted into the name using the notation  \"M{name}\".  This allows names to be created dynamically for each analysis run.  The location in the filenam of the variable image number is specified using the \"Z{0000}\" notation, where the number of \"0\" characters specifies the number of digits.  It is also necessary to specify the filepath (input file filepath stored as metadata value \"M{Filepath}\".   <br><br>For example, loading the sequence \"image0001.tif\", etc. from the same folder as the input file would require the format \"M{Filepath}\\\\imageZ{0000}.tif\".  Note: Backslash characters specifying the folder path need to be double typed (standard Java formatting)."
            },
            {
              "name": "Name format",
              "description": "Method to use for generation of the input filename:<br><ul><li>\"Generic (from metadata)\" (default) will generate a name from metadata values stored in the current workspace.<\/li><li>\"Huygens\" will generate a name matching the SVI Huygens format, where channel numbers are specified as \"ch00\", \"ch01\", etc.<\/li><li>\"Incucyte short filename\" will generate a name matching the short Incucyte Zoom format.  The root name is specified as the parameter \"Comment\".<\/li><li>\"Yokogowa\" will generate a name matching the Yokogawa high content microscope name format.<\/li><\/ul>"
            },
            {
              "name": "Comment",
              "description": "Root name for generation of Incucyte Zoom filenames.  This will be added before the standard well and field values."
            },
            {
              "name": "Extension",
              "description": "Extension for the generated filename."
            },
            {
              "name": "Generic format",
              "description": "Format for a generic filename.  Plain text can be mixed with global variables or metadata values currently stored in the workspace.  Global variables are specified using the \"V{name}\" notation, where \"name\" is the name of the variable to insert.  Similarly, metadata values are specified with the \"M{name}\" notation."
            },
            {
              "name": "Include series number",
              "description": "Option to include the current series number when compiling filenames.  This may be necessary when working with multi-series files, as there will be multiple analyses completed for the same root file."
            },
            {
              "name": "File path",
              "description": "Path to file to be loaded."
            },
            {
              "name": "Series mode",
              "description": "Controls which series should be loaded for multiseries files (e.g. Leica LIF files):<br><ul><li>\"Current series\" will load the same series as the current root file (i.e. that selected via \"Input control\").<\/li><li>\"Specific series\" will load a specific series specified by the \"Series number\"parameter.<\/li><\/ul>"
            },
            {
              "name": "Series number",
              "description": "If a specific series is being loaded (\"Series mode\" set to \"Specific series\"), this is the series that will be used."
            },
            {
              "name": "Channels",
              "description": "Range of channels to be loaded for the current file.  These can be specified as a comma-separated list, using a range (e.g. \"4-7\" will load channels 4,5,6 and 7) or as a range loading every nth channel (e.g. \"4-10-2\" will load channels 4,6,8 and 10).  The \"end\" keyword will be converted to the total number of available channels at runtime."
            },
            {
              "name": "Slices",
              "description": "Range of slices to be loaded for the current file.  These can be specified as a comma-separated list, using a range (e.g. \"4-7\" will load slices 4,5,6 and 7) or as a range loading every nth slice (e.g. \"4-10-2\" will load slices 4,6,8 and 10).  The \"end\" keyword will be converted to the total number of available slices at runtime."
            },
            {
              "name": "Frames",
              "description": "Range of frames to be loaded for the current file.  These can be specified as a comma-separated list, using a range (e.g. \"4-7\" will load frames 4,5,6 and 7) or as a range loading every nth frame (e.g. \"4-10-2\" will load frames 4,6,8 and 10).  The \"end\" keyword will be converted to the total number of available frames at runtime."
            },
            {
              "name": "Channel",
              "description": "Channel to load when constructing a \"Yokogawa\" format name."
            },
            {
              "name": "Crop mode",
              "description": "Choice of loading the entire image, or cropping in XY:<br><ul><li>\"None\" (default) will load the entire image in XY.<\/li><li>\"Fixed\" will apply a pre-defined crop to the input image based on the parameters \"Left\", \"Top\",\"Width\" and \"Height\".<\/li><li>\"From reference\" will display a specified image and ask the user to select a region to crop the input image to.<\/li><li>\"Object collection limits\" will crop to the limits of the objects in the collection specified by \"Objects for limits\".<\/li><\/ul>"
            },
            {
              "name": "Reference image",
              "description": "The image to be displayed for selection of the cropping region if the cropping mode is set to \"From reference\"."
            },
            {
              "name": "Left coordinate",
              "description": "Left coordinate limit for image cropping (specified in pixel units)."
            },
            {
              "name": "Top coordinate",
              "description": "Top coordinate limit for image cropping (specified in pixel units)."
            },
            {
              "name": "Width",
              "description": "Width of the final cropped region (specified in pixel units)."
            },
            {
              "name": "Height",
              "description": "Height of the final cropped region (specified in pixel units)."
            },
            {
              "name": "Objects for limits",
              "description": "If \"Crop mode\" is set to \"Object collection limits\", this is the object collection that will define the limits of the cropped region."
            },
            {
              "name": "Scale mode",
              "description": "Controls if the input image is scaled upon importing.  This only works for scaling in X and Y (magnitudes determined by the \"X scale factor\" and \"Y scale factor\" parameters):<br><ul><li>\"Scaling (bicubic)\" Scales the input images using a bicubic filter.  This leads to smooth intensity transitions between interpolated pixels.<\/li><li>\"Scaling (bilinear)\" Scales the input images using a bilinear filter.  This leads to smooth intensity transitions between interpolated pixels.<\/li><li>\"No scaling\" (default) Input images are not scaled.  They are loaded into the workspace at their native resolutions.<\/li><li>\"Scaling (no interpolation)\" Scales the input images using a nearest-neighbour approach.  This leads to a \"blocky\" apppearance to scaled images.<\/li><\/ul>"
            },
            {
              "name": "X scale factor",
              "description": "Scale factor applied to X-axis of input images (if \"Scale mode\" is in an image scaling mode).  Values <1 will reduce image width, while values >1 will increase it."
            },
            {
              "name": "Y scale factor",
              "description": "Scale factor applied to Y-axis of input images (if \"Scale mode\" is in an image scaling mode).  Values <1 will reduce image height, while values >1 will increase it."
            },
            {
              "name": "Dimension mismatch mode",
              "description": "If loading an image sequence from separate files, this parameter controls how dimensions mismatches between images are handled:<br><ul><li>\"Crop (centred)\" The images will be centred-aligned and cropped to the smallest image dimensions.  This prevents empty borders around some images, but means information is lost from larger images.<\/li><li>\"Pad (centred)\" The images will be centre-aligned and padded to the largest image dimensions.  This prevents any image infomation being lost at edges, but means smaller images will have borders.<\/li><li>\"Disallow (fail upon mismatch)\" All images must have exactly the same dimensions.  If a dimension mismatch is detected, the image loading process will fail.<\/li><\/ul>"
            },
            {
              "name": "Pad intensity mode",
              "description": "If loading an image series from separate files and \"Dimension mismatch mode\" is set to \"Pad (centred)\" this parameter will control the intensity of the padding border around images smaller than the largest loaded image."
            },
            {
              "name": "Set manual spatial calibration",
              "description": "Option to use the automatically-applied spatial calibration or manually specify these values."
            },
            {
              "name": "XY calibration (dist/px)",
              "description": "Distance per pixel in the XY plane.  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Z calibration (dist/px)",
              "description": "Distance per slice (Z-axis).  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Set manual temporal calibration",
              "description": "Option to use the automatically-applied temporal calibration or manually specify this value."
            },
            {
              "name": "Frame interval (time/frame)",
              "description": "Time duration between the start of consecutive frames.  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Force bit depth",
              "description": "Enable to force the output image to adopt a specific bit-depth."
            },
            {
              "name": "Output bit depth",
              "description": "Output bit depth of the loaded image."
            },
            {
              "name": "Minimum input intensity",
              "description": "Minimum intensity in the input image when in the native bit depth.  This is used for scaling intensities to the desired bit depth."
            },
            {
              "name": "Maximum input intensity",
              "description": "Maximum intensity in the input image when in the native bit depth.  This is used for scaling intensities to the desired bit depth."
            }
          ],
          "slug": "load-image"
        },
        {
          "path": "/modules/input-output/load-objects",
          "deprecated": false,
          "name": "Load objects",
          "shortDescription": "Load centroid coordinates of pre-detected objects from file.",
          "fullDescription": "Load centroid coordinates of pre-detected objects from file.  Loaded objects are stored in a single object collection and are represented by a single coordinate point.  For example, this module could be used to import detections from another piece of software or from a previous analysis run.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Output objects",
              "description": "Objects loaded into the workspace will be stored with this name.  They will be accessible by subsequent modules using this name."
            },
            {
              "name": "Coordinate source",
              "description": "Controls where the coordinates for the output object collection will be loaded from:<br><ul><li>\"Current file\" (default option) will use the current root-file for the workspace (this is the file specified in the \"Input control\" module).<\/li><li>\"Matching format\" will load the coordinate file matching a filename based on the root-file for the workspace and a series of rules.<\/li><li>\"Specific file\" will load the coordinate file at the location specified by \"Input file\".<\/li><\/ul>"
            },
            {
              "name": "Name format",
              "description": "Method to use for generation of the input filename:<br><ul><li>\"Generic (from metadata)\" (default) will generate a name from metadata values stored in the current workspace.<\/li><li>\"Input filename with prefix\" will load a file with the same name as the input image, but with an additional prefix, specified by the \"Prefix\" parameter.<\/li><li>\"Input filename with suffix\" will load a file with the same name as the input image, but with an additional suffix, specified by the \"Suffix\" parameter.<\/li><\/ul>"
            },
            {
              "name": "Generic format",
              "description": "Format for a generic filename.  Plain text can be mixed with global variables or metadata values currently stored in the workspace.  Global variables are specified using the \"V{name}\" notation, where \"name\" is the name of the variable to insert.  Similarly, metadata values are specified with the \"M{name}\" notation."
            },
            {
              "name": "Prefix",
              "description": "Prefix to use when generating coordinate file filename in \"Input filename with prefix\" mode."
            },
            {
              "name": "Suffix",
              "description": "Suffix to use when generating coordinate file filename in \"Input filename with suffix\" mode."
            },
            {
              "name": "Extension",
              "description": "Extension for the generated filename."
            },
            {
              "name": "Include series number",
              "description": "Option to include the current series number when compiling filenames.  This may be necessary when working with multi-series files, as there will be multiple analyses completed for the same root file."
            },
            {
              "name": "Input file",
              "description": "Path to specific file to be loaded when \"Coordinate source\" is in \"Specific file\" mode."
            },
            {
              "name": "ID-column index",
              "description": "Index of column in input coordinates file specifying the object ID number."
            },
            {
              "name": "X-column index",
              "description": "Index of column in input coordinates file specifying the object x-centroid location (pixel units)."
            },
            {
              "name": "Y-column index",
              "description": "Index of column in input coordinates file specifying the object y-centroid location (pixel units)."
            },
            {
              "name": "Z-column index",
              "description": "Index of column in input coordinates file specifying the object z-centroid location (slice units)."
            },
            {
              "name": "Timepoint-column index",
              "description": "Index of column in input coordinates file specifying the timepoint the object appears in.  Timepoint numbering starts at 0."
            },
            {
              "name": "Limits source",
              "description": "Controls how the spatial limits (width, height, number of slices and number of timepoints) for the output object collection are defined:<br><ul><li>\"From image\" limits match the spatial and temporal dimensions of an image in the workspace (specified with the \"Limits reference image\" parameter).  This is equivalent to how spatial limits are determined when identifying objects directly from an image.<\/li><li>\"Manual\" limits are specified using the \"Width\", \"Height\", \"Number of slices\" and \"Number of timepoints\" parameters.<\/li><li>\"Maximum coordinate\" limits are determined from the maximum x,y,z coordinates and timepoint present in the loaded coordinate set.<\/li><\/ul>"
            },
            {
              "name": "Limits reference image",
              "description": "Image used to determine spatial and temporal limits of the output object collection if \"Limits source\" is set to \"From image\"."
            },
            {
              "name": "Width",
              "description": "Output object collection spatial width to be used if \"Limits source\" is set to \"Manual\".  Specified in pixel units."
            },
            {
              "name": "Height",
              "description": "Output object collection spatial height to be used if \"Limits source\" is set to \"Manual\".  Specified in pixel units."
            },
            {
              "name": "Number of slices",
              "description": "Output object collection number of slices (depth) to be used if \"Limits source\" is set to \"Manual\".  Specified in slice units."
            },
            {
              "name": "Number of timepoints",
              "description": "Output object collection number of frames to be used if \"Limits source\" is set to \"Manual\"."
            },
            {
              "name": "Frame interval",
              "description": "Time between adjacent frames if dealing with objects detected across multiple timepoints.  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Spatial calibration source",
              "description": "Controls how the spatial calibration for the output object collection are defined:<br><ul><li>\"From image\" spatial calibrations match those of an image in the workspace (specified with the \"Spatial calibration ref. image\" parameter).  This is equivalent to how calibrations are determined when identifying objects directly from an image.<\/li><li>\"Manual\" spatial calibrations are specified using the \"XY calibration (dist/px)\" and \"Z calibration (dist/px)\" parameters.<\/li><\/ul>"
            },
            {
              "name": "Spatial calibration ref. image",
              "description": "Image used to determine spatial calibrations of the output object collection if \"Spatial calibration source\" is set to \"From image\"."
            },
            {
              "name": "XY calibration (dist/px)",
              "description": "Distance per pixel in the XY plane.  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Z calibration (dist/px)",
              "description": "Distance per slice (Z-axis).  Units for this are specified in the main \"Input control\" module."
            },
            {
              "name": "Temporal calibration source",
              "description": "Controls how the temporal calibration for the output object collection is defined:<br><ul><li>\"From image\" temporal calibration matches that of an image in the workspace (specified with the \"Temporal calibration ref. image\" parameter).  This is equivalent to how calibrations are determined when identifying objects directly from an image.<\/li><li>\"Manual\" temporal calibration is specified using the \"Frame interval\" parameter.<\/li><\/ul>"
            },
            {
              "name": "Temporal calibration ref. image",
              "description": "Image used to determine the temporal calibration of the output object collection if \"Temporal calibration source\" is set to \"From image\"."
            },
            {
              "name": "Create parent objects",
              "description": "When selected, an output parent object collection can also be specified which allows objects to be linked.  These parent objectcs can only perform a linking function for the output objects; the parent objects themselves do not contain any coordinate information.  For example, the loaded parent objects could be tracks or clusters."
            },
            {
              "name": "Output parent objects name",
              "description": "Name of the output parent objects collection."
            },
            {
              "name": "Parent object ID index",
              "description": "Index of column in input coordinates file specifying the parent object ID number."
            }
          ],
          "slug": "load-objects"
        },
        {
          "path": "/modules/input-output/load-objects-from-rois",
          "deprecated": false,
          "name": "Load objects from ROIs",
          "shortDescription": "",
          "fullDescription": "",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Output objects",
              "description": ""
            },
            {
              "name": "Assign tracks",
              "description": ""
            },
            {
              "name": "Output track objects",
              "description": ""
            },
            {
              "name": "Reference image",
              "description": ""
            },
            {
              "name": "File path mode",
              "description": ""
            },
            {
              "name": "Generic file path",
              "description": ""
            },
            {
              "name": "Specific file path",
              "description": ""
            }
          ],
          "slug": "load-objects-from-rois"
        },
        {
          "path": "/modules/input-output/save-image",
          "deprecated": false,
          "name": "Save image",
          "shortDescription": "Save an image/stack from the workspace to file.",
          "fullDescription": "Save an image/stack from the workspace to file.  These files can be placed in the same folder as the input file, located in a specific folder or placed in a directory structure mirroring the input structure, but based at a new location.  For greater flexibility in output file locations and filenames, the \"Save image (generic)\" module can be used.  To prevent overwriting of previously-saved files, the current date and time can be appended to the end of each filename.  Images can be saved in a variety of formats (AVI, TIF and Zipped TIF).",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Save location",
              "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
            },
            {
              "name": "Mirrored directory root",
              "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
            },
            {
              "name": "File path",
              "description": "Path to folder where images will be saved."
            },
            {
              "name": "File name (generic)",
              "description": ""
            },
            {
              "name": "Save name mode",
              "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
            },
            {
              "name": "File name",
              "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
            },
            {
              "name": "Append series mode",
              "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
            },
            {
              "name": "Append date/time mode",
              "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
            },
            {
              "name": "Add filename suffix",
              "description": "A custom suffix to be added to each filename."
            },
            {
              "name": "Input image",
              "description": "Image to be saved to file."
            },
            {
              "name": "File format",
              "description": "The format the output image will be saved as:<br><ul><li>\"AVI\" Video written using the stock ImageJ \"<a href=\"https://github.com/imagej/imagej1/blob/master/ij/plugin/filter/AVI_Writer.java\">AVI Writer<\/a>\".  Videos can use different compression algorithms specified using \"Compression mode\".  Framerate specified by \"Frame rate (fps)\" parameter.<\/li><li>\"TIF\" Standard multidimensional (multi-page) TIF image saving.<\/li><li>\"ZIP\" TIF images stored using ZIP compression.  For images with large homogeneous regions of pixel intensity this can greatly reduce file size in a lossless manner.  Zipped images can be read directly back into ImageJ/Fiji without the need for prior decompression.<\/li><\/ul>"
            },
            {
              "name": "Channel mode",
              "description": "Control whether saved images should be in ImageJ \"Composite\" (display all channels simultaneously) or \"Color\" (display one channel at a time) mode."
            },
            {
              "name": "Save as RGB",
              "description": "Convert images to RGB prior to saving.  This is useful for displaying multi-channel images to a format that can be easily viewed outside ImageJ."
            },
            {
              "name": "Compression mode",
              "description": "Compression mode used when saving AVI videos (\"File format\" parameter):<br><ul><li>\"JPEG\" Lossy video compression with quality specified by \"Quality (0-100)\" parameter.  This option is good when reducing video size is more important than retaining perfect image quality.<\/li><li>\"None\" Frames are stored in their raw format (uncompressed).  This gives the highest quality, but also the largest file size.<\/li><li>\"PNG\" PNG video compression.<\/li><\/ul>"
            },
            {
              "name": "Quality (0-100)",
              "description": "Quality of output JPEG-compressed video (values in range 0-100).  For reference, saving AVIs via ImageJ's \"File > Save As...\" menu uses a quality of 90."
            },
            {
              "name": "Frame rate (fps)",
              "description": "Output video framerate (frames per second)."
            },
            {
              "name": "Flatten overlay",
              "description": "Flatten any overlay elements onto the image prior to saving."
            }
          ],
          "slug": "save-image"
        },
        {
          "path": "/modules/input-output/save-image-(generic)",
          "deprecated": true,
          "name": "Save image (generic)",
          "shortDescription": "Save an image/stack from the workspace to file.",
          "fullDescription": "Save an image/stack from the workspace to file.  Output file locations and filenames are generated from metadata items and fixed values.  This gives greater flexibility to output image locations than the standard \"Save image\" module.  To prevent overwriting of previously-saved files, the current date and time can be appended to the end of each filename.  Images can be saved in a variety of formats (AVI, TIF and Zipped TIF).",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input image",
              "description": "Image to be saved to file."
            },
            {
              "name": "Generic format",
              "description": "Format for a generic filename.  Plain text can be mixed with global variables or metadata values currently stored in the workspace.  Global variables are specified using the \"V{name}\" notation, where \"name\" is the name of the variable to insert.  Similarly, metadata values are specified with the \"M{name}\" notation."
            },
            {
              "name": "Append date/time mode",
              "description": ""
            },
            {
              "name": "File format",
              "description": "The format the output image will be saved as:<br><ul><li>\"AVI\" Video written using the stock ImageJ \"<a href=\"https://github.com/imagej/imagej1/blob/master/ij/plugin/filter/AVI_Writer.java\">AVI Writer<\/a>\".  Videos can use different compression algorithms specified using \"Compression mode\".  Framerate specified by \"Frame rate (fps)\" parameter.<\/li><li>\"TIF\" Standard multidimensional (multi-page) TIF image saving.<\/li><li>\"ZIP\" TIF images stored using ZIP compression.  For images with large homogeneous regions of pixel intensity this can greatly reduce file size in a lossless manner.  Zipped images can be read directly back into ImageJ/Fiji without the need for prior decompression.<\/li><\/ul>"
            },
            {
              "name": "Channel mode",
              "description": "Control whether saved images should be in ImageJ \"Composite\" (display all channels simultaneously) or \"Color\" (display one channel at a time) mode."
            },
            {
              "name": "Save as RGB",
              "description": "Convert images to RGB prior to saving.  This is useful for displaying multi-channel images to a format that can be easily viewed outside ImageJ."
            },
            {
              "name": "Compression mode",
              "description": "Compression mode used when saving AVI videos (\"File format\" parameter):<br><ul><li>\"JPEG\" Lossy video compression with quality specified by \"Quality (0-100)\" parameter.  This option is good when reducing video size is more important than retaining perfect image quality.<\/li><li>\"None\" Frames are stored in their raw format (uncompressed).  This gives the highest quality, but also the largest file size.<\/li><li>\"PNG\" PNG video compression.<\/li><\/ul>"
            },
            {
              "name": "Quality (0-100)",
              "description": "Quality of output JPEG-compressed video (values in range 0-100).  For reference, saving AVIs via ImageJ's \"File > Save As...\" menu uses a quality of 90."
            },
            {
              "name": "Frame rate (fps)",
              "description": "Output video framerate (frames per second)."
            },
            {
              "name": "Flatten overlay",
              "description": "Flatten any overlay elements onto the image prior to saving."
            }
          ],
          "slug": "save-image-(generic)"
        },
        {
          "path": "/modules/input-output/save-objects-as-rois",
          "deprecated": false,
          "name": "Save objects as ROIs",
          "shortDescription": "",
          "fullDescription": "",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Save location",
              "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
            },
            {
              "name": "Mirrored directory root",
              "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
            },
            {
              "name": "File path",
              "description": "Path to folder where images will be saved."
            },
            {
              "name": "File name (generic)",
              "description": ""
            },
            {
              "name": "Save name mode",
              "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
            },
            {
              "name": "File name",
              "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
            },
            {
              "name": "Append series mode",
              "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
            },
            {
              "name": "Append date/time mode",
              "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
            },
            {
              "name": "Add filename suffix",
              "description": "A custom suffix to be added to each filename."
            },
            {
              "name": "Input objects",
              "description": ""
            },
            {
              "name": "Add track ID",
              "description": ""
            },
            {
              "name": "Track objects",
              "description": ""
            },
            {
              "name": "File mode",
              "description": ""
            }
          ],
          "slug": "save-objects-as-rois"
        }
      ]
    },
    {
      "path": "/modules/objects",
      "name": "Objects",
      "description": "Modules dealing primarily with objects.  These include object detection, measurements and operations leading to changes in existing objects.",
      "slug": "objects",
      "subCategories": [
        {
          "path": "/modules/objects/convert",
          "name": "Convert",
          "description": "Operations which allow conversion between objects and images.",
          "slug": "convert",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/objects/convert/convert-image-to-objects",
              "deprecated": false,
              "name": "Convert image to objects",
              "shortDescription": "Converts objects encoded in a labelled image stack back into objects.",
              "fullDescription": "Converts objects encoded in a labelled image stack back into objects.  Each output object is comprised of all pixels in a single timepoint with the same pixel intensity.  As such, pixels need not be in direct contact to be assigned the same object.  For objects tracked through time, the image intensity can be interpreted as the ID of a parent track object.<br><br>Note: This module has different behaviour to the \"Identify objects\" module, which takes a binary image, identifies contiguous foreground regions and assigns new object IDs.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Labelled image to convert to objects.  The background (non-object region) of this image should be black (0 intensity) and all pixels corresponding to the same object should have the same value.  Objects can only exist in a single timepoint, so pixels of the same intensity but in different timepoints will be assigned to different objects."
                },
                {
                  "name": "Output objects",
                  "description": "Output objects created by the conversion process.  These will be stored in the workspace and be accessible via this name."
                },
                {
                  "name": "Volume type",
                  "description": "The method used to store pixel coordinates.  This only affects performance and memory usage, there is no difference in results obtained using difference storage methods.<br><ul><li>\"Pointlist\" (default) stores object coordinates as a list of XYZ coordinates.  This is most efficient for small objects, very thin objects or objects with lots of holes.<\/li><li>\"Octree\" stores objects in an octree format.  Here, the coordinate space is broken down into cubes of different sizes, each of which is marked as foreground (i.e. an object) or background.  Octrees are most efficient when there are lots of large cubic regions of the same label, as the space can be represented by larger (and thus fewer) cubes.  This is best used when there are large, completely solid objects.  If z-axis sampling is much larger than xy-axis sampling, it's typically best to opt for the quadtree method.<\/li><li>\"Quadtree\" stores objects in a quadtree format.  Here, each Z-plane of the object is broken down into squares of different sizes, each of which is marked as foreground (i.e. an object) or background.  Quadtrees are most efficient when there are lots of large square regions of the same label, as the space can be represented by larger (and thus fewer) squares.  This is best used when there are large, completely solid objects.<\/li><\/ul>"
                },
                {
                  "name": "Create track objects",
                  "description": "When selected, the intensity of the image for each object will be assumed as corresponding to the ID of a parent track object.  This allows track relationships for objects present in multiple timepoints to be loaded back into MIA."
                },
                {
                  "name": "Output track objects name",
                  "description": "If creating track objects (\"Create track objects\" is selected), this is the name of the output tracks.  These will be parents of the output objects."
                }
              ],
              "slug": "convert-image-to-objects"
            },
            {
              "path": "/modules/objects/convert/convert-objects-to-image",
              "deprecated": false,
              "name": "Convert objects to image",
              "shortDescription": "Creates an image showing all objects in a specified collection.",
              "fullDescription": "Creates an image showing all objects in a specified collection.  The value (intensity) of each pixel can be based on object (or relative) ID numbers as well as various metrics, such as measurements or relationship counts.  Output images will be 32-bit type, except when in \"Random colour\" or \"Single colour\" modes, which are 8-bit as the extra precision is not required.<br><br>Note: This output method is unable to correctly render overlapping objects (those with any matching coordinates); as such, the output image will show the result for one of objects for these coordinates.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Object collection to convert to an image.  All objects will be rendered onto the same output image."
                },
                {
                  "name": "Output image",
                  "description": "Image showing all objects in the input collection.  Note: This output method is unable to correctly render overlapping objects (those with any matching coordinates); as such, the output image will show the result for one of objects for these coordinates."
                },
                {
                  "name": "Output mode",
                  "description": "Controls what coordinates are used to represent each object.<br><ul><li>\"Object centroid\" Only the pixel closest to the centroid (mean XYZ coordinate) of each object is added to the output image.<\/li><li>\"Whole object\" All coordinates of each object are added to the output image.<\/li><\/ul>"
                },
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Single colour mode",
                  "description": "When \"Colour mode\" is set to \"Single colour\", the input objects will be converted to a binary image.  This parameter controls if the output image will have the logic \"Black objects, white background\" or \"White objects, black background\"."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                }
              ],
              "slug": "convert-objects-to-image"
            },
            {
              "path": "/modules/objects/convert/create-distance-map",
              "deprecated": false,
              "name": "Create distance map",
              "shortDescription": "Creates a distance map for a selected object set.",
              "fullDescription": "Creates a distance map for a selected object set.  Pixels in the output image are encoded with the distance to the nearest image edge or centroid (depending on setting).  A single distance map image is created for all objects in the specified set.  Uses the plugin \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\".",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from workspace for which distance map will be created.  A single distance map will be created for all objects."
                },
                {
                  "name": "Output image",
                  "description": "Output distance map image which will be added to the workspace.  This will contain the distance map for each object."
                },
                {
                  "name": "Reference mode",
                  "description": "Controls where the distances are calculated from:<br><ul><li>\"Distance from object centroid\" Each pixel is encoded with the distance from the centre of the respective object.<\/li><li>\"Distance from object edge\" Each pixel is encoded with the distance from the edge of the respective object.<\/li><\/ul>"
                },
                {
                  "name": "Invert map within objects",
                  "description": "When selected (and \"Reference mode\" is set to \"Distance from object edge\"), the distance map will be inverted, such that the distances inside objects are also positive.  If not selected, the distances inside objects will be negative.  Distance values outside objects are always positive."
                },
                {
                  "name": "Masking mode",
                  "description": "Controls which regions of the image are displayed:<br><ul><li>\"Inside and outside\" Distances both inside and outside the objects are non-zero.<\/li><li>\"Inside only\" Distances are shown inside each object, but are set to zero for all pixels outside an object.<\/li><li>\"Outside only\" Distances are shown outside each object, but are set to zero for all pixels inside an object.<\/li><\/ul>"
                },
                {
                  "name": "Normalise map per object",
                  "description": "When selected, the distance values inside each object are normalised to the range 0-1.  Normalisation is performed on an object-by-object basis, so the absolute distance values cannot be directly compared between objects."
                },
                {
                  "name": "Spatial units mode",
                  "description": "Controls whether spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                }
              ],
              "slug": "create-distance-map"
            }
          ]
        },
        {
          "path": "/modules/objects/detect",
          "name": "Detect",
          "description": "Modules resulting in creation of new objects to be stored in the workspace.",
          "slug": "detect",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/objects/detect/circle-detection",
              "deprecated": false,
              "name": "Circle detection",
              "shortDescription": "Detects circles within grayscale images using the Hough transform.",
              "fullDescription": "Detects circles within grayscale images using the Hough transform.  Input images can be of binary or grayscale format, but the circle features must be brighter than their surrounding background and have dark centres (i.e. be rings).  For solid circles, a gradient filter or equivalent should be applied to the image first.  Detected circles are output to the workspace as solid objects.  Circles are detected within a user-defined radius range and must exceed a user-defined threshold score (based on the intensity of the circle feartures in the input image and the feature circularity).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input image from which objects will be detected."
                },
                {
                  "name": "Output objects",
                  "description": "Output objects to be added to the workspace.  Irrespective of the form of the input features, output objects are always solid."
                },
                {
                  "name": "Output transform image",
                  "description": "When selected, the Hough-transform image will be output to the workspace with the name specified by \"Output image\"."
                },
                {
                  "name": "Output image",
                  "description": "If \"Output transform image\" is selected, this will be the name assigned to the transform image added to the workspace.  The transform image has XY dimensions equal to the input image and an equal number of Z-slices to the number of radii tested.  Circluar features in the input image appear as bright points, where the XYZ location of the point corresponds to the XYR (i.e. X, Y, radius) parameters for the circle."
                },
                {
                  "name": "Downsample factor",
                  "description": "To speed up the detection process, the image can be downsampled.  For example, a downsample factor of 2 will downsize the image in X and Y by a factor of 2 prior to detection of circles."
                },
                {
                  "name": "Normalise scores",
                  "description": "When selected, scores for each parameter combination will be normalised based on the number of samples used to produce them.  Normalisation will prevent reduction of scores towards the image edge; however, it can also lead to some overestimation in these regions."
                },
                {
                  "name": "Detection mode",
                  "description": "Controls which objects will be added to the output collection:<br><ul><li>\"All above score\" All objects with scores above the threshold specified by \"Detection threshold\" will be exported.  Objects will be detected starting with that with the highest score.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><li>\"N highest scores\" A specific number (controlled by \"Number of objects\") of detected objects with the highest scores will be exported.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><\/ul>"
                },
                {
                  "name": "Detection threshold",
                  "description": "If \"Detection mode\" is set to \"All above score\", this is the minimum score a detected circle must have to be stored.  Scores are the sum of all pixel intensities lying on the perimeter of the circle.  As such, higher scores correspond to brighter circles, circles with high circularity (where all points lie on the perimeter of the detected circle) and circles with continuous intensity along their perimeter (no gaps)."
                },
                {
                  "name": "Number of objects",
                  "description": "If \"Detection mode\" is set to \"N highest scores\", this is the number of objects with the highest scores that will be added to the output object collection."
                },
                {
                  "name": "Exclusion radius (px)",
                  "description": "The minimum distance between adjacent circles.  For multiple candidate points within this range, the circle with the highest score will be retained.  Specified in pixel units."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple radii simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                },
                {
                  "name": "Show transform image",
                  "description": "When selected, the transform image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection image",
                  "description": "When selected, the detection image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection score",
                  "description": "When selected, the detection image will also show the score associated with each detected object."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the detection score text label."
                },
                {
                  "name": "X range (px)",
                  "description": "Range of X-position values to be tested.  X-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7).  Unlike other parameter ranges, X-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  X-position values are specified in pixel units."
                },
                {
                  "name": "Y range (px)",
                  "description": "Range of Y-position values to be tested.  Y-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values  4,5,6 and 7).  Unlike other parameter ranges, Y-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  Y-position values are specified in pixel units."
                },
                {
                  "name": "Radius range (px)",
                  "description": "Range of radius values to be tested.  Radii can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" specifies values 4,6,8 and 10).  Radii are specified in pixel units."
                },
                {
                  "name": "Output radius resize (px)",
                  "description": "Radius of output objects will be adjusted by this value.  For example, a detected circle of radius 5 with a \"radius resize\" of 2 will have an output radius of 7.  Similarly, setting \"radius resize\" to -3 would produce a circle of radius 2."
                }
              ],
              "slug": "circle-detection"
            },
            {
              "path": "/modules/objects/detect/cluster-points",
              "deprecated": false,
              "name": "Cluster points",
              "shortDescription": "Clusters objects (based on centroid locations) using K-Means++ and/or DBSCAN algorithms.",
              "fullDescription": "Clusters objects (based on centroid locations) using K-Means++ and/or DBSCAN algorithms.  In K-Means++ [1], an optimisation of the standard K-Means algorithm, the points are assigned to a pre-determined number of clusters such that each point is assigned to its closest cluster mean position (this process is repeated until the cluster assignments stabilise or a maximum number of iterations is reached).  For DBSCAN [2], points are clustered based on a minimum number of neighbours within a specified spatial range.  As such, this algorithm doesn't require prior knowledge of the number of clusters.  Both algorithms use their respective <a href=\"https://commons.apache.org/proper/commons-math/\">Apache Commons Math implementations.<\/a><br><br>References:<br>[1] Arthur, D.; Vassilvitskii, S. (2007). \"k-means++: the advantages of careful seeding.\" <i>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics Philadelphia, PA, USA.<\/i> pp. 1027\u20131035<br>[2] Ester, M.; Kriegel, H.-P.; Sander, J.; Xu, X. (1996). \"A density-based algorithm for discovering clusters in large spatial databases with noise.\" <i>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press.<\/i> pp. 226\u2013231",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Output objects",
                  "description": ""
                },
                {
                  "name": "Binary logic",
                  "description": ""
                },
                {
                  "name": "Clustering algorithm",
                  "description": "The clustering algorithm to use:<br><ul><li>\"DBSCAN\" Points are clustered based on a minimum number of neighbours (\"Minimum number of points per cluster\") within a specified distance (\"Neighbourhood for clustering (epsilon)\").  All proximal points which satisfy these criteria are added to a common cluster.  This uses the <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/stat/clustering/DBSCANClusterer.html\">Apache Commons Math3<\/a> implementation of DBSCAN, which describes the algorithm as: \"A point p is density connected to another point q, if there exists a chain of points pi, with i = 1 .. n and p1 = p and pn = q, such that each pair <pi, pi+1> is directly density-reachable. A point q is directly density-reachable from point p if it is in the -neighborhood of this point.\".<\/li><li>\"KMeans++\" Points are assigned into a pre-determined number of clusters (defined by \"Number of clusters\"), with each point assigned to the cluster with the closest centroid.  Since the cluster centroids will vary with each added point, this process is optimised iteratively.  The algorithm continues until either no points switch clusters or the maximum number of allowed iterations (\"Maximum number of iterations\") is reached.<\/li><\/ul>"
                },
                {
                  "name": "Number of clusters",
                  "description": "If \"Clustering algorithm\" is set to \"KMeans++\", this is the number of clusters the points will be assigned to."
                },
                {
                  "name": "Maximum number of iterations",
                  "description": "If \"Clustering algorithm\" is set to \"KMeans++\", this is the maximum number of optimisation iterations that will be performed.  If cluster assignment has stabilised prior to reaching this number of iterations the algorithm will terminate early."
                },
                {
                  "name": "Neighbourhood for clustering (epsilon)",
                  "description": "If \"Clustering algorithm\" is set to \"DBSCAN\", this is the minimum distance to neighbour points that must be satisfied for a point to be added to a cluster.  This distance is specified in pixel units."
                },
                {
                  "name": "Minimum number of points per cluster",
                  "description": "If \"Clustering algorithm\" is set to \"DBSCAN\", this is the minimum number of neighbour points which must be within a specified distance (\"Neighbourhood for clustering (epsilon)\") of a point for that point to be included in the cluster."
                }
              ],
              "slug": "cluster-points"
            },
            {
              "path": "/modules/objects/detect/create-whole-slice-objects",
              "deprecated": false,
              "name": "Create whole slice objects",
              "shortDescription": "For conversion of an image to individual slice objects.",
              "fullDescription": "For conversion of an image to individual slice objects.  This is useful for subsequently making independent measurements on each slice of an image.  Individual objects can either be created per slice or per timepoint.  Output objects are automatically assigned measurements for timepoint and optionally also slice if processing slice-by-slice.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Output objects",
                  "description": ""
                },
                {
                  "name": "Output mode",
                  "description": ""
                }
              ],
              "slug": "create-whole-slice-objects"
            },
            {
              "path": "/modules/objects/detect/distance-bands",
              "deprecated": false,
              "name": "Distance bands",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Binary logic",
                  "description": ""
                },
                {
                  "name": "Output objects",
                  "description": ""
                },
                {
                  "name": "Band mode",
                  "description": ""
                },
                {
                  "name": "Volume type",
                  "description": ""
                },
                {
                  "name": "Merge bands",
                  "description": ""
                },
                {
                  "name": "Match Z to XY",
                  "description": ""
                },
                {
                  "name": "Weight mode",
                  "description": ""
                },
                {
                  "name": "Band width",
                  "description": ""
                },
                {
                  "name": "Spatial units mode",
                  "description": ""
                },
                {
                  "name": "Apply minimum band distance",
                  "description": ""
                },
                {
                  "name": "Minimum band distance",
                  "description": ""
                },
                {
                  "name": "Apply maximum band distance",
                  "description": ""
                },
                {
                  "name": "Maximum band distance",
                  "description": ""
                }
              ],
              "slug": "distance-bands"
            },
            {
              "path": "/modules/objects/detect/identify-objects",
              "deprecated": false,
              "name": "Identify objects",
              "shortDescription": "Creates objects from an input binary image.",
              "fullDescription": "Creates objects from an input binary image.  Each object is identified in 3D as a contiguous region of foreground labelled pixels.  All coordinates corresponding to that object are stored for use later.<br><br>Note: Input binary images must be 8-bit and only contain values 0 and 255.<br><br>Note: Uses MorphoLibJ to perform connected components labelling in 3D.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input binary image from which objects will be identified.  This image must be 8-bit and only contain values 0 and 255."
                },
                {
                  "name": "Output objects",
                  "description": "Name of output objects to be stored in workspace."
                },
                {
                  "name": "Binary logic",
                  "description": "Controls whether objects are considered to be white (255 intensity) on a black (0 intensity) background, or black on a white background."
                },
                {
                  "name": "Detection mode",
                  "description": ""
                },
                {
                  "name": "Identify as single object",
                  "description": "Add all pixels to a single output object.  Enabling this skips the connected-components step."
                },
                {
                  "name": "Connectivity",
                  "description": "When performing connected components labelling, the connectivity determines which neighbouring pixels are considered to be in contact.<br><ul><li>\"6\" considers immediate neighbours to lie in the cardinal directions (i.e. left, right, in-front, behind, above and below).  In 2D this is actually 4-way connectivity.<\/li><li> - \"26\" (default) considers neighbours to include the cardinal directions as well as diagonal to the pixel in question.  In 2D this is actually 8-way connectivity.<\/li><\/ul>"
                },
                {
                  "name": "Volume type",
                  "description": "The method used to store pixel coordinates.  This only affects performance and memory usage, there is no difference in results obtained using difference storage methods.<br><ul><li>\"Pointlist\" (default) stores object coordinates as a list of XYZ coordinates.  This is most efficient for small objects, very thin objects or objects with lots of holes.<\/li><li>\"Octree\" stores objects in an octree format.  Here, the coordinate space is broken down into cubes of different sizes, each of which is marked as foreground (i.e. an object) or background.  Octrees are most efficient when there are lots of large cubic regions of the same label, as the space can be represented by larger (and thus fewer) cubes.  This is best used when there are large, completely solid objects.  If z-axis sampling is much larger than xy-axis sampling, it's typically best to opt for the quadtree method.<\/li><li>\"Quadtree\" stores objects in a quadtree format.  Here, each Z-plane of the object is broken down into squares of different sizes, each of which is marked as foreground (i.e. an object) or background.  Quadtrees are most efficient when there are lots of large square regions of the same label, as the space can be represented by larger (and thus fewer) squares.  This is best used when there are large, completely solid objects.<\/li><\/ul>"
                },
                {
                  "name": "Enable multithreading",
                  "description": "Break the image down into strips, each one processed on a separate CPU thread.  The overhead required to do this means it's best for large multi-core CPUs, but should be left disabled for small images or on CPUs with few cores."
                },
                {
                  "name": "Minimum strip width (px)",
                  "description": "Minimum width of each strip to be processed on a separate CPU thread.  Measured in pixel units."
                }
              ],
              "slug": "identify-objects"
            },
            {
              "path": "/modules/objects/detect/manually-identify-objects",
              "deprecated": false,
              "name": "Manually identify objects",
              "shortDescription": "Manually create objects using the ImageJ selection tools.",
              "fullDescription": "Manually create objects using the ImageJ selection tools.  Selected regions can be interpolated in Z and T to speed up the object creation process.<br><br>This module will display a control panel and an image onto which selections are made.  <br><br>Following selection of a region to be included in the object, the user can either add this region to a new object (\"Add new\" button), or add it to an existing object (\"Add to existing\" button).  The target object for adding to an existing object is specified using the \"Existing object number\" control (a list of existing object IDs is shown directly below this control).<br><br>References to each selection are displayed below the controls.  Previously-added regions can be re-selected by clicking the relevant reference.  This allows selections to be deleted or used as a basis for further selections.<br><br>Once all selections have been made, objects are added to the workspace with the \"Finish\" button.<br><br>Objects need to be added slice-by-slice and can be linked in 3D using the \"Add to existing\" control.",
              "version": "1.0.1",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image onto which selections will be drawn.  This will be displayed automatically when the module runs."
                },
                {
                  "name": "Output objects",
                  "description": "Objects created by this module."
                },
                {
                  "name": "Volume type",
                  "description": "The method used to store pixel coordinates.  This only affects performance and memory usage, there is no difference in results obtained using difference storage methods.<br><ul><li>\"Pointlist\" (default) stores object coordinates as a list of XYZ coordinates.  This is most efficient for small objects, very thin objects or objects with lots of holes.<\/li><li>\"Octree\" stores objects in an octree format.  Here, the coordinate space is broken down into cubes of different sizes, each of which is marked as foreground (i.e. an object) or background.  Octrees are most efficient when there are lots of large cubic regions of the same label, as the space can be represented by larger (and thus fewer) cubes.  This is best used when there are large, completely solid objects.  If z-axis sampling is much larger than xy-axis sampling, it's typically best to opt for the quadtree method.<\/li><li>\"Quadtree\" stores objects in a quadtree format.  Here, each Z-plane of the object is broken down into squares of different sizes, each of which is marked as foreground (i.e. an object) or background.  Quadtrees are most efficient when there are lots of large square regions of the same label, as the space can be represented by larger (and thus fewer) squares.  This is best used when there are large, completely solid objects.<\/li><\/ul>"
                },
                {
                  "name": "Spatial interpolation",
                  "description": "Interpolate objects in Z.  Objects assigned the same ID will be interpolated to appear in all slices between the top-most and bottom-most specific slices.  Specified regions must contain a degree of overlap (higher overlap will give better results)."
                },
                {
                  "name": "Output tracks",
                  "description": "When selected, the same object can be identified across multiple timepoints.  The same ID should be used for all objects in this \"track\" - this will become the ID of the track object itself, while each timepoint instance will be assigned its own unique ID.  This feature also enables the use of temporal intepolation of objects."
                },
                {
                  "name": "Output track objects",
                  "description": "Name of track objects to be added to the workspace.  These will be parents of the individual timepoint instances and provide a way of grouping all the individual timepoint instances of a particular object.  Track objects themselves do not contain any coordinate information."
                },
                {
                  "name": "Temporal interpolation",
                  "description": "Interpolate objects across multiple frames.  Objects assigned the same ID will be interpolated to appear in all frames between the first and last specified timepoints.  Specified regions must contain a degree of overlap (higher overlap will give better results)."
                },
                {
                  "name": "Instruction text",
                  "description": "Text that will be displayed to the user on the object selection control panel.  This can inform them of the steps they need to take to select the objects."
                },
                {
                  "name": "Default selector type",
                  "description": "Default region drawing tool to enable.  This tool can be changed by the user when selecting regions.  Choices are: Freehand line, Freehand region, Line, Oval, Points, Polygon, Rectangle, Segmented line, Wand (tracing) tool."
                },
                {
                  "name": "Point mode (point-type ROIs only)",
                  "description": ""
                },
                {
                  "name": "Message on image",
                  "description": "Message to display in title of image."
                }
              ],
              "slug": "manually-identify-objects"
            },
            {
              "path": "/modules/objects/detect/rectangle-detection",
              "deprecated": false,
              "name": "Rectangle detection",
              "shortDescription": "Detects rectangles within grayscale images using the Hough transform.",
              "fullDescription": "Detects rectangles within grayscale images using the Hough transform.  Input images can be of binary or grayscale format, but the rectangular features must be brighter than their surrounding background and have dark centres (i.e. not be solid).  For solid rectangles, a gradient filter or equivalent should be applied to the image first.  Detected rectangles are output to the workspace as solid objects.  Rectangles are detected within a user-defined width, length and orientation range and must exceed a user-defined threshold score (based on the intensity of the rectangles feartures in the input image).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input image from which objects will be detected."
                },
                {
                  "name": "Output objects",
                  "description": "Output objects to be added to the workspace.  Irrespective of the form of the input features, output objects are always solid."
                },
                {
                  "name": "Output transform image",
                  "description": "When selected, the Hough-transform image will be output to the workspace with the name specified by \"Output image\"."
                },
                {
                  "name": "Output image",
                  "description": "If \"Output transform image\" is selected, this will be the name assigned to the transform image added to the workspace.  The transform image has XY dimensions equal to the input image, an equal number of channels to the number of widths tested, Z-slices to the number of lengths tested and frames to the number of orientations tests.  Rectangular features in the input image appear as bright points, where the XYCZT location of the point corresponds to the XYWLO (i.e. X, Y, width, length and orientation) parameters for the rectangle."
                },
                {
                  "name": "Downsample factor",
                  "description": "To speed up the detection process, the image can be downsampled.  For example, a downsample factor of 2 will downsize the image in X and Y by a factor of 2 prior to detection of circles."
                },
                {
                  "name": "Normalise scores",
                  "description": "When selected, scores for each parameter combination will be normalised based on the number of samples used to produce them.  Normalisation will prevent reduction of scores towards the image edge; however, it can also lead to some overestimation in these regions."
                },
                {
                  "name": "Detection mode",
                  "description": "Controls which objects will be added to the output collection:<br><ul><li>\"All above score\" All objects with scores above the threshold specified by \"Detection threshold\" will be exported.  Objects will be detected starting with that with the highest score.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><li>\"N highest scores\" A specific number (controlled by \"Number of objects\") of detected objects with the highest scores will be exported.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><\/ul>"
                },
                {
                  "name": "Detection threshold",
                  "description": "If \"Detection mode\" is set to \"All above score\", this is the minimum score a detected circle must have to be stored.  Scores are the sum of all pixel intensities lying on the perimeter of the circle.  As such, higher scores correspond to brighter circles, circles with high circularity (where all points lie on the perimeter of the detected circle) and circles with continuous intensity along their perimeter (no gaps)."
                },
                {
                  "name": "Number of objects",
                  "description": "If \"Detection mode\" is set to \"N highest scores\", this is the number of objects with the highest scores that will be added to the output object collection."
                },
                {
                  "name": "Exclusion radius (px)",
                  "description": "The minimum distance between adjacent circles.  For multiple candidate points within this range, the circle with the highest score will be retained.  Specified in pixel units."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple radii simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                },
                {
                  "name": "Show transform image",
                  "description": "When selected, the transform image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection image",
                  "description": "When selected, the detection image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection score",
                  "description": "When selected, the detection image will also show the score associated with each detected object."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the detection score text label."
                },
                {
                  "name": "X range (px)",
                  "description": "Range of X-position values to be tested.  X-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7).  Unlike other parameter ranges, X-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  X-position values are specified in pixel units."
                },
                {
                  "name": "Y range (px)",
                  "description": "Range of Y-position values to be tested.  Y-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values  4,5,6 and 7).  Unlike other parameter ranges, Y-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  Y-position values are specified in pixel units."
                },
                {
                  "name": "Width range (px)",
                  "description": "Range of width values to be tested.  Widths can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" specifies values 4,6,8 and 10).  Widths are specified in pixel units."
                },
                {
                  "name": "Length range (px)",
                  "description": "Range of length values to be tested.  Lengths can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" specifies values 4,6,8 and 10).  Lengths are specified in pixel units."
                },
                {
                  "name": "Orientation range (degs)",
                  "description": "Range of orientation values to be tested.  Orientations can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" specifies values 4,6,8 and 10).  Orientations are specified in degree units."
                }
              ],
              "slug": "rectangle-detection"
            },
            {
              "path": "/modules/objects/detect/ridge-detection",
              "deprecated": false,
              "name": "Ridge detection",
              "shortDescription": "Detects ridge objects in an image from the workspace.",
              "fullDescription": "Detects ridge objects in an image from the workspace.  A ridge is considered as a line of higher (or lower) intensity pixels in an image.  Ridges are output as objects to the workspace with relevant measurements associated with each object (e.g. ridge length).  This module uses the \"<a href=\"https://imagej.net/Ridge_Detection\">Ridge Detection<\/a>\" plugin, which itself is based on the paper \"An Unbiased Detector of Curvilinear Structures\" (Steger, C., <i>IEEE Transactions on Pattern Analysis and Machine Intelligence<\/i> (1998) <b>20<\/b> 113\u2013125).<br><br>Note: This module detects ridges in 2D, but can be run on multi-dimensional images.  For higher dimensionality images than 2D, ridge detection is performed slice-by-slice, with output objects confined to a single slice.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace on which ridges will be identified.  This can be a multi-dimensional image, although ridges are currently only detected in 2D.  In the case of higher-dimensionality images, ridges are detected on a slice-by-slice basis."
                },
                {
                  "name": "Output objects",
                  "description": "Output ridge objects, which will be added to the workspace.  These objects will have measurements associated with them."
                },
                {
                  "name": "Contour contrast",
                  "description": "Controls if we are detecting dark lines on a lighter background or light lines on a darker background."
                },
                {
                  "name": "Lower threshold",
                  "description": "Lower response threshold for points on a line to be accepted.  This threshold is based on the value of points after the ridge enhancement filtering and is not a reflection of their absolute pixel intensities."
                },
                {
                  "name": "Upper threshold",
                  "description": "Upper response threshold for points on a line to be accepted.  This threshold is based on the value of points after the ridge enhancement filtering and is not a reflection of their absolute pixel intensities."
                },
                {
                  "name": "Sigma",
                  "description": "Sigma of the derivatives to be applied to the input image when detecting ridges.  This is related to the width of the lines to be detected and is greater than or equal to \"width/(2*sqrt(3))\"."
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                },
                {
                  "name": "Extend line",
                  "description": "Lines are extended in an attempt to locate more junction points."
                },
                {
                  "name": "Estimate width",
                  "description": "When this is selected, the width of each line is estimated and exported as a measurement.  This allows the output objects to be exported as \"wide\" objects, although only if Apply width to output is also selected"
                },
                {
                  "name": "Apply width to output",
                  "description": "When selected, the output ridge objects will have the width applied (rather than having single pixel width)."
                },
                {
                  "name": "Overlap mode",
                  "description": "Controls how intersecting lines should be handled.  For more information see <a href=\"https://imagej.net/Ridge_Detection.html#Overlap_resolution\">https://imagej.net/Ridge_Detection.html#Overlap_resolution<\/a>.<br><ul><li>\"None\" Ridges are terminated at line intersections, so two overlapping ridges will likely be identified as at least four separate ridge objects.<\/li><li>\"Slope\" When selected, this will attempt to resolve ridge overlaps such that two overlapping ridges would be identified as two separate objects.  Ridges output in this manner can have common, shared paths (i.e. during the overlap region).<\/li><\/ul>"
                },
                {
                  "name": "Minimum length",
                  "description": "Minimum length of a detected line for it to be retained and output.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Maximum length",
                  "description": "Maximum length of a detected line for it to be retained and output.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Join at junctions",
                  "description": ""
                },
                {
                  "name": "Link ends",
                  "description": "When selected, ridges with ends in close proximity will be linked into a single object."
                },
                {
                  "name": "Alignment range (px)",
                  "description": "If linking contours, but limiting the end misalignment, this is the number of points from each contour end for which the orientation of that end is calculated."
                },
                {
                  "name": "Maximum end separation (px)",
                  "description": ""
                },
                {
                  "name": "Maximum end misalignment (degs)",
                  "description": "If linking contours, but limiting the end misalignment, this is the maximum permitted misalignment of each contour end."
                }
              ],
              "slug": "ridge-detection"
            },
            {
              "path": "/modules/objects/detect/run-trackmate",
              "deprecated": true,
              "name": "Run TrackMate",
              "shortDescription": "Uses the TrackMate plugin included with Fiji to detect and track spots in images.",
              "fullDescription": "Uses the TrackMate plugin included with Fiji to detect and track spots in images.  For more information, see the <a href=\"https://imagej.net/TrackMate\">TrackMate<\/a> documentation.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image in which to detect spots."
                },
                {
                  "name": "Output spot objects",
                  "description": "Spot objects that will be added to the workspace.  If tracking is enabled, each spot will have a parent track object."
                },
                {
                  "name": "Calibrated units",
                  "description": "Enable if spatial parameters (e.g. \"Radius\" or \"Max linking distance\") are being specified in calibrated units.  If disabled, parameters are assumed to be specified in pixel units."
                },
                {
                  "name": "Do sub-pixel localisation",
                  "description": "Enable TrackMate's \"Subpixel localisation\" functionality.  When enabled, subpixel centroid coordinates will be stored as measurements associated with each detected object."
                },
                {
                  "name": "Median filtering",
                  "description": "Enable TrackMate's \"Median filtering\" functionality."
                },
                {
                  "name": "Radius",
                  "description": "Expected radius of spots in the input image.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Threshold",
                  "description": "Threshold for spot detection.  Threshold is applied to filtered image (Laplacian of Gaussian), so will be affected by the specified \"Radius\" value.  Increase this value to make detection more selective (i.e. detect fewer spots)."
                },
                {
                  "name": "Estimate spot size",
                  "description": "When enabled, output spot objects will have explicit size (rather than a single, centroid coordinate) determined by the TrackMate-calculated estimated diameter."
                },
                {
                  "name": "Run tracking",
                  "description": "Track spot objects over time.  Spots in each frame will become children of a parent track object.  The track object itself won't contain any coordinate information."
                },
                {
                  "name": "Tracking method",
                  "description": "Method with which spots are tracked between frames:<br><ul><li>\"Linear motion (Kalman)\" Uses the previous position of a spot and its current velocity to estimate where the spot will be in the next frame. These predicted spots are linked to the spots in the current frame.  When dealing with particles moving at roughly constant speeds, this method should be more accurate.<\/li><li>\"Simple\" (default) Calculates links between spot positions in the previous and current frames.  This does not take motion into account.<\/li><\/ul>"
                },
                {
                  "name": "Max linking distance",
                  "description": "Maximum distance a spot can travel between frames and still be linked to its starting spot.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Gap closing max distance",
                  "description": "Maximum distance a spot can travel between \"Max frame gap\" frames and still be linked to its starting spot.  This accounts for the greater distance a spot can move between detections when it's allowed to go undetected in some timepoints.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Initial search radius",
                  "description": "Minimum spot separation required for creation of a new track."
                },
                {
                  "name": "Search radius",
                  "description": "Maximum distance between predicted spot location and location of spot in current frame."
                },
                {
                  "name": "Max frame gap",
                  "description": "Maximum number of frames a spot can go undetected before it will be classed as a new track upon reappearance."
                },
                {
                  "name": "Output track objects",
                  "description": "Track objects that will be added to the workspace.  These are parent objects to the spots in that track.  Track objects are simply used for linking spots to a common track and storing track-specific measurements."
                }
              ],
              "slug": "run-trackmate"
            },
            {
              "path": "/modules/objects/detect/sphere-detection",
              "deprecated": false,
              "name": "Sphere detection",
              "shortDescription": "Detects spheres within grayscale images using the Hough transform.",
              "fullDescription": "Detects spheres within grayscale images using the Hough transform.  Input images can be of binary or grayscale format, but the sphere features must be brighter than their surrounding background and have dark centres (i.e. be shells).  For solid spheres, a gradient filter or equivalent should be applied to the image first.  Detected spheres are output to the workspace as solid objects.  Spheres are detected within a user-defined radius range and must exceed a user-defined threshold score (based on the intensity of the spherical feartures in the input image and the feature sphericity.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Input image from which objects will be detected."
                },
                {
                  "name": "Output objects",
                  "description": "Output objects to be added to the workspace.  Irrespective of the form of the input features, output objects are always solid."
                },
                {
                  "name": "Output transform image",
                  "description": "When selected, the Hough-transform image will be output to the workspace with the name specified by \"Output image\"."
                },
                {
                  "name": "Output image",
                  "description": "If \"Output transform image\" is selected, this will be the name assigned to the transform image added to the workspace.  The transform image has XY dimensions equal to the input image and an equal number of Z-slices to the number of radii tested.  Circluar features in the input image appear as bright points, where the XYZ location of the point corresponds to the XYR (i.e. X, Y, radius) parameters for the sphere."
                },
                {
                  "name": "Downsample factor",
                  "description": "To speed up the detection process, the image can be downsampled.  For example, a downsample factor of 2 will downsize the image in X and Y by a factor of 2 prior to detection of circles."
                },
                {
                  "name": "Normalise scores",
                  "description": "When selected, scores for each parameter combination will be normalised based on the number of samples used to produce them.  Normalisation will prevent reduction of scores towards the image edge; however, it can also lead to some overestimation in these regions."
                },
                {
                  "name": "Detection mode",
                  "description": "Controls which objects will be added to the output collection:<br><ul><li>\"All above score\" All objects with scores above the threshold specified by \"Detection threshold\" will be exported.  Objects will be detected starting with that with the highest score.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><li>\"N highest scores\" A specific number (controlled by \"Number of objects\") of detected objects with the highest scores will be exported.  Following export of each object all potential objects within \"Exclusion radius (px)\" will be suppressed to prevent the same object being detected multiple times.<\/li><\/ul>"
                },
                {
                  "name": "Detection threshold",
                  "description": "If \"Detection mode\" is set to \"All above score\", this is the minimum score a detected circle must have to be stored.  Scores are the sum of all pixel intensities lying on the perimeter of the circle.  As such, higher scores correspond to brighter circles, circles with high circularity (where all points lie on the perimeter of the detected circle) and circles with continuous intensity along their perimeter (no gaps)."
                },
                {
                  "name": "Number of objects",
                  "description": "If \"Detection mode\" is set to \"N highest scores\", this is the number of objects with the highest scores that will be added to the output object collection."
                },
                {
                  "name": "Exclusion radius (px)",
                  "description": "The minimum distance between adjacent circles.  For multiple candidate points within this range, the circle with the highest score will be retained.  Specified in pixel units."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple radii simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                },
                {
                  "name": "Show transform image",
                  "description": "When selected, the transform image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection image",
                  "description": "When selected, the detection image will be displayed (as long as the module is currently set to show its output)."
                },
                {
                  "name": "Show detection score",
                  "description": "When selected, the detection image will also show the score associated with each detected object."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the detection score text label."
                },
                {
                  "name": "X range (px)",
                  "description": "Range of X-position values to be tested.  X-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7).  Unlike other parameter ranges, X-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  X-position values are specified in pixel units."
                },
                {
                  "name": "Y range (px)",
                  "description": "Range of Y-position values to be tested.  Y-position can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values  4,5,6 and 7).  Unlike other parameter ranges, Y-position can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  Y-position values are specified in pixel units."
                },
                {
                  "name": "Z range (slices)",
                  "description": "Range of Z-slices values to be tested.  Z-slices can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values  4,5,6 and 7).  Unlike other parameter ranges, Z-slices can't be specified as a range extracting every nth slice (e.g. \"4-10-2\"), instead image downsampling (\"Downsample factor\" parameter) should be used.  Z-slices values are specified in slice units."
                },
                {
                  "name": "Radius range (px)",
                  "description": "Range of radius values to be tested.  Radii can be specified as a comma-separated list, using a range (e.g. \"4-7\" specifies values 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" specifies values 4,6,8 and 10).  Radii are specified in pixel units."
                },
                {
                  "name": "Output radius resize (px)",
                  "description": "Radius of output objects will be adjusted by this value.  For example, a detected sphere of radius 5 with a \"radius resize\" of 2 will have an output radius of 7.  Similarly, setting \"radius resize\" to -3 would produce a sphere of radius 2."
                }
              ],
              "slug": "sphere-detection"
            },
            {
              "path": "/modules/objects/detect/spot-detection",
              "deprecated": false,
              "name": "Spot detection",
              "shortDescription": "Detects spot-like features in 2D and 3D using TrackMate's LogDetector.",
              "fullDescription": "Detects spot-like features in 2D and 3D using TrackMate's LogDetector.  By default, detected spots are stored as individual, single pixel, objects centred on the detected feature.  Optionally, spots can be given area or volume based on the estimated size of the spot.  Adds measurements to each output spot for estimated radius and quality.  If sub-pixel localisation is specified, the sub-pixel centroid location in X,Y and Z is also stored as a measurement.<br><br>For more information, see the <a href=\"https://imagej.net/TrackMate\">TrackMate<\/a> documentation.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image in which to detect spots."
                },
                {
                  "name": "Output spot objects",
                  "description": "Spot objects that will be added to the workspace."
                },
                {
                  "name": "Detection mode",
                  "description": ""
                },
                {
                  "name": "Calibrated units",
                  "description": "Enable if spatial parameters (e.g. \"Radius\") are being specified in calibrated units.  If disabled, parameters are assumed to be specified in pixel units."
                },
                {
                  "name": "Do sub-pixel localisation",
                  "description": "Enable TrackMate's \"Subpixel localisation\" functionality.  When enabled, subpixel centroid coordinates will be stored as measurements associated with each detected object."
                },
                {
                  "name": "Median filtering",
                  "description": "Enable TrackMate's \"Median filtering\" functionality."
                },
                {
                  "name": "Radius",
                  "description": "Expected radius of spots in the input image.  Specified in pixel units, unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Threshold",
                  "description": "Threshold for spot detection.  Threshold is applied to filtered image (Laplacian of Gaussian), so will be affected by the specified \"Radius\" value.  Increase this value to make detection more selective (i.e. detect fewer spots)."
                },
                {
                  "name": "Estimate spot size",
                  "description": "When enabled, output spot objects will have explicit size (rather than a single, centroid coordinate) determined by the TrackMate-calculated estimated diameter."
                }
              ],
              "slug": "spot-detection"
            },
            {
              "path": "/modules/objects/detect/stardist-detection",
              "deprecated": false,
              "name": "StarDist detection",
              "shortDescription": "Implements the StarDist plugin to detect objects.",
              "fullDescription": "Implements the StarDist plugin to detect objects.  For more information on StarDist please see <a href=\"https://imagej.net/plugins/stardist\">https://imagej.net/plugins/stardist<\/a>.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": ""
                },
                {
                  "name": "Output objects",
                  "description": ""
                },
                {
                  "name": "Model mode",
                  "description": ""
                },
                {
                  "name": "Model name",
                  "description": ""
                },
                {
                  "name": "Model path",
                  "description": ""
                },
                {
                  "name": "Normalise image",
                  "description": ""
                },
                {
                  "name": "Percentile low",
                  "description": ""
                },
                {
                  "name": "Percentile high",
                  "description": ""
                },
                {
                  "name": "Number of tiles",
                  "description": ""
                },
                {
                  "name": "Probability threshold",
                  "description": ""
                },
                {
                  "name": "Overlap threshold",
                  "description": ""
                },
                {
                  "name": "Boundary exclusion",
                  "description": ""
                }
              ],
              "slug": "stardist-detection"
            }
          ]
        },
        {
          "path": "/modules/objects/filter",
          "name": "Filter",
          "description": "Modules which can remove objects from an object collection (or move them to a new collection) based on various metrics.",
          "slug": "filter",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/objects/filter/based-on-measurement",
              "deprecated": false,
              "name": "Based on measurement",
              "shortDescription": "Filter an object collection based on a measurement value associated with this object.",
              "fullDescription": "Filter an object collection based on a measurement value associated with this object.  The threshold (reference) value can be either a fixed value (same for all objects), a measurement associated with an image (same for all objects within a single analysis run) or a measurement associated with a parent object (potentially different for all objects).  Objects which satisfy the specified numeric filter (less than, equal to, greater than, etc.) can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Numeric comparison used to determine which objects should be removed.  Objects with specified property (e.g. a measurement value) that passes this filter will be removed.  For example, an object with a measurement value of 7 would be removed if \"Method for filtering\" is set to \"Less than\" with a reference value of 10.  Choices are: Less than, Less than or equal to, Equal to, Greater than or equal to, Greater than, Not equal to."
                },
                {
                  "name": "Reference mode",
                  "description": "Type of reference value used to compare objects to:<br><ul><li>\"Fixed value\" Objects will be compared to a single, fixed value specified using the \"Reference value\" parameter.<\/li><li>\"Image measurement\" Objects will be compared to a measurement associated with an image in the workspace.  The image and associated measurement are specified by the \"Reference value image\" and \"Reference image measurement\" parameters.  In this mode, all objects will still be compared to the same value; however, this value can vary between input files.<\/li><li>\"Parent object measurement\" Objects will be compared to a measurement associated with a parent object.  The parent object and measurement are specified by the \"Reference value parent object\" and \"Reference object measurement\" parameters.  In this mode all objects can (but won't necessarily) be compared to different values.<\/li><\/ul>"
                },
                {
                  "name": "Reference value",
                  "description": "When \"Reference mode\" is set to \"Fixed value\", all objects will be compared to this fixed value."
                },
                {
                  "name": "Reference value image",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to a measurement associated with this image."
                },
                {
                  "name": "Reference image measurement",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to this measurement, which itself is associated with the image specified by \"Reference value image\"."
                },
                {
                  "name": "Reference value parent object",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to a measurement associated with their parent object from this collection."
                },
                {
                  "name": "Reference object measurement",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to this measurement, which itself is associated with the parent object specified by \"Reference value parent object\"."
                },
                {
                  "name": "Reference value multiplier",
                  "description": "Irrespective of how the reference value was determined, it can be systematically adjusted prior to use for comparison using this multiplier.  For example, an image measurement reference value of 32 with a \"Reference value multiplier\" of \"0.5\" will see all objects compared against a value of 16.  This is useful when comparing to dynamic values coming from image and parent object measurements."
                },
                {
                  "name": "Store summary filter results",
                  "description": "When selected, a metadata value is stored in the workspace, which records the number of objects which failed the filter and were removed or moved to another object class (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Store individual filter results",
                  "description": "When selected, each input object will be assigned a measurement reporting if that object passed or failed the filter.  The measurement value is \"1\" for objects that failed the filter (i.e. would be removed if the relevant removal setting was enabled) and \"0\" for objects that passed (i.e. wouldn't be removed)."
                },
                {
                  "name": "Measurement to filter on",
                  "description": "Objects will be filtered against their value of this measurement.  Objects missing this measurement are not removed; however, they can be removed by using the module \"With / without measurement\"."
                }
              ],
              "slug": "based-on-measurement"
            },
            {
              "path": "/modules/objects/filter/filter-objects",
              "deprecated": true,
              "name": "Filter objects",
              "shortDescription": "DEPRECATED:  Please use individual filter modules instead (e.",
              "fullDescription": "DEPRECATED:  Please use individual filter modules instead (e.g. \"Based on measurement\").<br><br>Filter an object collection based on a variety of properties.  Objects that satisfy the relevant condition can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Controls the type of filter being used:<br><ul><li>\"Exclude objects on image edge (2D)\" Remove any objects based on contact with the image edge.  Contact is considered as a case where an object pixel is in the outer-most row, column or slice  of an image (e.g. x = 0 or y = max_value).  Can optionally also consider top and bottom of 3D stack (Z-position).<\/li><li>\"Remove objects with missing measurements\" Remove any objects without a specific measurement value (i.e. values with NaN values).<\/li><li>\"Remove objects without parent\" Remove any objects without an assigned parent from a specific object collection.<\/li><li>\"Remove objects with a parent\" Remove any objects with an assigned parent from a specific object collection.<\/li><li>\"Remove objects with fewer children than:\" Remove any objects with fewer children from a specific object collection than a threshold value.<\/li><li>\"Remove objects with more children than:\" Remove any objects with more children from a specific object collection than a threshold value.<\/li><li>\"Remove objects with measurements < than:\" Remove any objects with a smaller measurement value a specific threshold.<\/li><li>\"Remove objects with measurements > than:\" Remove any objects with a larger measurement value a specific threshold.<\/li><li>\"Specific object IDs (runtime)\" Remove objects by their specific ID numbers.  This is a manual option, whereby an image is displayed to the user (pre-prepared to show object ID numbers) along with a dialog box allowing the user to write a comma-separated list of objects to remove.<\/li><\/ul>"
                },
                {
                  "name": "Include Z-position",
                  "description": "When selected, object pixels which make contact with the lower (z = 0) and upper (z = max_value) slices of the image stack will lead to removal of that object.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal).  If enabled for single slice stacks all objects will removed."
                },
                {
                  "name": "Measurement to filter on",
                  "description": "Objects will be filtered against their value of this measurement.  Objects missing this measurement are not removed; however, they can be removed by using the module \"With / without measurement\"."
                },
                {
                  "name": "Parent object",
                  "description": "Parent object to filter by.  The presence or absence of this relationship will determine which of the input objects are counted, removed or moved (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Child objects",
                  "description": "Objects will be filtered against the number of children they have from this object collection."
                },
                {
                  "name": "Reference mode",
                  "description": "Type of reference value used to compare objects to:<br><ul><li>\"Fixed value\" Objects will be compared to a single, fixed value specified using the \"Reference value\" parameter.<\/li><li>\"Image measurement\" Objects will be compared to a measurement associated with an image in the workspace.  The image and associated measurement are specified by the \"Reference value image\" and \"Reference image measurement\" parameters.  In this mode, all objects will still be compared to the same value; however, this value can vary between input files.<\/li><li>\"Parent object measurement\" Objects will be compared to a measurement associated with a parent object.  The parent object and measurement are specified by the \"Reference value parent object\" and \"Reference object measurement\" parameters.  In this mode all objects can (but won't necessarily) be compared to different values.<\/li><\/ul>"
                },
                {
                  "name": "Reference value",
                  "description": "When \"Reference mode\" is set to \"Fixed value\", all objects will be compared to this fixed value."
                },
                {
                  "name": "Reference value image",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to a measurement associated with this image."
                },
                {
                  "name": "Reference image measurement",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to this measurement, which itself is associated with the image specified by \"Reference value image\"."
                },
                {
                  "name": "Reference value parent object",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to a measurement associated with their parent object from this collection."
                },
                {
                  "name": "Reference object measurement",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to this measurement, which itself is associated with the parent object specified by \"Reference value parent object\"."
                },
                {
                  "name": "Reference value multiplier",
                  "description": "Irrespective of how the reference value was determined, it can be systematically adjusted prior to use for comparison using this multiplier.  For example, an image measurement reference value of 32 with a \"Reference value multiplier\" of \"0.5\" will see all objects compared against a value of 16.  This is useful when comparing to dynamic values coming from image and parent object measurements."
                },
                {
                  "name": "Show image",
                  "description": "When selected, a specific image will be displayed when this module executes.  This can be used to display a pre-prepared, object ID-labelled image to the user, thus acting as a reference for which object IDs to remove.  The image to be displayed is set using the \"Image to display\" parameter."
                },
                {
                  "name": "Image to display",
                  "description": "Image to display when the module executes.  For example, this could be a pre-prepared image with object IDs inserted as text overlays using the \"Add labels\" module."
                }
              ],
              "slug": "filter-objects"
            },
            {
              "path": "/modules/objects/filter/live-measurement-filter",
              "deprecated": false,
              "name": "Live measurement filter",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": ""
                },
                {
                  "name": "Filter mode",
                  "description": ""
                },
                {
                  "name": "Output (filtered) objects",
                  "description": ""
                },
                {
                  "name": "Measurement to filter on",
                  "description": ""
                },
                {
                  "name": "Method for filtering",
                  "description": ""
                },
                {
                  "name": "Reference value",
                  "description": ""
                },
                {
                  "name": "Live selection",
                  "description": ""
                },
                {
                  "name": "Store summary filter results",
                  "description": ""
                },
                {
                  "name": "Store individual filter results",
                  "description": ""
                }
              ],
              "slug": "live-measurement-filter"
            },
            {
              "path": "/modules/objects/filter/measurement-extremes",
              "deprecated": false,
              "name": "Measurement extremes",
              "shortDescription": "Filter an object collection to remove/retain the object with the largest/smallest value for a specific measurement.",
              "fullDescription": "Filter an object collection to remove/retain the object with the largest/smallest value for a specific measurement.  The objects identified for removal can be indeed removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Controls what happens to objects which don't pass the filter:<br><br>- \"Remove with largest measurements\" Remove the object with the largest value measurement specified by \"Measurement to filter on\".<br><br>- \"Remove with smallest measurements\" Remove the object with the smallest value measurement specified by \"Measurement to filter on\".<br><br>- \"Retain with largest measurements\" Retain only the object with the largest value measurement specified by \"Measurement to filter on\".<br><br>- \"Retain with smallest measurements\" Retain only the object with the smallest value measurement specified by \"Measurement to filter on\".<br>"
                },
                {
                  "name": "Filter per timepoint",
                  "description": "When selected, the measurements will be considered on a timepoint-by-timepoint basis.  For example, if retaining the object with the largest measurement, the object in each timepoint with the largest measurement would be retained; however, when not selected, only one object in the entire timeseries would be retained."
                },
                {
                  "name": "Filter per parent object",
                  "description": ""
                },
                {
                  "name": "Parent object",
                  "description": ""
                },
                {
                  "name": "Measurement to filter on",
                  "description": "Objects will be filtered against their value of this measurement.  Objects missing this measurement are not removed; however, they can be removed by using the module \"With / without measurement\"."
                },
                {
                  "name": "Number of measurements",
                  "description": "Number of objects with the most extreme measurements to remove or retain.  If an insufficient number of objects are available, the most possible will be used."
                }
              ],
              "slug": "measurement-extremes"
            },
            {
              "path": "/modules/objects/filter/number-of-children",
              "deprecated": false,
              "name": "Number of children",
              "shortDescription": "Filter an object collection based on the number of children each object has from another object collection.",
              "fullDescription": "Filter an object collection based on the number of children each object has from another object collection.  The threshold (reference) value can be either a fixed value (same for all objects), a measurement associated with an image (same for all objects within a single analysis run) or a measurement associated with a parent object (potentially different for all objects).  Objects which satisfy the specified numeric filter (less than, equal to, greater than, etc.) can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Numeric comparison used to determine which objects should be removed.  Objects with specified property (e.g. a measurement value) that passes this filter will be removed.  For example, an object with a measurement value of 7 would be removed if \"Method for filtering\" is set to \"Less than\" with a reference value of 10.  Choices are: Less than, Less than or equal to, Equal to, Greater than or equal to, Greater than, Not equal to."
                },
                {
                  "name": "Reference mode",
                  "description": "Type of reference value used to compare objects to:<br><ul><li>\"Fixed value\" Objects will be compared to a single, fixed value specified using the \"Reference value\" parameter.<\/li><li>\"Image measurement\" Objects will be compared to a measurement associated with an image in the workspace.  The image and associated measurement are specified by the \"Reference value image\" and \"Reference image measurement\" parameters.  In this mode, all objects will still be compared to the same value; however, this value can vary between input files.<\/li><li>\"Parent object measurement\" Objects will be compared to a measurement associated with a parent object.  The parent object and measurement are specified by the \"Reference value parent object\" and \"Reference object measurement\" parameters.  In this mode all objects can (but won't necessarily) be compared to different values.<\/li><\/ul>"
                },
                {
                  "name": "Reference value",
                  "description": "When \"Reference mode\" is set to \"Fixed value\", all objects will be compared to this fixed value."
                },
                {
                  "name": "Reference value image",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to a measurement associated with this image."
                },
                {
                  "name": "Reference image measurement",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to this measurement, which itself is associated with the image specified by \"Reference value image\"."
                },
                {
                  "name": "Reference value parent object",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to a measurement associated with their parent object from this collection."
                },
                {
                  "name": "Reference object measurement",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to this measurement, which itself is associated with the parent object specified by \"Reference value parent object\"."
                },
                {
                  "name": "Reference value multiplier",
                  "description": "Irrespective of how the reference value was determined, it can be systematically adjusted prior to use for comparison using this multiplier.  For example, an image measurement reference value of 32 with a \"Reference value multiplier\" of \"0.5\" will see all objects compared against a value of 16.  This is useful when comparing to dynamic values coming from image and parent object measurements."
                },
                {
                  "name": "Store summary filter results",
                  "description": "When selected, a metadata value is stored in the workspace, which records the number of objects which failed the filter and were removed or moved to another object class (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Store individual filter results",
                  "description": "When selected, each input object will be assigned a measurement reporting if that object passed or failed the filter.  The measurement value is \"1\" for objects that failed the filter (i.e. would be removed if the relevant removal setting was enabled) and \"0\" for objects that passed (i.e. wouldn't be removed)."
                },
                {
                  "name": "Child objects",
                  "description": "Objects will be filtered against the number of children they have from this object collection."
                }
              ],
              "slug": "number-of-children"
            },
            {
              "path": "/modules/objects/filter/number-of-partners",
              "deprecated": false,
              "name": "Number of partners",
              "shortDescription": "Filter an object collection based on the number of partners each object has from another object collection.",
              "fullDescription": "Filter an object collection based on the number of partners each object has from another object collection.  The threshold (reference) value can be either a fixed value (same for all objects), a measurement associated with an image (same for all objects within a single analysis run) or a measurement associated with a parent object (potentially different for all objects).  Objects which satisfy the specified numeric filter (less than, equal to, greater than, etc.) can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Numeric comparison used to determine which objects should be removed.  Objects with specified property (e.g. a measurement value) that passes this filter will be removed.  For example, an object with a measurement value of 7 would be removed if \"Method for filtering\" is set to \"Less than\" with a reference value of 10.  Choices are: Less than, Less than or equal to, Equal to, Greater than or equal to, Greater than, Not equal to."
                },
                {
                  "name": "Reference mode",
                  "description": "Type of reference value used to compare objects to:<br><ul><li>\"Fixed value\" Objects will be compared to a single, fixed value specified using the \"Reference value\" parameter.<\/li><li>\"Image measurement\" Objects will be compared to a measurement associated with an image in the workspace.  The image and associated measurement are specified by the \"Reference value image\" and \"Reference image measurement\" parameters.  In this mode, all objects will still be compared to the same value; however, this value can vary between input files.<\/li><li>\"Parent object measurement\" Objects will be compared to a measurement associated with a parent object.  The parent object and measurement are specified by the \"Reference value parent object\" and \"Reference object measurement\" parameters.  In this mode all objects can (but won't necessarily) be compared to different values.<\/li><\/ul>"
                },
                {
                  "name": "Reference value",
                  "description": "When \"Reference mode\" is set to \"Fixed value\", all objects will be compared to this fixed value."
                },
                {
                  "name": "Reference value image",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to a measurement associated with this image."
                },
                {
                  "name": "Reference image measurement",
                  "description": "When \"Reference mode\" is set to \"Image measurement\", all objects will be compared to this measurement, which itself is associated with the image specified by \"Reference value image\"."
                },
                {
                  "name": "Reference value parent object",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to a measurement associated with their parent object from this collection."
                },
                {
                  "name": "Reference object measurement",
                  "description": "When \"Reference mode\" is set to \"Parent object measurement\", all objects will be compared to this measurement, which itself is associated with the parent object specified by \"Reference value parent object\"."
                },
                {
                  "name": "Reference value multiplier",
                  "description": "Irrespective of how the reference value was determined, it can be systematically adjusted prior to use for comparison using this multiplier.  For example, an image measurement reference value of 32 with a \"Reference value multiplier\" of \"0.5\" will see all objects compared against a value of 16.  This is useful when comparing to dynamic values coming from image and parent object measurements."
                },
                {
                  "name": "Store summary filter results",
                  "description": "When selected, a metadata value is stored in the workspace, which records the number of objects which failed the filter and were removed or moved to another object class (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Store individual filter results",
                  "description": "When selected, each input object will be assigned a measurement reporting if that object passed or failed the filter.  The measurement value is \"1\" for objects that failed the filter (i.e. would be removed if the relevant removal setting was enabled) and \"0\" for objects that passed (i.e. wouldn't be removed)."
                },
                {
                  "name": "Partner objects",
                  "description": "Objects will be filtered against the number of partners they have from this object collection."
                }
              ],
              "slug": "number-of-partners"
            },
            {
              "path": "/modules/objects/filter/object-proximity",
              "deprecated": false,
              "name": "Object proximity",
              "shortDescription": "Filters objects in close XY proximity based on a specific measurement.",
              "fullDescription": "Filters objects in close XY proximity based on a specific measurement.  For two, or more, objects within close proximity of each other the object with the largest (or smallest) measurement will be retained, whilst the others will be removed.  This can be used to filter instances of the same object being detected multiple times.  Distances are only considered in XY.  Any Z-axis information on object position will be ignored.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Reference mode",
                  "description": "Controls the method used for determining the nearest neighbour distances:<br><ul><li>\"Centroid (2D)\" Distances are between the input and neighbour object centroids, but only in the XY plane.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Centroid (3D)\" Distances are between the input and neighbour object centroids.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Surface (2D)\" Distances are between the closest points on the input and neighbour surfaces, but only in the XY plane.  These distances increase in magnitude the greater the minimum input-neighbour object surface distance is; however, they are assigned a positive value if the closest input object surface point is outside the neighbour and a negative value if the closest input object surface point is inside the neighbour.  For example, a closest input object surface point 5px outside the neighbour will be simply \"5px\", whereas a closest input object surface point 5px from the surface, but contained within the neighbour object will be recorded as \"-5px\".  Note: Any instances where the input and neighbour object surfaces overlap will be recorded as \"0px\" distance.<\/li><li>\"Surface (3D)\" Distances are between the closest points on the input and neighbour surfaces.  These distances increase in magnitude the greater the minimum input-neighbour object surface distance is; however, they are assigned a positive value if the closest input object surface point is outside the neighbour and a negative value if the closest input object surface point is inside the neighbour.  For example, a closest input object surface point 5px outside the neighbour will be simply \"5px\", whereas a closest input object surface point 5px from the surface, but contained within the neighbour object will be recorded as \"-5px\".  Note: Any instances where the input and neighbour object surfaces overlap will be recorded as \"0px\" distance.<\/li><\/ul>"
                },
                {
                  "name": "Minimum separation",
                  "description": "Minimum allowed distance in XY plane for two objects to co-exist.  Any objects with XY separation smaller than this value will be subject to filtering, where the \"less suitable\" (depending on filter settings) object will be removed."
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, object-object distances are to be specified in calibrated units; otherwise, units are specified in pixels."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, objects must be in the same time frame for them to be linked."
                },
                {
                  "name": "Method for filtering",
                  "description": "For objects closer than the value specified by \"Minimum separation\" this parameter controls which will be retained."
                },
                {
                  "name": "Measurement to filter on",
                  "description": "Objects will be filtered against their value of this measurement.  Objects missing this measurement are not removed; however, they can be removed by using the module \"With / without measurement\"."
                }
              ],
              "slug": "object-proximity"
            },
            {
              "path": "/modules/objects/filter/objects-with-specific-ids",
              "deprecated": false,
              "name": "Objects with specific IDs",
              "shortDescription": "Filter an object collection based on user-defined list of object ID numbers.",
              "fullDescription": "Filter an object collection based on user-defined list of object ID numbers.  When the module executes, the user is presented with a dialog box where they can enter a comma-separated list of object IDs to remove.  Once the list is complete, the user presses \"OK\" to proceed.  All objects with ID numbers matching those in the list can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  To assist with selection of ID numbers, an optional image can be displayed - this could be pre-prepared to display object ID numbers using the \"Add labels\" module.  The number of objects specified for removal can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Show image",
                  "description": "When selected, a specific image will be displayed when this module executes.  This can be used to display a pre-prepared, object ID-labelled image to the user, thus acting as a reference for which object IDs to remove.  The image to be displayed is set using the \"Image to display\" parameter."
                },
                {
                  "name": "Image to display",
                  "description": "Image to display when the module executes.  For example, this could be a pre-prepared image with object IDs inserted as text overlays using the \"Add labels\" module."
                },
                {
                  "name": "Store filter results",
                  "description": "When selected, the number of removed (or moved) objects is counted and stored as a metadata item (name in the format \"FILTER // NUM_[inputObjectsName]_BY_ID\")."
                }
              ],
              "slug": "objects-with-specific-ids"
            },
            {
              "path": "/modules/objects/filter/remove-on-image-edge",
              "deprecated": false,
              "name": "Remove on image edge",
              "shortDescription": "Filter an object collection based on contact of each object with the image edge.",
              "fullDescription": "Filter an object collection based on contact of each object with the image edge.  Contact is considered as a case where an object pixel is in the outer-most row, column or slice  of an image (e.g. x = 0 or y = max_value).  The maximum number of contact pixels before an object is removed can be set to permit a degree of contact.  Objects identified as being in contact with the image edge can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.  <br><br>Image edge filters can be used when counting the number of objects in a field of view - in this case, typically two adjacent edges are removed (e.g. bottom and right) to prevent over-counting.  Alternatively, removing objects on all edges can be performed when measuring whole-object properties such as area or volume to prevent under-measuring values.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Maximum permitted contact",
                  "description": "Maximum number of object pixels which can lie along any of the specified edges without the object being removed.  This provides tolerance for objects which only just make contact with the image edge."
                },
                {
                  "name": "Remove on top",
                  "description": "When selected, object pixels which make contact with the top of the image (y = 0) will count towards the \"Maximum permitted contact\" limit.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal)."
                },
                {
                  "name": "Remove on left",
                  "description": "When selected, object pixels which make contact with the left side of the image (x = 0) will count towards the \"Maximum permitted contact\" limit.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal)."
                },
                {
                  "name": "Remove on bottom",
                  "description": "When selected, object pixels which make contact with the bottom of the image (y = max_value) will count towards the \"Maximum permitted contact\" limit.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal)."
                },
                {
                  "name": "Remove on right",
                  "description": "When selected, object pixels which make contact with the right side of the image (x = max_value) will count towards the \"Maximum permitted contact\" limit.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal)."
                },
                {
                  "name": "Include Z-position",
                  "description": "When selected, object pixels which make contact with the lower (z = 0) and upper (z = max_value) slices of the image stack will count towards the \"Maximum permitted contact\" limit.  If not selected, pixels along this edge will be ignored (i.e. contact won't lead to object removal).  If enabled for single slice stacks all objects will removed."
                },
                {
                  "name": "Store filter results",
                  "description": "When selected, the number of removed (or moved) objects is counted and stored as a metadata item (name in the format \"FILTER // NUM_[inputObjectsName] TOUCHING_IM_EDGE (3D)\")."
                }
              ],
              "slug": "remove-on-image-edge"
            },
            {
              "path": "/modules/objects/filter/with--without-measurement",
              "deprecated": false,
              "name": "With / without measurement",
              "shortDescription": "Filter an object collection based on the presence of a specific measurement for each object.",
              "fullDescription": "Filter an object collection based on the presence of a specific measurement for each object.  Objects which do/don't have the relevant measurement can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Controls whether objects are removed when a specific measurement is present or not:<br><br>- \"Remove objects without measurement\" Objects without the measurement specified by \"Measurement to filter on\" are removed, counted or moved (depending on the \"Filter mode\" parameter).<br><br>- \"Remove objects with measurement\" Objects with the measurement specified by \"Measurement to filter on\" are removed, counted or moved (depending on the \"Filter mode\" parameter).<br>"
                },
                {
                  "name": "Measurement to filter on",
                  "description": "Measurement to filter by.  The presence or absence of this measurement will determine which of the input objects are counted, removed or moved (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Store filter results",
                  "description": "When selected, the number of removed (or moved) objects is counted and stored as a metadata item (name in the format \"FILTER // NUM_[inputObjectsName] WITHOUT [measurementName] MEASUREMENT\")."
                }
              ],
              "slug": "with--without-measurement"
            },
            {
              "path": "/modules/objects/filter/with--without-parent",
              "deprecated": false,
              "name": "With / without parent",
              "shortDescription": "Filter an object collection based on the presence of a specific parent for each object.",
              "fullDescription": "Filter an object collection based on the presence of a specific parent for each object.  Objects which do/don't have the relevant parent can be removed from the input collection, moved to another collection (and removed from the input collection) or simply counted (but retained in the input collection).  The number of objects failing the filter can be stored as a metadata value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be filtered."
                },
                {
                  "name": "Filter mode",
                  "description": "Controls what happens to objects which don't pass the filter:<br><ul><li>\"Do nothing\" Retains all input objects, irrespective of whether they passed or failed the filter.  This is useful when also storing the filter results as metadata values (i.e. just counting the number of objects which pass the filter).<\/li><li>\"Move filtered objects to new class\" Objects failing the filter are moved to a new object class.  The name of the class is determined by the \"Output (filtered) objects\" parameter.  All existing measurements and relationships are carried forward into the new object collection.<\/li><li>\"Remove filtered objects\" (default) Removes objects failing the filter.  Once removed, these objects are unavailable for further use by modules and won't be included in exported results.<\/li><\/ul>"
                },
                {
                  "name": "Output (filtered) objects",
                  "description": "New object collection containing input objects which did not pass the filter.  These objects are only stored if \"Filter mode\" is set to \"Move filtered objects to new class\"."
                },
                {
                  "name": "Method for filtering",
                  "description": "Controls whether objects are removed when a specific parent object is present or not:<br><br>- \"Remove objects without parent\" Objects without the parent specified by \"Parent object\" are removed, counted or moved (depending on the \"Filter mode\" parameter).<br><br>- \"Remove objects with parent\" Objects with the parent specified by \"Parent object\" are removed, counted or moved (depending on the \"Filter mode\" parameter).<br>"
                },
                {
                  "name": "Parent object",
                  "description": "Parent object to filter by.  The presence or absence of this relationship will determine which of the input objects are counted, removed or moved (depending on the \"Filter mode\" parameter)."
                },
                {
                  "name": "Store filter results",
                  "description": "When selected, the number of removed (or moved) objects is counted and stored as a metadata item (name in the format \"FILTER // NUM_[inputObjectsName] WITHOUT [parentObjectsName] PARENT\")."
                }
              ],
              "slug": "with--without-parent"
            }
          ]
        },
        {
          "path": "/modules/objects/measure",
          "name": "Measure",
          "description": "Operations making measurements of individual objects in the workspace.  Measurements are associated with the relevant input objects.",
          "slug": "measure",
          "subCategories": [
            {
              "path": "/modules/objects/measure/intensity",
              "name": "Intensity",
              "description": "Modules performing intensity-based measurements on objects in the workspace.",
              "slug": "intensity",
              "subCategories": [],
              "modules": [
                {
                  "path": "/modules/objects/measure/intensity/measure-intensity-along-path",
                  "deprecated": false,
                  "name": "Measure intensity along path",
                  "shortDescription": "Measures the intensity profile along the pixel-wide backbone of an object and outputs this profile to .",
                  "fullDescription": "Measures the intensity profile along the pixel-wide backbone of an object and outputs this profile to .xlsx file.  Input objects are skeletonised to single pixel-wide representations prior to measurement; however, pre-skeletonised objects can also be processed.<br><br>Output results are stored in a multi-sheet .xlsx file, where each sheet includes the profile for a specific input image.  Each row of a sheet contains the profile for a single object.  Profiles are linearly-interpolated such that each measured position along a profile is 1px from the previous.<br><br>Note: Objects must either form a single line (i.e. not contain multiple branches) or reduce to a single line during skeletonisation.  No profile will be recorded for any objects which fail this requirement.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Save location",
                      "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
                    },
                    {
                      "name": "Mirrored directory root",
                      "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
                    },
                    {
                      "name": "File path",
                      "description": "Path to folder where images will be saved."
                    },
                    {
                      "name": "File name (generic)",
                      "description": ""
                    },
                    {
                      "name": "Save name mode",
                      "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
                    },
                    {
                      "name": "File name",
                      "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
                    },
                    {
                      "name": "Append series mode",
                      "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
                    },
                    {
                      "name": "Append date/time mode",
                      "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
                    },
                    {
                      "name": "Add filename suffix",
                      "description": "A custom suffix to be added to each filename."
                    },
                    {
                      "name": "Input objects",
                      "description": "Objects for which intensity profiles will be generated."
                    },
                    {
                      "name": "Measure another image",
                      "subParameters": [{
                        "name": "Input image",
                        "description": "Image for which the intensity profile will be measured.  Results from this profile will be added to a separate sheet in the .xlsx file."
                      }],
                      "description": "Include another image from the workspace to be measured.  Each separate image will be measured at the same spatial points and be saved to a separate sheet of the .xlsx file."
                    },
                    {
                      "name": "Include centroids",
                      "description": "Include columns recording the XYZ object centroid in pixel (or slice) units."
                    },
                    {
                      "name": "Include timepoints",
                      "description": "Include a column recording the timepoint that the objects were present in."
                    }
                  ],
                  "slug": "measure-intensity-along-path"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-gini-coefficient",
                  "deprecated": false,
                  "name": "Measure object Gini coefficient",
                  "shortDescription": "Measure the Gini coefficient of all pixels within each object on an object-by-object basis.",
                  "fullDescription": "Measure the Gini coefficient of all pixels within each object on an object-by-object basis.  The Gini coefficient measures the inequality in intensity of pixels.  A coefficient of 0 indicates perfect intensity homogeneity (all pixels with the same value), while a value of 1 indicates the maximum possible inequality (all pixels are black, except for a single bright pixel).",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": ""
                    },
                    {
                      "name": "Input image",
                      "description": ""
                    }
                  ],
                  "slug": "measure-object-gini-coefficient"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-colocalisation",
                  "deprecated": false,
                  "name": "Measure object colocalisation",
                  "shortDescription": "Calculates colocalisation of two input images individually for each object.",
                  "fullDescription": "Calculates colocalisation of two input images individually for each object.  Measurements for each object only consider pixels within that object.  All measurements are associated with the relevant object.  Colocalisation analysis has many potential pitfalls, so users are advised to read the Fiji <a href=\"https://imagej.net/imaging/colocalization-analysis\">Colocalization analysis<\/a> page and/or the <a href=\"https://journals.physiology.org/doi/full/10.1152/ajpcell.00462.2010\">Dunn et al 2011 review<\/a>.<br><br>All calculations are performed using the <a href=\"https://imagej.net/plugins/coloc-2\">Coloc2 plugin<\/a>.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects for which colocalisation will be measured.  For each object, colocalisation will be independently measured for the pixels coincident with the object's coordinates.  Measurements will be associated with the corresponding object."
                    },
                    {
                      "name": "Input image 1",
                      "description": "First image for which colocalisation will be calculated."
                    },
                    {
                      "name": "Input image 2",
                      "description": "Second image for which colocalisation will be calculated."
                    },
                    {
                      "name": "Thresholding mode",
                      "description": "Controls how the thresholds for measurements such as Manders' are set:<br><ul><li>\"Bisection (correlation)\" A faster method to calculate thresholds than the Costes approach.<\/li><li>\"Costes (correlation)\" The \"standard\" method to calculate thresholds for Manders' colocalisation measures.  This approach sets the thresholds for the two input images such that the pixels with intensities lower than their respective thresholds don't have any statistical correlation (i.e. have PCC values less than or equal to 0).  This is based on Costes' 2004 paper (Costes et al., <i>Biophys. J.<\/i> <b>86<\/b> (2004) 3993\u20134003.<\/li><li>\"Image measurements\" Thresholds for each image will be set equal to measurements associated with each object.<\/li><li>\"Manual\" Threshold values are manually set from user-defined values (\"Threshold (C1)\" and \"Threshold (C2)\" parameters).<\/li><li>\"None\" No threshold is set.  In this instance, Manders' metrics will only be calculated above zero intensity rather than both above zero and above the thresholds.  Similarly, Pearson's correlation coefficients will only be calculated for the entire region (after masking) rather than also for above and below the thresholds.<\/li><\/ul>"
                    },
                    {
                      "name": "Image measurement (C1)",
                      "description": "If \"Thresholding mode\" is set to \"Image measurements\", this is the measurement associated with \"Input image 1\" that will be applied to the first image."
                    },
                    {
                      "name": "Image measurement (C2)",
                      "description": "If \"Thresholding mode\" is set to \"Image measurements\", this is the measurement associated with \"Input image 2\" that will be applied to the second image."
                    },
                    {
                      "name": "Threshold (C1)",
                      "description": "If \"Thresholding mode\" is set to \"Manual\", this is the threshold that will be applied to the first image."
                    },
                    {
                      "name": "Threshold (C2)",
                      "description": "If \"Thresholding mode\" is set to \"Manual\", this is the threshold that will be applied to the second image."
                    },
                    {
                      "name": "PCC implementation",
                      "description": "Controls whether PCC should be calculated using the classic algorithm or using the Coloc2-default \"fast\" method."
                    },
                    {
                      "name": "Measure Kendall's Rank Correlation",
                      "description": "When selected, Kendall's rank correlation will be calculated.  This works in a similar manner to Pearson's PCC, except it's calculated on ranked data rather than raw pixel intensities."
                    },
                    {
                      "name": "Measure Li's ICQ",
                      "description": "When selected, Li's ICQ (intensity correlation quotient) will be calculated.  This measure reports the frequency with which both corresponding pixels for both channels are either both above or both below their respective means.  Values are scaled into the range -0.5 to +0.5, with values below 0 corresponding to anti-correlation and values above 0 indicating correlation."
                    },
                    {
                      "name": "Measure Manders' Correlation",
                      "description": "When selected, Manders' M1 and M2 coefficients will be calculated.  \"Proportional to the amount of fluorescence of the colocalizing pixels or voxels in each colour channel. You can get more details in Manders et al. Values range from 0 to 1, expressing the fraction of intensity in a channel that is located in pixels where there is above zero (or threshold) intensity in the other colour channel.\" Description taken from <a href=\"https://imagej.net/imaging/colocalization-analysis\">https://imagej.net/imaging/colocalization-analysis<\/a>"
                    },
                    {
                      "name": "Measure PCC",
                      "description": "When selected, Pearson's Correlation Coefficient (PCC) will be calculated.  \"It is not sensitive to differences in mean signal intensities or range, or a zero offset between the two components. The result is +1 for perfect correlation, 0 for no correlation, and -1 for perfect anti-correlation. Noise makes the value closer to 0 than it should be.\" Description taken from <a href=\"https://imagej.net/imaging/colocalization-analysis\">https://imagej.net/imaging/colocalization-analysis<\/a>"
                    },
                    {
                      "name": "Measure Spearman's Rank Correlation",
                      "description": "When selected, Spearman's rank correlation will be calculated.  Spearman's rho is calculated in a similar manner to Pearson's PCC, except the image intensities are replaced by their respective rank.  Spearman's correlation works with monotonic relationships.  As with PCC, values are in the range -1 to +1."
                    }
                  ],
                  "slug": "measure-object-colocalisation"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-greyscale-k-function",
                  "deprecated": false,
                  "name": "Measure object greyscale K-function",
                  "shortDescription": "Measure's Ripley's K-function for greyscale images on an object-by-object basis.",
                  "fullDescription": "Measure's Ripley's K-function for greyscale images on an object-by-object basis.  This method is re-written from the publication \"Extending Ripley\u2019s K-Function to Quantify Aggregation in 2-D Grayscale Images\" by M. Amgad, et al. (doi: 10.1371/journal.pone.0144404).  Results are output to an Excel spreadsheet, with one file per input image.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Save location",
                      "description": ""
                    },
                    {
                      "name": "Mirrored directory root",
                      "description": ""
                    },
                    {
                      "name": "File path",
                      "description": ""
                    },
                    {
                      "name": "File name (generic)",
                      "description": ""
                    },
                    {
                      "name": "Save name mode",
                      "description": ""
                    },
                    {
                      "name": "File name",
                      "description": ""
                    },
                    {
                      "name": "Append series mode",
                      "description": ""
                    },
                    {
                      "name": "Append date/time mode",
                      "description": ""
                    },
                    {
                      "name": "Add filename suffix",
                      "description": ""
                    },
                    {
                      "name": "Input objects",
                      "description": ""
                    },
                    {
                      "name": "Input image",
                      "description": ""
                    },
                    {
                      "name": "Minimum radius (px)",
                      "description": ""
                    },
                    {
                      "name": "Maximum radius (px)",
                      "description": ""
                    },
                    {
                      "name": "Radius increment (px)",
                      "description": ""
                    }
                  ],
                  "slug": "measure-object-greyscale-k-function"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-intensity",
                  "deprecated": false,
                  "name": "Measure object intensity",
                  "shortDescription": "Measure intensity of each object in a specified image.",
                  "fullDescription": "Measure intensity of each object in a specified image.  Measurements of intensity are taken at all pixel coordinates corresponding to each object.  By default, basic measurements such as mean, minimum and maximum will be calculated.  Additional measurements can optionally be enabled.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects from the workspace for which intensities will be measured."
                    },
                    {
                      "name": "Input image",
                      "description": "Image from which pixel intensities will be measured.  This image can be 8-bit, 16-bit or 32-bit.  Measurements are always taken from the first channel if more than one channel is present (to measure additional channels, please first use the \"Extract substack\" module)."
                    },
                    {
                      "name": "Measure weighted centre",
                      "description": "When selected, the intensity-weighted centroid of each input object will be calculated.  With this, the greater the intensity in a particular region of an object, the more the \"centre of mass\" will be drawn towards it."
                    }
                  ],
                  "slug": "measure-object-intensity"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-intensity-orientation",
                  "deprecated": false,
                  "name": "Measure object intensity orientation",
                  "shortDescription": "Calculates the orientation of structures in each object for a specific image.",
                  "fullDescription": "Calculates the orientation of structures in each object for a specific image.  This module uses the <a href=\"https://imagej.net/plugins/directionality\">Directionality_<\/a> plugin to calculate core measures.  Additional measurements, such as the Alignment Index [1] are also calculated.  All measurements are made for all slices within an object; that is, the individual slice histograms are merged and normalised prior to calculation of all measurements.<br><br>References:<br><ol><li>Sun, M., et al. \"Rapid Quantification of 3D Collagen Fiber Alignment and Fiber Intersection Correlations with High Sensitivity\" <i>PLOS ONE<\/i> (2015), doi: https://doi.org/10.1371/journal.pone.0131814<\/li><\/ol>",
                  "version": "1.1.0",
                  "parameters": [
                    {
                      "name": "Save location",
                      "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
                    },
                    {
                      "name": "Mirrored directory root",
                      "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
                    },
                    {
                      "name": "File path",
                      "description": "Path to folder where images will be saved."
                    },
                    {
                      "name": "File name (generic)",
                      "description": ""
                    },
                    {
                      "name": "Save name mode",
                      "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
                    },
                    {
                      "name": "File name",
                      "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
                    },
                    {
                      "name": "Append series mode",
                      "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
                    },
                    {
                      "name": "Append date/time mode",
                      "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
                    },
                    {
                      "name": "Add filename suffix",
                      "description": "A custom suffix to be added to each filename."
                    },
                    {
                      "name": "Input image",
                      "description": ""
                    },
                    {
                      "name": "Input objects",
                      "description": ""
                    },
                    {
                      "name": "Method",
                      "description": ""
                    },
                    {
                      "name": "Number of bins",
                      "description": ""
                    },
                    {
                      "name": "Histogram start ()",
                      "description": ""
                    },
                    {
                      "name": "Histogram end ()",
                      "description": ""
                    },
                    {
                      "name": "Include bin range in name",
                      "description": ""
                    },
                    {
                      "name": "Include number of bins in name",
                      "description": ""
                    },
                    {
                      "name": "Save histogram",
                      "description": ""
                    },
                    {
                      "name": "Histogram grouping",
                      "description": ""
                    },
                    {
                      "name": "Parent objects name",
                      "description": ""
                    }
                  ],
                  "slug": "measure-object-intensity-orientation"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-object-texture",
                  "deprecated": false,
                  "name": "Measure object texture",
                  "shortDescription": "Calculates Haralick's texture features for each object in a collection for a specific image.",
                  "fullDescription": "Calculates Haralick's texture features for each object in a collection for a specific image.  Each point in each object is compared to a corresponding point, a defined offset away (e.g. x-offset = 1, y-offset=0, z-offset=0 to compare to the pixel immediately right of each pixel).  The intensities of each point pair are added to a 2D gray-level co-occurrence matrix (GLCM) from which measures of angular second moment, contrast, correlation and entropy can be calculated.<br><br>Robert M Haralick; K Shanmugam; Its'hak Dinstein, \"Textural Features for Image Classification\" <i>IEEE Transactions on Systems, Man, and Cybernetics. SMC-3<\/i> (1973) <b>6<\/b> 610\u2013621.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects from the workspace for which the corresponding texture of the image (specified by \"Input image\") will be calculated.  Textures will be calculated for each coordinate of each object and will include instances where the corresponding point (the intensity at the specified offset) is outside the object.  Texture measurements will be assigned to the relevant objects."
                    },
                    {
                      "name": "Input image",
                      "description": "Image from the workspace from which texture metrics for each object will be calculated."
                    },
                    {
                      "name": "X-offset",
                      "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the x-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                    },
                    {
                      "name": "Y-offset",
                      "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the y-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                    },
                    {
                      "name": "Z-offset",
                      "description": "Each pixel in the input image will be compared to the pixel a defined offset-away.  This parameter controls the z-axis offset.  Offset specified in pixel units unless \"Calibrated offset\" is selected.  If using calibrated units, the offset will be rounded to the closest integer value."
                    },
                    {
                      "name": "Calibrated offset",
                      "description": "When selected, offsets are specified in calibrated units.  Otherwise, offsets are assumed to be in pixel units."
                    }
                  ],
                  "slug": "measure-object-texture"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-spot-intensity",
                  "deprecated": true,
                  "name": "Measure spot intensity",
                  "shortDescription": "DEPRECATED: Please use separate \"Get local object region\" and \"Measure object intensity\" modules.",
                  "fullDescription": "DEPRECATED: Please use separate \"Get local object region\" and \"Measure object intensity\" modules.<br><br>Measures the intensity of an image for a circular (2D object*) or spherical (3D object*) region coincident with the mean centroid of each object in a specified object collection.  Measurements are associated with the corresponding input objects.  The radius of the measurement region can be specified as a fixed value or determined on an object-by-object basis from associated object (or parent) measurements.<br><br>Note: This module differs from the \"Measure object intensity\" module, which measures the intensity of all coordinates of an object.<br><br>* 2D objects are defined as objects identified from a single-slice image.  Objects with coordinates confined to a single plane, but identified from a 3D image stack are still considered 3D objects.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input image",
                      "description": "Image from the workspace to measure the intensity of."
                    },
                    {
                      "name": "Input spot objects",
                      "description": "Object collection from the workspace for which spot intensities will be measured.  One spot will be measured for each object."
                    },
                    {
                      "name": "Calibrated units",
                      "description": "When selected, spot radius values (irrespective of whether they are fixed values, measurements or parent measurements) are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                    },
                    {
                      "name": "Radius value source",
                      "description": "Controls how the radius of the spot is defined:<br><ul><li>\"Fixed value\" A single radius, defined by \"Fixed value\" will be used for all objects.<\/li><li>\"Measurement\" The radius will be equal to the value of a measurement (specified by \"Radius measurement\") associated with the object being measured.  Radii will potentially be different for each object.<\/li><li>\"Parent measurement\" The radius will be equal to the value of a measurement (specified by \"Parent radius measurement\") associated a parent of the object being measured (specified by \"Parent object\").  Radii will potentially be different for each object..<\/li><\/ul>"
                    },
                    {
                      "name": "Fixed value",
                      "description": "Fixed spot radius to use for all object measurements when \"Radius value source\" is in \"Fixed value\" mode."
                    },
                    {
                      "name": "Radius measurement",
                      "description": "Measurement associated with the input object.  This will be used as spot the radius for spot intensity measurements when \"Radius value source\" is in \"Measurement\" mode."
                    },
                    {
                      "name": "Parent object",
                      "description": "Parent object of the input object being measured.  This parent will provide the measurement (specified by \"Parent radius measurement\") to be used as the spot radius for spot intensity measurements when \"Radius value source\" is in \"Parent measurement\" mode."
                    },
                    {
                      "name": "Parent radius measurement",
                      "description": "Measurement associated with a parent of the input object.  This will be used as the spot radius for spot intensity measurements when \"Radius value source\" is in \"Parent measurement\" mode."
                    },
                    {
                      "name": "Measure mean",
                      "description": "When selected, the mean intensity of all coordinates in the spot is calculated and stored as a measurement associated with the input object."
                    },
                    {
                      "name": "Measure minimum",
                      "description": "When selected, the minimum intensity of all coordinates in the spot is calculated and stored as a measurement associated with the input object."
                    },
                    {
                      "name": "Measure maximum",
                      "description": "When selected, the maximum intensity of all coordinates in the spot is calculated and stored as a measurement associated with the input object."
                    },
                    {
                      "name": "Measure standard deviation",
                      "description": "When selected, the standard deviation of intensity of all coordinates in the spot is calculated and stored as a measurement associated with the input object."
                    },
                    {
                      "name": "Measure sum",
                      "description": "When selected, the summed intensity of all coordinates in the spot is calculated and stored as a measurement associated with the input object."
                    }
                  ],
                  "slug": "measure-spot-intensity"
                },
                {
                  "path": "/modules/objects/measure/intensity/measure-texture-along-path",
                  "deprecated": false,
                  "name": "Measure texture along path",
                  "shortDescription": "Texture measures, largely from  Robert M.",
                  "fullDescription": "Texture measures, largely from  Robert M. Haralick, K. Shanmugam, and Its'hak Dinstein, \"Textural Features for Image Classification\", IEEE Transactions on Systems, Man, and Cybernetics, 1973, SMC-3 (6): 610\u2013621",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects for which image texture along will be generated along the object backbone."
                    },
                    {
                      "name": "Input image",
                      "description": "Image for which the texture will be measured."
                    },
                    {
                      "name": "Offset (px)",
                      "description": "Each point along the object backbone will have its intensity compared to another point, this offset away along the backbone."
                    }
                  ],
                  "slug": "measure-texture-along-path"
                }
              ]
            },
            {
              "path": "/modules/objects/measure/miscellaneous",
              "name": "Miscellaneous",
              "description": "General object measurements, such as counting the number of child objects or binning measurements.",
              "slug": "miscellaneous",
              "subCategories": [],
              "modules": [
                {
                  "path": "/modules/objects/measure/miscellaneous/apply-weka-object-classification",
                  "deprecated": false,
                  "name": "Apply Weka object classification",
                  "shortDescription": "Apply a previously-prepared WEKA object classifier to a specified object collection from the workspace.",
                  "fullDescription": "Apply a previously-prepared WEKA object classifier to a specified object collection from the workspace.  Classification can be based on a range of measurements associated with the input objects.  All measurements used to create this model should be present in the input objects and have the same names (i.e. measurement names shouldn't be changed during preparation of training data).<br><br>The probability of each input object belonging to each class is output as a measurement associated with that object.  Each object also has a class index (based on the order the classes are listed in the .model file) indicating the most probable class that object belongs to.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Input objects from workspace which will be classified based on model specified by \"Classifier path\" parameter."
                    },
                    {
                      "name": "Classifier path",
                      "description": "WEKA model (.model extension) that will be used to classify input objects based on a variety of measurements.  This model must be created in the <a href=\"https://www.cs.waikato.ac.nz/ml/index.html\">WEKA software<\/a>.  All measurements used to create this model should be present in the input objects and have the same names (i.e. measurement names shouldn't be changed during preparation of training data)."
                    },
                    {
                      "name": "Apply normalisation",
                      "description": "When selected, measurements will be normalised (set to the range 0-1) within their respective classes."
                    }
                  ],
                  "slug": "apply-weka-object-classification"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/bin-objects-by-measurement",
                  "deprecated": false,
                  "name": "Bin objects by measurement",
                  "shortDescription": "Distribute objects into a set of \"bins\" based on a measurement associated with each object.",
                  "fullDescription": "Distribute objects into a set of \"bins\" based on a measurement associated with each object.  Bins are evenly distributed between manually-specified smallest and largest bin centres.  The assigned bin for each object is stored as a new measurement associated with that object.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects from the workspace.  The specified measurement for each of these will be binned according to various parameters.  The assigned bin will be stored as a new measurement associated with this object."
                    },
                    {
                      "name": "Measurement",
                      "description": "Measurement associated with the input objects that will be binned according to the other parameters."
                    },
                    {
                      "name": "Smallest bin centre",
                      "description": "Centre value associated with the smallest bin.  Bins will be evenly distributed in bins between this value and the upper bin centre (specified by \"Largest bin centre\"."
                    },
                    {
                      "name": "Largest bin centre",
                      "description": "Centre value associated with the largest bin.  Bins will be evenly distributed in bins between this value and the lower bin centre (specified by \"Smallest bin centre\"."
                    },
                    {
                      "name": "Number of bins",
                      "description": "Number of bins to divide measurements into.  These will be evenly distributed in the range between \"Smallest bin centre\" and \"Largest bin centre\"."
                    }
                  ],
                  "slug": "bin-objects-by-measurement"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/calculate-statistics-for-children",
                  "deprecated": false,
                  "name": "Calculate statistics for children",
                  "shortDescription": "Calculates statistics for a measurement associated with all child objects of parent object.",
                  "fullDescription": "Calculates statistics for a measurement associated with all child objects of parent object.  The calculated statistics are stored as new measurements, associated with the relevant parent object.  For example, calculating the summed volume of all child objects (from a specified collection) of each parent object.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Parent objects",
                      "description": "Input object collection from the workspace for which statistics of child object measurements will be calculated.  This object collection is a parent to those selected by the \"Child objects\" parameter.  Statistics for one measurement associated with all children of each input parent object will be calculated and added to this object as a new measurement."
                    },
                    {
                      "name": "Child objects",
                      "description": "Input object collection from the workspace, where these objects are children of the collection selected by the \"Parent objects\" parameter.)"
                    },
                    {
                      "name": "Measurement",
                      "description": "Measurement associated with the child objects for which statistics will be calculated.  Statistics will be calculated for all children of a parent object."
                    },
                    {
                      "name": "Calculate mean",
                      "description": "When selected, the mean value of the measurements will be calculated and added to the relevant parent object."
                    },
                    {
                      "name": "Calculate median",
                      "description": "When selected, the median value of the measurements will be calculated and added to the relevant parent object."
                    },
                    {
                      "name": "Calculate standard deviation",
                      "description": "When selected, the standard deviation of the measurements will be calculated and added to the relevant parent object."
                    },
                    {
                      "name": "Calculate minimum",
                      "description": "When selected, the minimum value of the measurements will be calculated and added to the relevant parent object."
                    },
                    {
                      "name": "Calculate maximum",
                      "description": "When selected, the maximum value of the measurements will be calculated and added to the relevant parent object."
                    },
                    {
                      "name": "Calculate sum",
                      "description": "When selected, the sum of the measurements will be calculated and added to the relevant parent object."
                    }
                  ],
                  "slug": "calculate-statistics-for-children"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/calculate-statistics-for-partners",
                  "deprecated": false,
                  "name": "Calculate statistics for partners",
                  "shortDescription": "Calculates statistics for a measurement associated with all partner objects of an input object.",
                  "fullDescription": "Calculates statistics for a measurement associated with all partner objects of an input object.  The calculated statistics are stored as new measurements, associated with the relevant input object.  For example, calculating the summed volume of all partner objects (from a specified collection) of each input object.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Input object collection from the workspace for which statistics of partner object measurements will be calculated.  This object collection is a partner to those selected by the \"Partner objects\" parameter.  Statistics for one measurement associated with all partners of each input object will be calculated and added to this object as a new measurement."
                    },
                    {
                      "name": "Partner objects",
                      "description": "Input object collection from the workspace, where these objects are partners of the collection selected by the \"Input objects\" parameter.)"
                    },
                    {
                      "name": "Measurement",
                      "description": "Measurement associated with the partner objects for which statistics will be calculated.  Statistics will be calculated for all partners of an input object."
                    },
                    {
                      "name": "Calculate mean",
                      "description": "When selected, the mean value of the measurements will be calculated and added to the relevant input object."
                    },
                    {
                      "name": "Calculate median",
                      "description": "When selected, the median value of the measurements will be calculated and added to the relevant input object."
                    },
                    {
                      "name": "Calculate standard deviation",
                      "description": "When selected, the standard deviation of the measurements will be calculated and added to the relevant input object."
                    },
                    {
                      "name": "Calculate minimum",
                      "description": "When selected, the minimum value of the measurements will be calculated and added to the relevant input object."
                    },
                    {
                      "name": "Calculate maximum",
                      "description": "When selected, the maximum value of the measurements will be calculated and added to the relevant input object."
                    },
                    {
                      "name": "Calculate sum",
                      "description": "When selected, the sum of the measurements will be calculated and added to the relevant input object."
                    }
                  ],
                  "slug": "calculate-statistics-for-partners"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/child-object-count",
                  "deprecated": false,
                  "name": "Child object count",
                  "shortDescription": "Calculates the number of children from a specific class.",
                  "fullDescription": "Calculates the number of children from a specific class.  Measurements are assigned to all objects in the input collection.  Unlike normal measurements, this value is evaluated at the time of use, so should always be up to date.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "For each object in this collection the number of associated child objects (from the collection specified by \"Child objects\") will be calculated.  The count is stored as a measurement associated with each input object.  The measurement is evaluated at the time of access (unlike \"normal\" measurements which have fixed values), so should always be correct."
                    },
                    {
                      "name": "Child objects",
                      "description": "Child objects to be counted."
                    }
                  ],
                  "slug": "child-object-count"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/object-measurement-calculator",
                  "deprecated": false,
                  "name": "Object measurement calculator",
                  "shortDescription": "Perform a mathematical operation on measurements associated with each object of an object collection in the workspace.",
                  "fullDescription": "Perform a mathematical operation on measurements associated with each object of an object collection in the workspace.  The calculation can replace either or both values with fixed values, measurements associated with an image or a statistic of all measurements associated with another object collection (e.g. the mean volume of all objects).  The resulting measurements are associated with the corresponding input objects as new measurements.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Object collection from the workspace to perform the measurement calculation for.  The specified calculation will be performed once per object in the collection."
                    },
                    {
                      "name": "Value mode 1",
                      "description": "Controls how the first value in the calculation is defined:<br><ul><li>\"Fixed\" A single, fixed value defined by \"Fixed value 1\"is used.<\/li><li>\"Image measurement\" A measurement associated with an image (specified by \"Image 1\") and defined by \"Image measurement 1\" is used.<\/li><li>\"Measurement\" A measurement associated with the input object and defined by \"Measurement 1\" is used.<\/li><li>\"Object collection statistic\" A statistic (specified by \"Statistic mode 1\") of a measurement (specified by \"Reference measurement 1\") associated with all objects in an object collection (specified by \"Reference objects 1\") is used.  For example, the mean object volume.  Note: The object collection used to calculate the statistic doesn't have to be the same as the input object collection.<\/li><\/ul>"
                    },
                    {
                      "name": "Fixed value 1",
                      "description": "Fixed value to use in the calculation when \"Value mode 1\" is in \"Fixed\" mode."
                    },
                    {
                      "name": "Image 1",
                      "description": "A measurement associated with this image will be used in the calculated when \"Value mode 1\" is in \"Image measurement\" mode."
                    },
                    {
                      "name": "Image measurement 1",
                      "description": "Measurement associated with an image (specified by \"Image 1\") to use in the calculation when \"Value mode 1\" is in \"Image measurement\" mode."
                    },
                    {
                      "name": "Measurement 1",
                      "description": "Measurement, associated with the current object, to use in the calculation when \"Value mode 1\" is in \"Measurement\" mode."
                    },
                    {
                      "name": "Reference objects 1",
                      "description": "Object collection for which a statistic of an associated measurement will be used in the calculation when \"Value mode 1\" is in \"Object collection statistic\" mode."
                    },
                    {
                      "name": "Reference measurement 1",
                      "description": "Measurement associated with the objects in the collection specified by \"Reference objects 1\".  A statistic of all object measurements will be used in the calculation when \"Value mode 1\" is in \"Object collection statistic\" mode."
                    },
                    {
                      "name": "Statistic mode 1",
                      "description": "Statistic to apply to all measurements (specified by \"Reference measurement 1\") of an object collection (specified by \"Reference objects 1\").  The resulting value will be used in the calculation when \"Value mode 1\" is in \"Object collection statistic\" mode.  Choices are: First value, Last value, Minimum, Mean, Maximum, Range, Standard deviation, Sum."
                    },
                    {
                      "name": "Value mode 2",
                      "description": "Controls how the second value in the calculation is defined:<br><ul><li>\"Fixed\" A single, fixed value defined by \"Fixed value 2\"is used.<\/li><li>\"Image measurement\" A measurement associated with an image (specified by \"Image 2\") and defined by \"Image measurement 2\" is used.<\/li><li>\"Measurement\" A measurement associated with the input object and defined by \"Measurement 2\" is used.<\/li><li>\"Object collection statistic\" A statistic (specified by \"Statistic mode 2\") of a measurement (specified by \"Reference measurement 2\") associated with all objects in an object collection (specified by \"Reference objects 2\") is used.  For example, the mean object volume.  Note: The object collection used to calculate the statistic doesn't have to be the same as the input object collection.<\/li><\/ul>"
                    },
                    {
                      "name": "Fixed value 2",
                      "description": "Fixed value to use in the calculation when \"Value mode 2\" is in \"Fixed\" mode."
                    },
                    {
                      "name": "Image 2",
                      "description": "A measurement associated with this image will be used in the calculated when \"Value mode 2\" is in \"Image measurement\" mode."
                    },
                    {
                      "name": "Image measurement 2",
                      "description": "Measurement associated with an image (specified by \"Image 2\") to use in the calculation when \"Value mode 2\" is in \"Image measurement\" mode."
                    },
                    {
                      "name": "Measurement 2",
                      "description": "Measurement, associated with the current object, to use in the calculation when \"Value mode 2\" is in \"Measurement\" mode."
                    },
                    {
                      "name": "Reference objects 2",
                      "description": "Object collection for which a statistic of an associated measurement will be used in the calculation when \"Value mode 2\" is in \"Object collection statistic\" mode."
                    },
                    {
                      "name": "Reference measurement 2",
                      "description": "Measurement associated with the objects in the collection specified by \"Reference objects 2\".  A statistic of all object measurements will be used in the calculation when \"Value mode 2\" is in \"Object collection statistic\" mode."
                    },
                    {
                      "name": "Statistic mode 2",
                      "description": "Statistic to apply to all measurements (specified by \"Reference measurement 2\") of an object collection (specified by \"Reference objects 2\").  The resulting value will be used in the calculation when \"Value mode 2\" is in \"Object collection statistic\" mode.  Choices are: First value, Last value, Minimum, Mean, Maximum, Range, Standard deviation, Sum."
                    },
                    {
                      "name": "Output measurement",
                      "description": "The value resulting from the calculation will be stored as a new measurement with this name.  This output measurement will be associated with the corresponding object from the input object collection."
                    },
                    {
                      "name": "Calculation mode",
                      "description": "Calculation to perform.  Choices are: Add measurement 1 and measurement 2, Divide measurement 1 by measurement 2, Multiply measurement 1 and measurement 2, Subtract measurement 2 from measurement 1."
                    }
                  ],
                  "slug": "object-measurement-calculator"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/object-timepoint",
                  "deprecated": false,
                  "name": "Object timepoint",
                  "shortDescription": "Store object timepoint as a measurement.",
                  "fullDescription": "Store object timepoint as a measurement.  Timepoint counting starts at 0 (e.g. the third frame will have a timepoint of 2).",
                  "version": "1.0.0",
                  "parameters": [{
                    "name": "Input objects",
                    "description": "Objects from the workspace.  Each object in this collection will have the index of timepoint it's present in stored as a measurement.  Note: Timepoint indexing starts at 0."
                  }],
                  "slug": "object-timepoint"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/parent-object-id",
                  "deprecated": false,
                  "name": "Parent object ID",
                  "shortDescription": "Stores the ID number of an associated parent from a specific class.",
                  "fullDescription": "Stores the ID number of an associated parent from a specific class.  Associated IDs are stored as measurements and are assigned to all objects in the input collection.  Unlike normal measurements, this value is evaluated at the time of use, so should always be up to date.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "For each object in this collection the ID number of an associated parent object (from the collection specified by \"Parent object\") will be stored as a measurement.  This measurement will be associated with each input object.  The measurement is evaluated at the time of access (unlike \"normal\" measurements which have fixed values), so should always be correct."
                    },
                    {
                      "name": "Parent object",
                      "description": "Associated parent object collection."
                    }
                  ],
                  "slug": "parent-object-id"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/partner-object-count",
                  "deprecated": false,
                  "name": "Partner object count",
                  "shortDescription": "Calculates the number of partners from a specific class.",
                  "fullDescription": "Calculates the number of partners from a specific class.  Measurements are assigned to all objects in the input collection.  Unlike normal measurements, this value is evaluated at the time of use, so should always be up to date.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "For each object in this collection the number of associated partner objects (from the collection specified by \"Partner objects\") will be calculated.  The count is stored as a measurement associated with each input object.  The measurement is evaluated at the time of access (unlike \"normal\" measurements which have fixed values), so should always be correct."
                    },
                    {
                      "name": "Partner objects",
                      "description": "Partner objects to be counted."
                    }
                  ],
                  "slug": "partner-object-count"
                },
                {
                  "path": "/modules/objects/measure/miscellaneous/replace-measurement-value",
                  "deprecated": false,
                  "name": "Replace measurement value",
                  "shortDescription": "Replaces measurement values matching specific criteria with different values.",
                  "fullDescription": "Replaces measurement values matching specific criteria with different values.  This can be used to replace instances of NaN (not a number) values with numeric values or to replace all measurement values failing a numeric test (e.g. less than) with, for example, NaN.<br><br>Note: \"NaN\" stands for \"Not a Number\" and can arise from certain calculations (e.g. division of 0 by 0) or if a measurement couldn't be made (e.g. fitting an ellipse to an object with too few coordinates).",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects from the workspace for which specific measurement values will be replaced.  Any object measurements with values matching the specified criteria will be replaced by another specified value."
                    },
                    {
                      "name": "Measurement",
                      "description": "Measurement associated with the input objects.  Any object measurements with values matching the specified criteria will be replaced by another specified value."
                    },
                    {
                      "name": "Replacement condition",
                      "description": "Controls under what condition the input object measurement (specified by \"Measurement\") will be replaced by the specified value.  Note: \"NaN\" stands for \"Not a Number\" and can arise from certain calculations (e.g. division of 0 by 0) or if a measurement couldn't be made (e.g. fitting an ellipse to an object with too few coordinates).  Choices are: Is NaN, Is not NaN, Less than, Less than or equal to, Equal to, Greater than or equal to, Greater than, Not equal to."
                    },
                    {
                      "name": "Reference value",
                      "description": "If \"Replacement condition\" is set to a numeric condition (e.g. less than), this value will be used as the threshold against which the measurement value will be tested."
                    },
                    {
                      "name": "Replacement value type",
                      "description": "Controls what type of value any measurements identified for replacement will be replaced by:<br><ul><li>\"NaN (not a number)\" Measurement values will be replaced by NaN (not a number).<\/li><li>\"Number\" Measurement values will be replaced by the numeric value specified by \"Replacement value\".<\/li><li>\"Negative infinity\" Measurement values will be replaced by the specific \"negative infinity\" value.<\/li><li>\"Positive infinity\" Measurement values will be replaced by the specific \"positive infinity\" value.<\/li><\/ul>"
                    },
                    {
                      "name": "Replacement value",
                      "description": "Value to replace identified measurements with if \"Replacement value type\" is set to \"Number\" mode."
                    }
                  ],
                  "slug": "replace-measurement-value"
                }
              ]
            },
            {
              "path": "/modules/objects/measure/spatial",
              "name": "Spatial",
              "description": "Modules performing spatial-based measurements on objects in the workspace.",
              "slug": "spatial",
              "subCategories": [],
              "modules": [
                {
                  "path": "/modules/objects/measure/spatial/calculate-nearest-neighbour",
                  "deprecated": false,
                  "name": "Calculate nearest neighbour",
                  "shortDescription": "Measures the shortest distance between all objects in the specified input collection and all other objects in the same, or a different collection.",
                  "fullDescription": "Measures the shortest distance between all objects in the specified input collection and all other objects in the same, or a different collection.  The shortest distance (nearest neighbour distance) is recorded as a measurement associated with the input object.  Optionally, the distances between all objects can be calculated and exported as a standalone spreadsheet.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Save location",
                      "description": "Select where the image should be saved.<br><ul><li>\"Mirrored directory\" Save the image to a new directory structure which has the same layout as the input.  This is useful when batch processing from a multi-layer folder structure.  The subdirectory layout will match that of the input structure, but will have its root at the folder specified in \"Mirrored directory root\".<\/li><li>\"Match Output Control\" Save the image to the folder specified by the \"Save location\" parameter in \"Output control\".<\/li><li>\"Save with input file\" Save the image in the same file as the root file for this workspace (i.e. the image specified in \"Input control\".<\/li><li>\"Specific location\" Save the image to a specific folder.<\/li><\/ul>"
                    },
                    {
                      "name": "Mirrored directory root",
                      "description": "The root path for the mirrored directory structure.  This path is the equivalent of the folder specified in \"Input control\".  All subfolders will be in the same relative locations to their input counterparts."
                    },
                    {
                      "name": "File path",
                      "description": "Path to folder where images will be saved."
                    },
                    {
                      "name": "File name (generic)",
                      "description": ""
                    },
                    {
                      "name": "Save name mode",
                      "description": "Controls how saved image names will be generated.<br><ul><li>\"Match input file name\" Use the same name as the root file for this workspace (i.e. the input file in \"Input control\".<\/li><li>\"Specific name\" Use a specific name for the output file.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images.<\/li><\/ul>"
                    },
                    {
                      "name": "File name",
                      "description": "Filename for saved image.  Care should be taken with this when working in batch mode as it's easy to continuously write over output images."
                    },
                    {
                      "name": "Append series mode",
                      "description": "Controls if any series information should be appended to the end of the filename.  This is useful when working with multi-series files, as it should help prevent writing files from multiple runs with the same filename.  Series numbers are prepended by \"S\".  Choices are: None, Series name, Series number."
                    },
                    {
                      "name": "Append date/time mode",
                      "description": "Controls under what conditions the time and date will be appended on to the end of the image filename.  This can be used to prevent accidental over-writing of images from previous runs:<br><ul><li>\"Always\" Always append the time and date on to the end of the filename.<\/li><li>\"If file exists\" Only append the time and date if the results file already exists.<\/li><li>\"Never\" Never append time and date (unless the file is open and unwritable).<\/li><\/ul>"
                    },
                    {
                      "name": "Add filename suffix",
                      "description": "A custom suffix to be added to each filename."
                    },
                    {
                      "name": "Input objects",
                      "description": "Objects for which the distance to a closest neighbour will be calculated.  The closest distance will be stored as a measurement associated with this object."
                    },
                    {
                      "name": "Relationship mode",
                      "description": "Controls whether the nearest neighbour distance from each input object will be calculated relative to all other objects in that input collection (\"Within same object set\") or to all objects in another object collection (\"Different object set\")."
                    },
                    {
                      "name": "Neighbour objects",
                      "description": "If \"Relationship mode\" is set to \"Different object set\", the distance from the input objects to these objects will be calculated and the shortest distance recorded."
                    },
                    {
                      "name": "Reference mode",
                      "description": "Controls the method used for determining the nearest neighbour distances:<br><ul><li>\"Centroid (2D)\" Distances are between the input and neighbour object centroids, but only in the XY plane.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Centroid (3D)\" Distances are between the input and neighbour object centroids.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Surface (2D)\" Distances are between the closest points on the input and neighbour surfaces, but only in the XY plane.  These distances increase in magnitude the greater the minimum input-neighbour object surface distance is; however, they are assigned a positive value if the closest input object surface point is outside the neighbour and a negative value if the closest input object surface point is inside the neighbour.  For example, a closest input object surface point 5px outside the neighbour will be simply \"5px\", whereas a closest input object surface point 5px from the surface, but contained within the neighbour object will be recorded as \"-5px\".  Note: Any instances where the input and neighbour object surfaces overlap will be recorded as \"0px\" distance.<\/li><li>\"Surface (3D)\" Distances are between the closest points on the input and neighbour surfaces.  These distances increase in magnitude the greater the minimum input-neighbour object surface distance is; however, they are assigned a positive value if the closest input object surface point is outside the neighbour and a negative value if the closest input object surface point is inside the neighbour.  For example, a closest input object surface point 5px outside the neighbour will be simply \"5px\", whereas a closest input object surface point 5px from the surface, but contained within the neighbour object will be recorded as \"-5px\".  Note: Any instances where the input and neighbour object surfaces overlap will be recorded as \"0px\" distance.<\/li><\/ul>"
                    },
                    {
                      "name": "Only calculate for objects in same parent",
                      "description": "When selected, only distances between objects within the same parent (specified by \"Parent objects\") will be considered."
                    },
                    {
                      "name": "Parent objects",
                      "description": "When \"Only calculate for objects in same parent\" is selected, objects must have this same parent to have their nearest neighbour distances calculated."
                    },
                    {
                      "name": "Limit linking distance",
                      "description": "When selected, nearest neighbour distances will only be calculated if that distance (as calculated by the \"Reference mode\" metric) is less than or equal to the distance defined by \"Maximum linking distance\"."
                    },
                    {
                      "name": "Maximum linking distance",
                      "description": "If \"Limit linking distance\" is selected, this is the maximum permitted distance between objects for them to have their nearest neighbour distance recorded."
                    },
                    {
                      "name": "Calibrated distance",
                      "description": "When selected, linking distances are to be specified in calibrated units; otherwise, units are specified in pixels."
                    },
                    {
                      "name": "Only link objects in same frame",
                      "description": "When selected, objects must be in the same time frame for them to be linked."
                    },
                    {
                      "name": "Export all distances",
                      "description": "For each analysis run, create a separate spreadsheet file, which records the distance of all objects to all other objects."
                    },
                    {
                      "name": "Inside/outside mode",
                      "description": "When relating objects by surfaces it's possible to only consider objects inside, outside or on the edge of the neighbouring object.  This parameter controls which objects are allowed to be related to a neighbour.  Choices are: Inside and outside (all distances), Inside only (distances < 0), Inside and on surface (distances <= 0), On surface only (distances = 0), Outside and on surface (distances >= 0), Outside only (distances > 0)."
                    },
                    {
                      "name": "Include timepoints",
                      "description": "Include a column recording the timepoint that the objects were present in.  If only linking objects in the same frame, there will be a single timepoint column; however, if links are permitted between objects in different timepoints, a timepoint colummn for each of the related objects will be included."
                    },
                    {
                      "name": "Include input object parent",
                      "description": "Include a column recording the ID number of a specific parent of the input object.  For example, this could be a track ID number."
                    },
                    {
                      "name": "Input object parent",
                      "description": "Parent object collection of the input object.  If \"Include input object parent\" is selected, the corresponding parent ID number will be included as a column in the output distances spreadsheet."
                    },
                    {
                      "name": "Include neighbour object parent",
                      "description": "Include a column recording the ID number of a specific parent of the neighbour object.  For example, this could be a track ID number."
                    },
                    {
                      "name": "Neighbour object parent",
                      "description": "Parent object collection of the neighbour object.  If \"Include neighbour object parent\" is selected, the corresponding parent ID number will be included as a column in the output distances spreadsheet."
                    }
                  ],
                  "slug": "calculate-nearest-neighbour"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-centroid",
                  "deprecated": false,
                  "name": "Measure object centroid",
                  "shortDescription": "Measure mean XYZ centroid for all objects in an object colleciton.",
                  "fullDescription": "Measure mean XYZ centroid for all objects in an object colleciton.  <br>br>Note: Z-coordinates are specified in terms of slices (not pixels).",
                  "version": "1.0.0",
                  "parameters": [{
                    "name": "Input objects",
                    "description": "Objects to measure mean XYZ centroid for.  Measurements will be associated with each object."
                  }],
                  "slug": "measure-object-centroid"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-curvature",
                  "deprecated": false,
                  "name": "Measure object curvature",
                  "shortDescription": "Fits a 2D spline to the backbone of objects.",
                  "fullDescription": "Fits a 2D spline to the backbone of objects.  Each object in the input collection will be reduced to a single (longest skeleton path) backbone, which will be fit with the spline.  Local curvature of the spline can be calculated and any measurements will be assigned to the relevant object (irrespective of whether spline objects are exported).  Curvature values can be calculated as \"absolute\" (always greater than 0, irrespective of the direction of curvature), or \"signed\" (sign dependent on direction of curvature, but requires the \"start\" end of the backbone to be specified).<br><br>Note: Spline fitting will be performed in 2D, so any 3D objects will be projected into a single plane first.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects to which splines will be fit.  A single spline will be fit to each object in this collection.  Any calculated measurements will be assigned to the relevant object (irrespective of whether spline objects are exported).  Note: Spline fitting will be performed in 2D, so any 3D objects will be projected into a single plane first."
                    },
                    {
                      "name": "Object output mode",
                      "description": "Controls whether spline objects are exported:<br><ul><li>\"Do not store\" No spline objects are output by this module.<\/li><li>\"Individual control points\" Specific points along the contour are exported.  Each control point is itself a separate object, which is a child of the corresponding input object.  As such, each input object will generally have multiple child control point objects.<\/li><li>\"Full contour\" All points along the spline are exported as a single object.  This object is stored as a child of the corresponding input object.<\/li><\/ul>"
                    },
                    {
                      "name": "Output objects",
                      "description": "The name assigned to the objects if they are being exported."
                    },
                    {
                      "name": "Export every N points",
                      "description": "If spline objects are being exported (either as full contours or as individual control points), this value controls the interval between points.  As such, increasing values will export fewer and fewer points."
                    },
                    {
                      "name": "Spline fitting method",
                      "description": "Controls how the spline is fit to the input object:<br><ul><li>\"LOESS (smooth fitting)\" Performs a local regression (LOESS) interpolation of the line to give a smoothed representation of the object backbone (longest skeleton path).  Uses <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.3/org/apache/commons/math3/analysis/interpolation/LoessInterpolator.html\">Apache Math3 LoessInterpolator<\/a>.<\/li><li>\"Standard (fits all points)\" Fit spline is formed of straight line segments between every other point along the input object backbone (longest skeleton path).  This method doesn't perform any smoothing.  Uses <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.3/org/apache/commons/math3/analysis/interpolation/SplineInterpolator.html\">Apache Math3 SplineInterpolator<\/a>.<\/li><\/ul>"
                    },
                    {
                      "name": "Number of neighbours (smoothing)",
                      "description": "Number of neighbouring points used in the calculation of each spline control point.  The greater the number of neighbours, the smoother the output spline."
                    },
                    {
                      "name": "Iterations",
                      "description": "\"This many robustness iterations are done.  A sensible value is usually 0 (just the initial fit without any robustness iterations) to 4\".  Description taken from <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.3/org/apache/commons/math3/analysis/interpolation/LoessInterpolator.html\">LoessInterpolator documentation<\/a>"
                    },
                    {
                      "name": "Accuracy",
                      "description": "\"If the median residual at a certain robustness iteration is less than this amount, no more iterations are done\".  Description taken from <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.3/org/apache/commons/math3/analysis/interpolation/LoessInterpolator.html\">LoessInterpolator documentation<\/a>"
                    },
                    {
                      "name": "Relate to reference point",
                      "description": "When selected, the fit spline will be oriented such that the first point is closer to the reference point (specified by measurements \"X-axis reference measurement\" and \"Y-axis reference measurement\") than the final point in the spline.  Having this consistency to the spline orientation allows measurements relative to the first point to be calculated (e.g. relative location of maximum curvature along the spline).  When this is not selected, there's no guarantee of which end of the spline will be \"first\"."
                    },
                    {
                      "name": "X-axis reference measurement",
                      "description": "If \"Relate to reference point\" is selected, this is the measurement associated with the input object that will provide the x-axis reference for determining the orientation of the spline."
                    },
                    {
                      "name": "Y-axis reference measurement",
                      "description": "If \"Relate to reference point\" is selected, this is the measurement associated with the input object that will provide the y-axis reference for determining the orientation of the spline."
                    },
                    {
                      "name": "Measure absolute curvature",
                      "description": "When selected (and when \"Relate to reference point\" is also selected), absolute curvature values will be calculated.  Absolute curvatures are always greater than or equal to 0, irrespective of direction.  Increasing signed curvature values indicate increasing curvatures."
                    },
                    {
                      "name": "Measure signed curvature",
                      "description": "When selected (and when \"Relate to reference point\" is also selected), signed curvature values will be calculated.  Signed curvatures are increasingly positive as the spline bends left and increasingly negative as the spline bends right (directions relative to path along spline, starting at first point)."
                    },
                    {
                      "name": "Calculate angle between ends",
                      "description": "When selected, the angle between the two ends of the spline are calculated in degree units."
                    },
                    {
                      "name": "Fitting range (px)",
                      "description": "If the angle between spline ends is being calculated, this is the number of points at each end of the spline that are fit to get the orientation at that end."
                    },
                    {
                      "name": "Draw spline",
                      "description": "When selected, the fit spline(s) will be rendered as an overlay on the image specified by \"Input image\"."
                    },
                    {
                      "name": "Input image",
                      "description": "If drawing the spline(s), this is the image onto which they will be added as overlays."
                    },
                    {
                      "name": "Apply to image",
                      "description": "If drawing the spline(s), when this is selected, the spline overlays will be added to the image specified by \"Input image\".  If not selected, the image containing the overlays will be stored separately in the workspace with the name specified by \"Output image\"."
                    },
                    {
                      "name": "Output image",
                      "description": "If drawing the spline(s) and \"Apply to image\" is not selected, this is the name with which the overlay images will be stored in the workspace."
                    },
                    {
                      "name": "Line width",
                      "description": "If drawing the spline(s), this is the width of the spline overlay lines that will be drawn."
                    },
                    {
                      "name": "Maximum curvature (for colour)",
                      "description": "If drawing the spline(s), the local colour of each spline will represent the local absolute curvature.  This value controls the maximum absolute curvature that will correspond to the top-end of the curvature colourmap.  Values above this will see the colourmap cycling."
                    }
                  ],
                  "slug": "measure-object-curvature"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-limits",
                  "deprecated": false,
                  "name": "Measure object limits",
                  "shortDescription": "Measures the XYZ spatial limits of each object relative to the origin (x = 0, y = 0, z = 0) of the original image.",
                  "fullDescription": "Measures the XYZ spatial limits of each object relative to the origin (x = 0, y = 0, z = 0) of the original image.  Limits are stored as measurements associated with each object.  Measurements are reported in pixel (slice for z) coordinates and calibrated units.",
                  "version": "1.0.0",
                  "parameters": [{
                    "name": "Input objects",
                    "description": "Objects to measure spatial limits for.  Measurements will be associated with each object."
                  }],
                  "slug": "measure-object-limits"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-overlap",
                  "deprecated": false,
                  "name": "Measure object overlap",
                  "shortDescription": "Calculates the overlap of each object in an object collection with any object from another collection.",
                  "fullDescription": "Calculates the overlap of each object in an object collection with any object from another collection.  Overlaps are calculated for both specified object collections and are stored as measurements associated with the relevant object.  Overlap can occur for multiple objects; however, doubly-overlapped regions will only be counted once (i.e. an object can have no more than 100% overlap).  For example, an object in the first collection with 20% overlap with one object and 12% overlap with another would receive an overlap measurement of 32% (assuming the two overlapping objects weren't themselves overlapped in the overlapping region).",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Object source mode",
                      "description": "Controls whether overlap of objects from the same class should be calculated, or whether objects from two different classes should be compared."
                    },
                    {
                      "name": "Object set 1",
                      "description": "Object collection for which, the overlap of each object with any object from a separate object collection (specified by the \"Object set 2\" parameter) will be calculated."
                    },
                    {
                      "name": "Object set 2",
                      "description": "Object collection for which, the overlap of each object with any object from a separate object collection (specified by the \"Object set 1\" parameter) will be calculated."
                    },
                    {
                      "name": "Only link objects in same frame",
                      "description": "When selected, objects will only be considered to have any overlap if they're present in the same frame (timepoint)."
                    },
                    {
                      "name": "Enable multithreading",
                      "description": "Process multiple input objects simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                    }
                  ],
                  "slug": "measure-object-overlap"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-shape",
                  "deprecated": false,
                  "name": "Measure object shape",
                  "shortDescription": "Measures various spatial metrics for each object in a specified object collection from the workspace.",
                  "fullDescription": "Measures various spatial metrics for each object in a specified object collection from the workspace.  Measurements are associated with the relevant input object.  When dealing with 3D objects (those with coordinates spanning multiple Z-slices) a 2D projection into the XY plane will be used.  3D metrics are calculated using the \"<a href=\"https://github.com/ijpb/MorphoLibJ\">MorphoLibJ<\/a>\" plugin.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Input objects from workspace.  Shape metrics will be calculated for each object and stored as measurements associated with that object."
                    },
                    {
                      "name": "Measure volume",
                      "description": "When selected, 3D volume metrics will be calculated for each input object.  Metrics are: volume (px), volume (calibrated_units), number of voxels, base area (px), base area (calibrated_units), height (n_slices) and height (calibrated_units)."
                    },
                    {
                      "name": "Measure projected area",
                      "description": "When selected, 2D area metrics will be calculated for each input object.  For 3D objects, a 2D projection in the XY plane is used for measurements.  This projection includes all XY coordinates present in any Z-slice.  Metrics are: area (px) and area (calibrated_units)."
                    },
                    {
                      "name": "Measure projected diameter",
                      "description": "When selected, the diameter of 2D objects will be calculated for each input object.  For 3D objects, a 2D projection in the XY plane is used for measurements.  This projection includes all XY coordinates present in any Z-slice.  Metrics are: diameter (px) and diameter (calibrated_units)."
                    },
                    {
                      "name": "Measure projected perimeter",
                      "description": "When selected, the perimeter and circularity of 2D objects will be calculated for each input object.  For 3D objects, a 2D projection in the XY plane is used for measurements.  This projection includes all XY coordinates present in any Z-slice.  Metrics are: perimeter (px), perimeter (calibrated_units) and circularity."
                    },
                    {
                      "name": "Measure 3D metrics",
                      "description": ""
                    },
                    {
                      "name": "Connectivity",
                      "description": ""
                    },
                    {
                      "name": "Surface area method",
                      "description": ""
                    },
                    {
                      "name": "Enable multithreading",
                      "description": "Process multiple input objects simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                    }
                  ],
                  "slug": "measure-object-shape"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-object-width",
                  "deprecated": false,
                  "name": "Measure object width",
                  "shortDescription": "",
                  "fullDescription": "",
                  "version": "1.0.0",
                  "parameters": [{
                    "name": "Input objects",
                    "description": ""
                  }],
                  "slug": "measure-object-width"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-relative-orientation",
                  "deprecated": false,
                  "name": "Measure relative orientation",
                  "shortDescription": "Measures the orientation of each object relative to a specific point.",
                  "fullDescription": "Measures the orientation of each object relative to a specific point.  Orientation of the objects themselves is provided by a previously-calculated measurement associated with each object.  The relative orientation is the angle between the vector specified by this measurement and a vector passing from the object centroid to a specific point.<br><br>Note: Currently only works for X-Y plane measurements",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects from the workspace for which orientation relative to a point will be measured.  Orientation of the objects themselves is provided by a previously-calculated measurement associated with each object.  The relative orientation is the angle between the vector specified by this measurement and a vector passing from the object centroid to a specific point.  Output relative orientation measurements are associated with the relevant input object."
                    },
                    {
                      "name": "Orientation in X/Y measurement",
                      "description": "Measurement associated with each input object which defines the orientation of the first vector for the relative orientation calculation.  The other vector passes between the object centroid and a specific point (controlled by the other parameters).  This orientation measurement is assumed to be specified in degree units."
                    },
                    {
                      "name": "Measurement range",
                      "description": "The range over which output relative orientation measurements are specified.  For cases where the orientation of the object could be inverted without consequence (e.g. fitting orientation to an ellipse), the range \"0-90 degrees\" can be used; however, in cases where the orientation is more specific (e.g. having come from object tracking), the range \"0-180 degrees\" can be used.  Relative orientation measurements are absolute, in that there's no distinction between positive and negative angular differences."
                    },
                    {
                      "name": "Reference mode",
                      "description": "Controls the source of the reference point to which object orientations are compared.  :<br><ul><li>\"Object centroid to image centre\" Object orientations are compared to the centre of the object collection (i.e. to the image from which they were first detected).<\/li><li>\"Object centroid to target centroid\" Object orientations are compared to the centre of a specific object.  Since only a single object can be used per timepoint, this will be either the smallest or largest object (controlled by \"Object choice mode\") in the specified collection (\"Reference objects\").  If \"Reference must be in same frame\" is not selected, the same object will be used in all timepoints.<\/li><li>\"Object centroid to target surface\" Object orientations are compared to the closest point on the surface of a specific object.  In this mode, the reference points can be different for all objects.  Since only a single object can be used per timepoint, this will be either the smallest or largest object (controlled by \"Object choice mode\") in the specified collection (\"Reference objects\").  If \"Reference must be in same frame\" is not selected, the same object will be used in all timepoints.<\/li><\/ul>"
                    },
                    {
                      "name": "Reference objects",
                      "description": "If \"Reference mode\" is set to \"Object centroid to target centroid\" or \"Object centroid to target surface\", this is the object collection from which the reference object(s) will be drawn."
                    },
                    {
                      "name": "Object choice mode",
                      "description": "If \"Reference mode\" is set to \"Object centroid to target centroid\" or \"Object centroid to target surface\", this controls whether the smallest or largest object from that collection will be used as the reference point for relative orientation calculations."
                    },
                    {
                      "name": "Reference must be in same frame",
                      "description": "When selected, the reference objects must be in the same frame as the input object being measured.  As such, all the objects in one frame will be compared to a common reference, but won't necessarily have the same reference as an object in another frame."
                    }
                  ],
                  "slug": "measure-relative-orientation"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-specific-width",
                  "deprecated": false,
                  "name": "Measure specific width",
                  "shortDescription": "Measures the width of the object along the axis passing through two specified reference points.",
                  "fullDescription": "Measures the width of the object along the axis passing through two specified reference points.  Reference points can be the object centroid, an image measurement or a measurement associated with the object.  Widths are determined as the distance between the two points along the reference line with the greatest separation (i.e. it doesn't matter if there are gaps between them).",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input objects",
                      "description": "Objects for which specific widths will be calculated.  Width measurements will be associated with these objects."
                    },
                    {
                      "name": "Reference mode 1",
                      "description": "The source of the first reference point coordinates:<br><ul><li>\"Object centroid\" Reference point will be positioned coincident with the centroid of the object being measured.<\/li><li>\"Image measurement\" X, Y and Z location of the reference point will be equal to specified measurement values associated with the image (set by \"Reference image 1\").<\/li><li>\"Object measurement\" X, Y and Z location of the reference point will be equal to the specified measurememnt values associated with the object being measured.<\/li><\/ul>"
                    },
                    {
                      "name": "Reference image 1",
                      "description": "If \"Reference mode 1\" is set to \"Image measurement\", this is the image that will provide the measurements determining the X, Y and Z location of the first reference point."
                    },
                    {
                      "name": "X-pos. image meas. 1 (px)",
                      "description": "If \"Reference mode 1\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 1\", that will be used as the X-axis location of the first reference point."
                    },
                    {
                      "name": "Y-pos. image meas. 1 (px)",
                      "description": "If \"Reference mode 1\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 1\", that will be used as the Y-axis location of the first reference point."
                    },
                    {
                      "name": "Z-pos. image meas. 1 (slice)",
                      "description": "If \"Reference mode 1\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 1\", that will be used as the Z-axis location of the first reference point."
                    },
                    {
                      "name": "X-pos. object meas. 1 (px)",
                      "description": "If \"Reference mode 1\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the X-axis location of the first reference point."
                    },
                    {
                      "name": "Y-pos. object meas. 1 (px)",
                      "description": "If \"Reference mode 1\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the Y-axis location of the first reference point."
                    },
                    {
                      "name": "Z-pos. object meas. 1 (slice)",
                      "description": "If \"Reference mode 1\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the Z-axis location of the first reference point."
                    },
                    {
                      "name": "Reference mode 2",
                      "description": "The source of the second reference point coordinates:<br><ul><li>\"Object centroid\" Reference point will be positioned coincident with the centroid of the object being measured.<\/li><li>\"Image measurement\" X, Y and Z location of the reference point will be equal to specified measurement values associated with the image (set by \"Reference image 1\").<\/li><li>\"Object measurement\" X, Y and Z location of the reference point will be equal to the specified measurememnt values associated with the object being measured.<\/li><\/ul>"
                    },
                    {
                      "name": "Reference image 2",
                      "description": "If \"Reference mode 2\" is set to \"Image measurement\", this is the image that will provide the measurements determining the X, Y and Z location of the second reference point."
                    },
                    {
                      "name": "X-pos. image meas. 2 (px)",
                      "description": "If \"Reference mode 2\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 2\", that will be used as the X-axis location of the second reference point."
                    },
                    {
                      "name": "Y-pos. image meas. 2 (px)",
                      "description": "If \"Reference mode 2\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 2\", that will be used as the Y-axis location of the second reference point."
                    },
                    {
                      "name": "Z-pos. image meas. 2 (slice)",
                      "description": "If \"Reference mode 2\" is set to \"Image measurement\", this is the measurement, associated with the image specified by \"Reference image 2\", that will be used as the Z-axis location of the second reference point."
                    },
                    {
                      "name": "X-pos. object meas. 2 (px)",
                      "description": "If \"Reference mode 2\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the X-axis location of the second reference point."
                    },
                    {
                      "name": "Y-pos. object meas. 2 (px)",
                      "description": "If \"Reference mode 2\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the Y-axis location of the second reference point."
                    },
                    {
                      "name": "Z-pos. object meas. 2 (slice)",
                      "description": "If \"Reference mode 2\" is set to \"Object measurement\", this is the measurement, associated with the current object, that will be used as the Z-axis location of the second reference point."
                    },
                    {
                      "name": "Measurement prefix",
                      "description": "The output measurement names will be prefixed with this value (although it can be left blank).  Using a prefix for these measurements allows multiple different widths to be output by multiple copies of this module."
                    }
                  ],
                  "slug": "measure-specific-width"
                },
                {
                  "path": "/modules/objects/measure/spatial/measure-track-motion",
                  "deprecated": false,
                  "name": "Measure track motion",
                  "shortDescription": "Measures various motion metrics for tracked objects.",
                  "fullDescription": "Measures various motion metrics for tracked objects.  Global motion statistics (e.g. total path length) are stored as measurements associated with the input track objects, whilst instantaneous motion statistics (e.g. instantaneous x-velocity) are associated with the input spot objects.",
                  "version": "1.0.0",
                  "parameters": [
                    {
                      "name": "Input track objects",
                      "description": "Input track objects to measure motion for.  These must be specific \"track\" class objects as output by modules such as \"Track objects\".  The track objects are parents of individual timepoint instance objects, which are specified using the \"Input spot objects\" parameter.  Global track measurements (e.g. total path length) are associated with the corresponding track objects."
                    },
                    {
                      "name": "Input spot objects",
                      "description": "Input individual timepoint instance objects for the track.  These are the spatial records of the tracked objects in a single timepoint and are children of the track object specified by \"Input track objects\".  Instantaneous track measurements (e.g. instantaneous x-velociyty) are associated with the corresponding spot objects."
                    },
                    {
                      "name": "Subtract average motion",
                      "description": "When selected, the average motion of all points between two frames is subtracted from the motion prior to calculation of any track measurements.  This can be used as a crude form of drift correction; however, it only works for global drift (where the whole sample moved together) and is less robust with few tracked objects.  Ideally, drift would be removed from the input images using image registration prior to object detection."
                    },
                    {
                      "name": "Identify leading point",
                      "description": "When selected, the \"leading point\" of each object in the track will be determined and stored as X,Y,Z coordinate measurements associated with the relevant object.  The orientation of the track at that location will also be calculated.  In this instance, \"leading point\" refers to the front-most point in the object when considering the direction the object is moving in that frame.  This can be calculated relative to the previous frame, next frame or both previous and next frames (controlled by the \"Orientation mode\" parameter)."
                    },
                    {
                      "name": "Orientation mode",
                      "description": "If calculating the leading point of each object in each track (\"Identify leading point\" selected), this controls whether the instantaneous orientation of the track is determined relative to the previous frame, next frame or both previous and next frames."
                    }
                  ],
                  "slug": "measure-track-motion"
                }
              ]
            }
          ],
          "modules": []
        },
        {
          "path": "/modules/objects/process",
          "name": "Process",
          "description": "Operations capable of creating new objects from existing ones.  For example, fitting ellipsoids or creating projections along an axis.",
          "slug": "process",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/objects/process/create-skeleton",
              "deprecated": false,
              "name": "Create skeleton",
              "shortDescription": "Creates and measures the skeletonised form of specified input objects.",
              "fullDescription": "Creates and measures the skeletonised form of specified input objects.  This module uses the <a href=\"https://imagej.net/AnalyzeSkeleton\">AnalyzeSkeleton<\/a> plugin by Ignacio Arganda-Carreras.<br><br>The optional, output skeleton object acts solely as a linking object for the edge, junction and loop objects.  It doesn't itself hold any coordinate data.",
              "version": "1.1.0",
              "parameters": [
                {
                  "name": "Input mode",
                  "description": ""
                },
                {
                  "name": "Input image",
                  "description": "Input image from the workspace to be skeletonised."
                },
                {
                  "name": "Binary logic",
                  "description": ""
                },
                {
                  "name": "Input objects",
                  "description": "Input objects from the workspace to be skeletonised.  These can be either 2D or 3D objects.  Skeleton measurements will be added to this object."
                },
                {
                  "name": "Add skeletons to workspace",
                  "description": "When selected, the coordinates for the various skeleton components (edges, junctions and loops) will be stored as new objects.  These objects will all be children of a parent \"Skeleton\" object, which itself will be a child of the corresponding input object."
                },
                {
                  "name": "Output skeleton objects",
                  "description": "If \"Add skeletons to workspace\" is selected, a single \"Skeleton\" object will be created per input object.  This skeleton object will act as a linking object (parent) for the edges, junctions and loops that comprise that skeleton.  As such, the skeleton object itself doesn't store any coordinate information."
                },
                {
                  "name": "Output edge objects",
                  "description": "If \"Add skeletons to workspace\" is selected, the edges of each skeleton will be stored in these objects.  An \"Edge\" is comprised of a continuous run of points each with one (end points) or two neighbours.  These edge objects are children of a \"Skeleton\" object (specified by the \"Output skeleton objects\" parameter), which itself is the child of the corresponding input object.  Each edge object has a partner relationship with its adjacent \"Junction\" and (optionally) \"Loop\" objects (specified by the \"Output junction objects\" and \"Output loop objects\" parameters, respectively)."
                },
                {
                  "name": "Output junction objects",
                  "description": "If \"Add skeletons to workspace\" is selected, the junctions of each skeleton will be stored in these objects.  A \"Junction\" is comprised of a contiguous regions of points each with three or neighbours.  These junction objects are children of a \"Skeleton\" object (specified by the \"Output skeleton objects\" parameter), which itself is the child of the corresponding input object.  Each junction object has a partner relationship with its adjacent \"Edge\" and (optionally) \"Loop\" objects (specified by the \"Output edge objects\" and \"Output loop objects\" parameters, respectively)."
                },
                {
                  "name": "Export loop objects",
                  "description": "When selected (and if \"Add skeletons to workspace\" is also selected), the loops of each skeleton will be stored in the workspace as new objects.  The name for the output loop objects is determined by the \"Output loop objects\" parameter."
                },
                {
                  "name": "Output loop objects",
                  "description": "If both \"Add skeletons to workspace\" and \"Export loop objects\" are selected, the loops of each skeleton will be stored in these objects.  A \"Loop\" is comprised of a continuous region of points bounded on all sides by either \"Edge\" or \"Junction\" points.  These loop objects are children of a \"Skeleton\" object (specified by the \"Output skeleton objects\" parameter), which itself is the child of the corresponding input object.  Each loop object has a partner relationship with its adjacent \"Edge\" and \"Junction\" objects (specified by the \"Output edge objects\" and \"Output junction objects\" parameters, respectively)."
                },
                {
                  "name": "Export largest shortest path",
                  "description": "When selected, the largest shortest path between any two points in the skeleton will be stored in the workspace as a new object.  For each input object, the shortest path between all point pairs within the skeleton is calculated and the largest of all these paths stored as a new object.  The name for the output largest shortest path object associated with each input object is determined by the \"Output largest shortest path\" parameter.  <a href=\"https://imagej.net/plugins/analyze-skeleton/\">Analyse Skeleton<\/a> calculates the largest shortest path using <a href=\"https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm\">Floyd-Warshall algorithm<\/a>.  Note: These objects are not the same as the <a href=\"https://en.wikipedia.org/wiki/Longest_path_problem\">longest possible path<\/a>."
                },
                {
                  "name": "Output largest shortest path",
                  "description": "If \"Export largest shortest path\"is selected, the largest shortest path for each skeleton will be stored in the workspace.  For each skeleton, the shortest path between all point pairs is calculated; the largest shortest path is the longest of all these paths.  The largest shortest path objects are children of the corresponding input object."
                },
                {
                  "name": "Minimum branch length",
                  "description": "The minimum length of a branch (edge terminating in point with just one neighbour) for it to be included in skeleton measurements and (optionally) exported as an object."
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Break the image down into strips, each one processed on a separate CPU thread.  The overhead required to do this means it's best for large multi-core CPUs, but should be left disabled for small images or on CPUs with few cores."
                }
              ],
              "slug": "create-skeleton"
            },
            {
              "path": "/modules/objects/process/extract-object-cross-section",
              "deprecated": false,
              "name": "Extract object cross section",
              "shortDescription": "Extracts XY-plane cross-sections of specified objects.",
              "fullDescription": "Extracts XY-plane cross-sections of specified objects.  The extracted cross-sections are stored as separate objects, which are children of the associated input object.  Slice indicies can be specified as fixed values or relative to image/object measurements (e.g. relative to the object centroids).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input objects from workspace for which cross-sections will be extracted.  Output cross-section objects will be stored as children associated with the relevant input object."
                },
                {
                  "name": "Output objects",
                  "description": "Output cross-section objects.  These will be stored in the workspace with this name.  These objects will be children of their respective input object."
                },
                {
                  "name": "Reference mode",
                  "description": "The source for the reference Z-position for each object:<br><ul><li>\"Absolute\" The slice indices specified by \"Relative slice indices\" correspond to the absolute slice index of the coordinates.  For example, an index of 0 will extract the first slice and indices of \"3-5\" will load the 4th, 5th and 6th slices (note the use of zero-based indexing).<\/li><li>\"Image measurement\" The reference slice index will be taken from a measurement (specified by \"Image measurement\") associated with an image from the workspace (specified by \"Image for measurement\").  The slices indices specified by \"Relative slice indices\" will be relative to this measurement value.  For example, with an image measurement of 4 and specified index of \"-2\", the 3rd slice will be extracted (i.e. 2 below 4 using zero-based indexing).<\/li><li>\"Object measurement\" The reference slice index will be taken from a measurement (specified by \"Object measurement\") associated with the object being processed.  The slices indices specified by \"Relative slice indices\" will be relative to this measurement value.  For example, for an object with measurement value of 2 and specified index of \"3\", the 6th slice will be extracted (i.e. 3 above 2 using zero-based indexing).<\/li><\/ul>"
                },
                {
                  "name": "Image for measurement",
                  "description": "If \"Reference mode\" is set to \"Image measurement\", this is the image from which the reference measurement (specified by \"Image measurement\") will be taken."
                },
                {
                  "name": "Image measurement",
                  "description": "If \"Reference mode\" is set to \"Image measurement\", this is the measurement (associated with the image specified by \"Image for measurement\") which will act as the reference slice index against which the relative slice indices are calculated."
                },
                {
                  "name": "Object measurement",
                  "description": "If \"Reference mode\" is set to \"Object measurement\", this is the measurement (associated with the relevant input object) which will act as the reference slice index against which the relative slice indices are calculated."
                },
                {
                  "name": "Relative slice indices",
                  "description": "Slices from the input objects will be extracted at these relative indices (relative to the position specified the \"Reference mode\" and associated parameters).  Indices can be specified as a comma-separated list, using a range (e.g. \"4-7\" will extract relative indices 4,5,6 and 7) or as a range extracting every nth slice (e.g. \"4-10-2\" will extract slices 4,6,8 and 10).  The \"end\" keyword will be converted to the maximum slice index at runtime."
                }
              ],
              "slug": "extract-object-cross-section"
            },
            {
              "path": "/modules/objects/process/extract-object-edges",
              "deprecated": false,
              "name": "Extract object edges",
              "shortDescription": "Extracts edge and interior objects for each object in a specified set.",
              "fullDescription": "Extracts edge and interior objects for each object in a specified set.  The boundary defining the transition between edges and interiors can be specified either at a fixed distance from the object edge or as a percentage of the maximum edge-centroid distance for each object.  Output edge and interior objects are stored as children of the associated input object.    ",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from workspace for which edge and/or interior regions will be extracted.  Any extracted regions will be children of the associated input objects."
                },
                {
                  "name": "Create edge objects",
                  "description": "When selected, an edge object will be created for each input object.  Edge objects contain only coordinates from the input object within a given distance of the object edge.  Output edge objects will be children associated with the relevant input object.  The edge objects will be stored in the workspace with the name specified by \"Output edge objects\"."
                },
                {
                  "name": "Output edge objects",
                  "description": "If \"Create edge objects\" is selected, this is the name assigned to output edge objects."
                },
                {
                  "name": "Create interior objects",
                  "description": "When selected, an interior object will be created for each input object.  Interior objects contain only coordinates from the input object a given distance from the object edge or greater (i.e. they contain any points from the input object which aren't assigned as \"edge\" objects).  Output interior objects will be children associated with the relevant input object.  The interior objects will be stored in the workspace with the name specified by \"Output interior objects\"."
                },
                {
                  "name": "Output interior objects",
                  "description": "If \"Create interior objects\" is selected, this is the name assigned to output interior objects."
                },
                {
                  "name": "Edge determination",
                  "description": "Controls how the boundary between \"edge\" and \"interior\" objects is defined:<br><ul><li>\"Distance to edge\" The boundary is defined by a fixed distance value specified by \"Distance\".  Any input object coordinates within (less than or equal to) this distance of the object edge can be output as \"edge\" coordinates, otherwise they can be output as \"interior\" coordinates.<\/li><li>\"Object measurement\" The boundary is defined by a measurement value associated with each object.  The measurement is specified by \"Measurement name\".  Any input object coordinates within (less than or equal to) this distance of the object edge can be output as \"edge\" coordinates, otherwise they can be output as \"interior\" coordinates.<\/li><li>\"Parent object measurement\" The boundary is defined by a measurement value associated with a parent of each object.  The measurement is specified by \"Parent measurement name\".  Any input object coordinates within (less than or equal to) this distance of the object edge can be output as \"edge\" coordinates, otherwise they can be output as \"interior\" coordinates.<\/li><li>\"Percentage of maximum distance to edge\" The boundary is defined as a percentage of the maximum distance from the edge of the object to its centroid.  As such, this boundary value will vary from object to object in terms of the absolute width of edge objects.<\/li><\/ul>"
                },
                {
                  "name": "Distance",
                  "description": "If \"Edge determination\" is set to \"Distance to edge\", this is the fixed distance value that defines the boundary between edge and interior objects.  It is assumed to be specified in pixel units unless \"Calibrated distances\" is selected, in which case they are assumed in calibrated units."
                },
                {
                  "name": "Measurement name",
                  "description": "If \"Edge determination\" is set to \"Object measurement\", this is the object measurement (associated with the object being processed) that defines the boundary between edge and interior objects.  It is assumed to be specified in pixel units unless \"Calibrated distances\" is selected, in which case they are assumed in calibrated units."
                },
                {
                  "name": "Parent objects",
                  "description": "If \"Edge determination\" is set to \"Parent object measurement\", this is the parent of the input object that will be used as a source for the edge width measurement specified by \"Parent object measurement\"."
                },
                {
                  "name": "Parent measurement name",
                  "description": "If \"Edge determination\" is set to \"Parent object measurement\", this is the object measurement (associated with a parent of the object being processed) that defines the boundary between edge and interior objects.  It is assumed to be specified in pixel units unless \"Calibrated distances\" is selected, in which case they are assumed in calibrated units."
                },
                {
                  "name": "Calibrated distances",
                  "description": "When selected, the fixed boundary distance specified by \"Distance\" is assumed to be in calibrated units.  Otherwise, the fixed distance is in pixel units."
                },
                {
                  "name": "Percentage",
                  "description": "If \"Edge determination\" is set to \"Percentage of maximum distance to edge\", this is the percentage of the maximum centroid-edge distance for an object that will be used to calculate the edge/interior boundary location.  Percentages approaching 0% will put the boundary increasingly close to the object edge (more detected as \"interiors\"), while percentages approaching 100% will have the boundary increasingly close to the object centroid (more detected as \"edges\")."
                }
              ],
              "slug": "extract-object-edges"
            },
            {
              "path": "/modules/objects/process/fit-gaussian-2d",
              "deprecated": false,
              "name": "Fit Gaussian 2D",
              "shortDescription": "Gaussian spot fitting.",
              "fullDescription": "Gaussian spot fitting.  Can take objects as estimated locations.\n***Only works in 2D***\n***Only works for refinement of existing spots***",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace to which the 2D Gaussian profiles will be fit."
                },
                {
                  "name": "Input objects",
                  "description": "Objects for which 2D Gaussians will be fit.  A single Gaussian will be calculated for each object, with the centroid of the input object used as the starting estimate for the Gaussian centroid.  Fit parameters will be stored as measurements associated with the relevant input objects."
                },
                {
                  "name": "Method to estimate sigma",
                  "description": "Controls how the initial estimate of Gaussian sigma will be calculated.  Note: The initial radius value can also influence the range of accepted sigma values (see \"Limit sigma range\" parameter).<br><ul><li>\"Fixed value\" The same initial sigma value is applied to the fitting of all objects.  This value is provided by the \"Sigma\" parameter.<\/li><li>\"Measurement\" A different sigma value is applied to the fitting of each object.  This value is provided by the measurement (specified by \"Sigma measurement\") associated with the object being fit.<\/li><\/ul>"
                },
                {
                  "name": "Sigma",
                  "description": "If \"Method to estimate sigma\" is set to \"Fixed value\", this is the initial sigma value that will be used in the fitting of all objects."
                },
                {
                  "name": "Sigma measurement",
                  "description": "If \"Method to estimate sigma\" is set to \"Measurement\", this is the measurement that will provide the intial sigma value used in the fitting of each object."
                },
                {
                  "name": "Measurement multiplier",
                  "description": "If using object-associated measurements as the intial estimate of sigma, these measurements can be systematically increased or decreased with this multiplier.  Values less than 1 will reduce the sigma estimate, while values greater than 1 will increase it.  To use the default sigma value, set this to 1."
                },
                {
                  "name": "Limit sigma range",
                  "description": "When selected, the final sigma value must lie within a specific range.  This range is controlled by the \"Minimum sigma (multiplier)\" and \"Maximum sigma (multiplier)\" parameters."
                },
                {
                  "name": "Minimum sigma (multiplier)",
                  "description": "If limiting the final sigma range (\"Limit sigma range\" parameter selected), this value controls the minimum allowed sigma value.  The minimum range is specified as a multiplier of the input sigma estimate.  For example, an estimate sigma value of 4 with a \"Minimum sigma (multiplier)\" multiplier of 0.5 forces the predicted sigma to have a value of at least 2.  Variable sigma ranges allow measurement-based sigma estimates to have appropriate limits."
                },
                {
                  "name": "Maximum sigma (multiplier)",
                  "description": "If limiting the final sigma range (\"Limit sigma range\" parameter selected), this value controls the maximum allowed sigma value.  The maximum range is specified as a multiplier of the input sigma estimate.  For example, an estimate sigma value of 4 with a \"Maximum sigma (multiplier)\" multiplier of 3 forces the predicted sigma to have a value of at most 12.  Variable sigma ranges allow measurement-based sigma estimates to have appropriate limits."
                },
                {
                  "name": "Fixed fitting window",
                  "description": "When selected, the size of the cropped image region, to which the 2D Gaussian is fit, is controlled by the \"Window size\" parameter.  Otherwise, the window size is automatically selected as 4 times the initial estimate of sigma, plus 1."
                },
                {
                  "name": "Window size",
                  "description": "If using a fixed fitting window (\"Fixed fitting window\" parameter selected), this is the size of the image crop (centered on the object being fit) that will be used."
                },
                {
                  "name": "Maximum number of evaluations",
                  "description": "The maximum number of iterations the fitting algorithm can use."
                },
                {
                  "name": "Remove objects with failed fitting",
                  "description": "If selected, any objects for which fits can't be determined (fitter runs out of evaluations before completing) will be removed from the input object set."
                },
                {
                  "name": "Apply volume",
                  "description": "If selected, the coordinates of the input objects will be updated to be circles.  The output circles will be centred on the best-fit location and have radii equal to the x-axis sigma value."
                },
                {
                  "name": "Create Gaussian image",
                  "description": "If selected, an image containing all calculated Gaussian fits will be added to the workspace."
                },
                {
                  "name": "Gaussian image name",
                  "description": "If \"Create Gaussian image\" is selected, this is the name of the output Gaussian profile image that will be added to the workspace."
                }
              ],
              "slug": "fit-gaussian-2d"
            },
            {
              "path": "/modules/objects/process/fit-active-contours",
              "deprecated": false,
              "name": "Fit active contours",
              "shortDescription": "Uses active contours to fit a 2D concave hull to specified objects.",
              "fullDescription": "Uses active contours to fit a 2D concave hull to specified objects.  The 2D perimeter of each input object is converted to a closed contour, the position of which is optimised in order to minimise various internal (contour) and external (image) energies.  Internal energies are elasticity and bending of the contour, which aim to minimise point-point separation and adjacent segment alignment, respectively.  External energies are provided by the image intensity along the path and are minimised when the contour sits in dark areas of the image.  Energies are iteratively optimised using a greedy algorithm which tests each point along the contour at all points within a specified search radius, taking the lowest energy point as the new location.  For more information on active contours, see Kass, M.; Witkin, A.; Terzopoulos, D., \"Snakes: Active contour models\",  <i>International Journal of Computer Vision<\/i>, 1988, <b>1<\/b>, 321.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image from the workspace to which the contours will be fit.  The intensity of this image will contribute to the external forces applied to the contour.  For example, the contour will attempt to minimise the intensity along the path of the contour."
                },
                {
                  "name": "Input objects",
                  "description": "Objects from the workspace to which active contours will be fit.  Active contours are fit in 2D to the object points from the first slice.  As such, input objects can be stored in 3D space, but only a single slice will be fit."
                },
                {
                  "name": "Update input objects",
                  "description": "When selected, the input objects will have their coordinates replaced with the coordinates from the fit contour.  Applied coordinates will be solid within the boundary of the associated contour."
                },
                {
                  "name": "Output objects",
                  "description": "If \"Update input objects\" is not selected, this is the name with which the output contour objects will be stored in the workspace."
                },
                {
                  "name": "Elastic energy contribution",
                  "description": "Weight assigned to the elastic energy of the contour.  The elastic energy grows with increasing separation between adjacent points along the contour.  During optimisation, the contour will attempt to minimise the elastic energy by reducing the separation between adjacent points (i.e. the contour will shrink).  The greater the associated weight, the more this term will contribute to the overall energy of the contour.  Larger weights will cause the contour to shrink more readily."
                },
                {
                  "name": "Bending energy contribution",
                  "description": "Weight assigned to the bending energy of the contour.  The bending energy grows as the angle between adjacent segments also increases.  During optimisation, the contour will attempt to minimise the bending energy by reducing small bends in the contour.  The lowest bending energy state for a contour is a perfect circle.  The greater the associated weight, the more this term will contribute to the overall energy of the contour.  Larger weights will cause the contour to become smoother."
                },
                {
                  "name": "Image path energy contribution",
                  "description": "Weight assigned to the external (image) energy of the contour.  The image path energy is equal to the intensity of the pixels along the path.  During optimisation, the contour will attempt to minimise the image path energy by sitting along low intensity lines in the image.  The greater the associated weight, the more this term will contribute to the overall energy of the contour.  Larger weights will cause the contour to stick to dark regions more readily, but may also cause it to get stuck on local minima in the image."
                },
                {
                  "name": "Balloon energy contribution",
                  "description": "Weight assigned to the balloon energy of the contour.  The balloon energy pushes the contour outwards in at attempt to overcome the elastic energy-induced shrinkage.  The greater the associated weight, the more this term will contribute to the overall energy of the contour.  Larger weights will cause the contour to grow outwards faster."
                },
                {
                  "name": "Node density",
                  "description": "Density of coordinates along the perimeter of the input object that will be used as control points in the contour.  Density is specified as a decimal in the range 0-1, where densities approaching 0 have fewer points and a density of 1 includes all points on the object perimeter."
                },
                {
                  "name": "Search radius (px)",
                  "description": "On each optimisation iteration, each point along the contour will be tested at all local points within this search radius, with the lowest energy point taken as the new location."
                },
                {
                  "name": "Maximum number of iterations",
                  "description": "The maximum number of optimisation iterations that will be completed.  If contour stability has not been reached by this number of iterations, the contour at this point will be exported."
                },
                {
                  "name": "Use motion threshold",
                  "description": "When selected, optimisation of the contour can be terminated early if successive iterations don't yield sufficient motion (i.e. the contour has reached stability).  The threshold amount of motion is specified by \"Motion threshold (px)\".  Early termination of optimisation for stable contours will result in a speed increase for this module."
                },
                {
                  "name": "Motion threshold (px)",
                  "description": "If \"Use motion threshold\" is selected, this is the average motion of contour points between successive optimisation iterations below which the contour will be assumed to have reached stability.  If stability is reached the optimisation routine is terminated."
                },
                {
                  "name": "Show contours in realtime",
                  "description": "When selected, the contour evolution will be displayed on the input image in realtime.  This may be useful for optimising weight parameters."
                }
              ],
              "slug": "fit-active-contours"
            },
            {
              "path": "/modules/objects/process/fit-concave-hull-2d",
              "deprecated": false,
              "name": "Fit concave hull 2D",
              "shortDescription": "Fits a 2D concave hull to all objects in a collection.",
              "fullDescription": "Fits a 2D concave hull to all objects in a collection.  Each input object will be fit with a single concave hull.  Even for non-contiguous input objects, a single concave hull will be created.  The smoothness of the output hull is controlled by the \"range\" parameter, with smaller range values more closely following the surface of the object.  Larger range values should be used to overcome gaps in object edges.<br><br>Generated concave hulls are set as children of their respective input object.  If objects are in 3D, a Z-projection of the object is used.<br><br>The implementation used in this module (\"chi-shapes\") is entirely from the \"Concave hulls\" library by Glenn Hudson and Matt Duckham (<a href=\"https://archive.md/l3Un5#selection-571.0-587.218\">link<\/a>).  A paper with full details of the characteristic hulls algorithm is published in Pattern Recognition:<br><br>Duckham, M., Kulik, L., Worboys, M.F., Galton, A. (2008) \"Efficient generation of simple polygons for characterizing the shape of a set of points in the plane\", <i>Pattern Recognition<\/i>, <b>41<\/b>, 3224-3236 (<a href=\"https://archive.md/o/l3Un5/www.geosensor.net/papers/duckham08.PR.pdf\">PDF<\/a>, <a href=\"https://archive.md/o/l3Un5/dx.doi.org/10.1016/j.patcog.2008.03.023\">DOI<\/a>).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input objects to create 2D concave hulls for.  Each convex hull will be a child of its respective input object."
                },
                {
                  "name": "Output objects",
                  "description": "Output concave hull objects will be stored in the workspace with this name.  Each concave hull object will be a child of the input object it was created from."
                },
                {
                  "name": "Range (px)",
                  "description": "The maximum gap in the surface (edge) of an object that the hull can smooth over.  For gaps larger than this the hull will follow the discontinuity."
                }
              ],
              "slug": "fit-concave-hull-2d"
            },
            {
              "path": "/modules/objects/process/fit-convex-hull-2d",
              "deprecated": false,
              "name": "Fit convex hull 2D",
              "shortDescription": "Fit 2D convex hull to a 2D object.",
              "fullDescription": "Fit 2D convex hull to a 2D object.  If objects are in 3D, a convex hull is fit slice-by-slice.<br><br>Uses the ImageJ \"Fit convex hull\" function.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input objects to create 2D convex hulls for.  Each convex hull will be a child of its respective input object."
                },
                {
                  "name": "Output objects",
                  "description": "Output convex hull objects will be stored in the workspace with this name.  Each convex hull object will be a child of the input object it was created from."
                }
              ],
              "slug": "fit-convex-hull-2d"
            },
            {
              "path": "/modules/objects/process/fit-ellipse",
              "deprecated": false,
              "name": "Fit ellipse",
              "shortDescription": "Fit ellipses to all objects in a collection using ImageJ's built-in ellipse fitter.",
              "fullDescription": "Fit ellipses to all objects in a collection using ImageJ's built-in ellipse fitter.  For 3D objects, a 2D projection in the XY plane is used for fitting.  Fit ellipses can be stored either as new objects, or replacing the input object coordinates.<br><br>Note: If updating input objects with ellipse coordinates, measurements associated with the input object (e.g. spatial measurements) will still be available, but may no longer be valid.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from workspace to which ellipses will be fit.  Objects stored in 3D will be projected into the XY 2D plane (using the \"Project objects\" module) prior to fitting.  If a projected object is used, any output measurements and relationships are still applied to the input object (the projected object is discarded after use).  Measurements made by this module are associated with these input objects, irrespective of whether the fit ellipses are also stored as objects."
                },
                {
                  "name": "Fitting mode",
                  "description": "Controls which object coordinates are used for ellipse fitting:<br><ul><li>\"Fit to whole\" All coordinates for the input object are passed to the ellipse fitter.<\/li><li>\"Fit to surface\" (default) Only surface coordinates of the input object are passed to the ellipse fitter.  Surface coordinates are calculated using 4-way connectivity.<\/li><\/ul>"
                },
                {
                  "name": "Limit axis length",
                  "description": "When selected, all axes of the the fit ellipses must be shorter than the length specified by \"Maximum axis length\".  This helps filter out mis-fit ellipses and prevents unnecessary, massive memory use when storing ellipses."
                },
                {
                  "name": "Maximum axis length",
                  "description": "Maximum length of any fit ellipse axis as measured in pixel units.  This is onyl used if \"Limit axis length\" is selected."
                },
                {
                  "name": "Object output mode",
                  "description": "Controls whether the fit ellipse is stored as an object in the workspace:<br><ul><li>\"Create new objects\" Fit ellipses are stored as new objects in the workspace (name specified by \"Output objects\").  Ellipses are \"solid\" objects, irrespective of whether they were only fit to input object surface coordinates.  Ellipse objects are children of the input objects to which they were fit.  If outputting ellipse objects, any measurements are still only applied to the corresponding input objects.<\/li><li>\"Do not store\" (default) The ellipse coordinates are not stored.<\/li><li>\"Update input objects\" The coordinates of the input object are removed and replaced with the fit ellipse coordinates.  Note: Measurements associated with the input object (e.g. spatial measurements) will still be available, but may no longer be valid.<\/li><\/ul>"
                },
                {
                  "name": "Output objects",
                  "description": "Name assigned to output ellipse objects if \"Object output mode\" is in \"Create new objects\" mode."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple input objects simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "fit-ellipse"
            },
            {
              "path": "/modules/objects/process/fit-ellipsoid",
              "deprecated": false,
              "name": "Fit ellipsoid",
              "shortDescription": "Fit ellipsoids to all objects in a collection using \"<a href=\"https://imagej.",
              "fullDescription": "Fit ellipsoids to all objects in a collection using \"<a href=\"https://imagej.net/BoneJ\">BoneJ<\/a>\".  Fit ellipsoids can be stored either as new objects, or replacing the input object coordinates.<br><br>Note: If updating input objects with ellipsoid coordinates, measurements associated with the input object (e.g. spatial measurements) will still be available, but may no longer be valid.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from workspace to which ellipsoids will be fit.  Measurements made by this module are associated with these input objects, irrespective of whether the fit ellipsoids are also stored as objects."
                },
                {
                  "name": "Fitting mode",
                  "description": "Controls which object coordinates are used for ellipsoid fitting:<br><ul><li>\"Fit to whole\" All coordinates for the input object are passed to the ellipsoid fitter.<\/li><li>\"Fit to surface\" (default) Only surface coordinates of the input object are passed to the ellipsoid fitter.  Surface coordinates are calculated using 6-way connectivity.<\/li><\/ul>"
                },
                {
                  "name": "Limit axis length",
                  "description": "When selected, all axes of the the fit ellipsoids must be shorter than the length specified by \"Maximum axis length\".  This helps filter out mis-fit ellipsoids and prevents unnecessary, massive memory use when storing ellipsoids."
                },
                {
                  "name": "Maximum axis length",
                  "description": "Maximum length of any fit ellipsoid axis as measured in pixel units.  This is onyl used if \"Limit axis length\" is selected."
                },
                {
                  "name": "Object output mode",
                  "description": "Controls whether the fit ellipsoid is stored as an object in the workspace:<br><ul><li>\"Create new objects\" Fit ellipsoids are stored as new objects in the workspace (name specified by \"Output objects\").  Ellipsoids are \"solid\" objects, irrespective of whether they were only fit to input object surface coordinates.  Ellipsoid objects are children of the input objects to which they were fit.  If outputting ellipsoid objects, any measurements are still only applied to the corresponding input objects.<\/li><li>\"Do not store\" (default) The ellipsoid coordinates are not stored.<\/li><li>\"Update input objects\" The coordinates of the input object are removed and replaced with the fit ellipsoid coordinates.  Note: Measurements associated with the input object (e.g. spatial measurements) will still be available, but may no longer be valid.<\/li><\/ul>"
                },
                {
                  "name": "Output objects",
                  "description": "Name assigned to output ellipsoid objects if \"Object output mode\" is in \"Create new objects\" mode."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple input objects simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "fit-ellipsoid"
            },
            {
              "path": "/modules/objects/process/fit-longest-chord",
              "deprecated": false,
              "name": "Fit longest chord",
              "shortDescription": "Measures the longest chord of each object in a specified object collection from the workspace.",
              "fullDescription": "Measures the longest chord of each object in a specified object collection from the workspace.  The longest chord of an object is defined as the line passing between the two furthest-spaced points on the surface of the object.  This can act as an approximate measure of object length.  In addition to the longest chord length, the distance of all object surface points from the longest chord can be measured, which themselves act as an approximation of object width.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from workspace to measure longest chord for.  Measurements will be associated with the corresponding object in this collection."
                },
                {
                  "name": "Measure object width",
                  "description": "When selected the width of the object from the longest chord will be estimated.  The distance of all object surface points (those with at least one non-object neighbour in 4/6-way connectivity) from the longest chord are calculated.  Statistics (mean, minimum, maximum, sum and standard deviation) of these distances for an object are stored as measurements associated with that object."
                },
                {
                  "name": "Measure object orientation",
                  "description": "When selected, the orientation of the line in the XY plane is measured and this measurement associated with the corresponding object.  Orientations are reported in degree units and are relative to positive x-axis (positive above x-axis, negative below x-axis)."
                },
                {
                  "name": "Store end points",
                  "description": "When selected, the two coordinates corresponding to the end points of the longest chord (the two furthest-spaced points on the object surface) are stored as measurements associated with the corresponding input object."
                }
              ],
              "slug": "fit-longest-chord"
            },
            {
              "path": "/modules/objects/process/get-local-object-region",
              "deprecated": false,
              "name": "Get local object region",
              "shortDescription": "Creates a local object region (sphere or circle) for each object in a specified object collection.",
              "fullDescription": "Creates a local object region (sphere or circle) for each object in a specified object collection.  The radius of each local region can be based on a fixed value, or taken from an object measurement.  Similarly, the output sphere or circle can either be centred on the input object centroid or a location specified by XYZ measurements.  Local object regions are stored as children of their respective input object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Object collection from the workspace for which local object regions will be generated.  One region will be generated for each object and assigned as a child of the respective input object."
                },
                {
                  "name": "Output objects",
                  "description": "Output local region objects to add to the workspace.  Each local object region is a sphere with centroid coincident with the centroid of the corresponding input object.  These objects are assigned as a child of their respective input object."
                },
                {
                  "name": "Centoid value source",
                  "description": "Controls how the centroid location of the output sphere/circle is defined:<br><ul><li>\"Fixed value\" A specific centroid location, defined by \"X position\", \"Y position\" and \"Z position\", will be used for all objects.<\/li><li>\"Measurement\" The centroid will be equal to the values of specific measurements (specified by \"X-measurement\", \"Y-measurement\" and \"Z-measurement\") associated with the object being measured.  Centroids will potentially be different for each object.<\/li><li>\"Object centroid\" The output objects will be centred on the centroid of the corresponding input object.<\/li><li>\"Parent measurement\" The centroid will be equal to the values of specific measurements (specified by \"X-measurement\", \"Y-measurement\" and \"Z-measurement\") associated a parent of the object being measured (specified by \"Parent object for centroid\").  Centroids will potentially be different for each object.<\/li><\/ul>"
                },
                {
                  "name": "X position",
                  "description": "Fixed X-coordinate of the centroid to use for generating all local object regions when \"Centoid value source\" is in \"Fixed value\" mode."
                },
                {
                  "name": "Y position",
                  "description": "Fixed Y-coordinate of the centroid to use for generating all local object regions when \"Centoid value source\" is in \"Fixed value\" mode."
                },
                {
                  "name": "Z position",
                  "description": "Fixed Z-coordinate of the centroid to use for generating all local object regions when \"Centoid value source\" is in \"Fixed value\" mode."
                },
                {
                  "name": "X-measurement",
                  "description": "Measurement associated with the input or specified parent object.  This will be used as X-centroid for generating the local object region when \"Centoid value source\" is in \"Measurement\" or \"Parent measurement\" mode."
                },
                {
                  "name": "Y-measurement",
                  "description": "Measurement associated with the input or specified parent object.  This will be used as Y-centroid for generating the local object region when \"Centoid value source\" is in \"Measurement\" or \"Parent measurement\" mode."
                },
                {
                  "name": "Z-measurement",
                  "description": "Measurement associated with the input or specified parent object.  This will be used as Z-centroid for generating the local object region when \"Centoid value source\" is in \"Measurement\" or \"Parent measurement\" mode."
                },
                {
                  "name": "Parent object for centroid",
                  "description": "Parent object of the input object being processed.  This parent will provide the measurements (specified by \"X-measurement\", \"Y-measurement\" and \"Z-measurement\") to be used as the spot centroid location for generating the local object region when \"Centoid value source\" is in \"Parent measurement\" mode."
                },
                {
                  "name": "Centroid spatial units",
                  "description": "Controls whether spot centroid values (irrespective of whether they are fixed values, measurements or parent measurements) are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                },
                {
                  "name": "Radius value source",
                  "description": "Controls how the radius of the output sphere/circle is defined:<br><ul><li>\"Fixed value\" A single radius, defined by \"Radius (px)\", will be used for all objects.<\/li><li>\"Measurement\" The radius will be equal to the value of a measurement (specified by \"Radius measurement\") associated with the object being measured.  Radii will potentially be different for each object.<\/li><li>\"Parent measurement\" The radius will be equal to the value of a measurement (specified by \"Radius measurement\") associated a parent of the object being measured (specified by \"Parent object for radius\").  Radii will potentially be different for each object.<\/li><li>\"Single point\" The output objects will all be a single point corresponding to the centroid of the input object.<\/li><\/ul>"
                },
                {
                  "name": "Radius (px)",
                  "description": "Fixed spot radius to use for generating all local object regions when \"Radius value source\" is in \"Fixed value\" mode."
                },
                {
                  "name": "Radius measurement",
                  "description": "Measurement associated with the input or specified parent object.  This will be used as spot the radius for generating the local object region when \"Radius value source\" is in \"Measurement\" or \"Parent measurement\" mode."
                },
                {
                  "name": "Parent object for radius",
                  "description": "Parent object of the input object being processed.  This parent will provide the measurement (specified by \"Radius measurement\") to be used as the spot radius for generating the local object region when \"Radius value source\" is in \"Parent measurement\" mode."
                },
                {
                  "name": "Radius spatial units",
                  "description": "Controls whether spot radius values (irrespective of whether they are fixed values, measurements or parent measurements) are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\") or pixel units."
                }
              ],
              "slug": "get-local-object-region"
            },
            {
              "path": "/modules/objects/process/get-object-surface",
              "deprecated": false,
              "name": "Get object surface",
              "shortDescription": "Create surface objects for each input object.",
              "fullDescription": "Create surface objects for each input object.  Surface coordinates are those with at least one non-object neighbouring pixel (using 6-way connectivity).  Surfaces are stored as children of the input object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input objects to extract surface from."
                },
                {
                  "name": "Output objects",
                  "description": "Output surface objects to be stored in the workspace."
                }
              ],
              "slug": "get-object-surface"
            },
            {
              "path": "/modules/objects/process/grow-objects",
              "deprecated": false,
              "name": "Grow objects",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": ""
                },
                {
                  "name": "Output objects",
                  "description": ""
                },
                {
                  "name": "Starting object mode",
                  "description": ""
                },
                {
                  "name": "Growth mode",
                  "description": ""
                },
                {
                  "name": "Intensity image",
                  "description": ""
                },
                {
                  "name": "Mask output objects",
                  "description": ""
                },
                {
                  "name": "Mask image",
                  "description": ""
                },
                {
                  "name": "Binary logic",
                  "description": ""
                },
                {
                  "name": "Connectivity",
                  "description": ""
                },
                {
                  "name": "Exclude input regions",
                  "description": ""
                }
              ],
              "slug": "grow-objects"
            },
            {
              "path": "/modules/objects/process/project-objects",
              "deprecated": false,
              "name": "Project objects",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be projected into the xy-plane.  Tese are related as a parent of their respective projected object."
                },
                {
                  "name": "Output objects",
                  "description": "Output projected objects to be stored in the workspace.  These are related as children of the respective input object."
                }
              ],
              "slug": "project-objects"
            }
          ]
        },
        {
          "path": "/modules/objects/relate",
          "name": "Relate",
          "description": "Modules creating relationships between objects.  Relationships can be one-to-many (parent-child) or one-to-one (partners).",
          "slug": "relate",
          "subCategories": [{
            "path": "/modules/objects/relate/merge",
            "name": "Merge",
            "description": "Modules used for combining either different objects into one or combining objects from different collections into a single collection.",
            "slug": "merge",
            "subCategories": [],
            "modules": [
              {
                "path": "/modules/objects/relate/merge/combine-object-sets",
                "deprecated": false,
                "name": "Combine object sets",
                "shortDescription": "Combines the objects from two collections stored in the workspace.",
                "fullDescription": "Combines the objects from two collections stored in the workspace.  Either the objects from one collection can be added to the other or they can both be combined into a new collection, which is added to the workspace.<br><br>Note: Any objects added to another collection (either the \"other\" object collection or to a new collection) are duplicates of the original objects.  These duplicates contain the same spatial and temporal information as well as any relationship connections and measurements.  The original objects are unaffected by this module.",
                "version": "1.0.0",
                "parameters": [
                  {
                    "name": "Input objects 1",
                    "description": "First of two object collections to combine.  Depending on the choice for parameter \"Output mode\", this collection may be updated to include the objects from the second collection (\"Input objects 2\")."
                  },
                  {
                    "name": "Input objects 2",
                    "description": "Second of two object collections to combine.  Depending on the choice for parameter \"Output mode\", this collection may be updated to include the objects from the first collection (\"Input objects 2\")."
                  },
                  {
                    "name": "Output mode",
                    "description": "Controls where the combined object collections are stored:<br><ul><li>\"Add set objects 2 to set 1\" Duplicates of all objects in the second collection (\"Input objects 2\") are made and added to the first collection (\"Input objects 1\").<\/li><li>\"Add set objects 1 to set 2\" Duplicates of all objects in the first collection (\"Input objects 1\") are made and added to the second collection (\"Input objects 1\").<\/li><li>\"Create new object set\". Duplicates of all objects in the first (\"Input objects 1\") and second (\"Input objects 2\") collections are made and added to a new collection with name specified by \"Output objects\"<\/li><\/ul>"
                  },
                  {
                    "name": "Output objects",
                    "description": "Name of the combined output collection to be added to the workspace if \"Output mode\" is set to \"Create new object set\"."
                  }
                ],
                "slug": "combine-object-sets"
              },
              {
                "path": "/modules/objects/relate/merge/merge-related-objects",
                "deprecated": false,
                "name": "Merge related objects",
                "shortDescription": "Combine coordinates from related objects into a single object.",
                "fullDescription": "Combine coordinates from related objects into a single object.  This module can either add coordinates from all child objects into the associated parent or create entirely new merged objects.  New merged objects can either contain just coordinates from child objects, or from the parent and its children.  Any duplicate coordinates arising from overlapping child objects will only be stored once.<br><br>Note: If updating the parent objects, any previously-measured object properties may be invalid (i.e. they are not updated).  To update such measurements it's necessary to re-run the relevant measurement modules.",
                "version": "1.0.0",
                "parameters": [
                  {
                    "name": "Parent objects",
                    "description": "Input parent objects for merging.  If \"Output mode\" is set to \"Merge children into parent\" all the coordinates from child objects will be added to this object.  However, if operating in \"Create new object\" mode and \"Merge mode\" is set to \"Merge parents and children\", coordinates from parent objects will be added to the new merged objects."
                  },
                  {
                    "name": "Child objects",
                    "description": "Child objects of the input parent.  If \"Output mode\" is set to \"Merge children into parent\" all the coordinates from these objects will be added to their respective parent.  However, if operating in \"Create new object\" coordinates from these objects will be added to the new merged objects."
                  },
                  {
                    "name": "Output mode",
                    "description": "Controls where the merged object coordinates are output to:<br><ul><li>\"Create new object\" For each input parent, a new merged object will be created.  These merged objects are themselves children of the parent object.<\/li><li>\"Merge children into parent\" Combined coordinates (original coordinates from parent and coordinates of children) are added to this parent object.  Note: In this mode the coordinates of the parent object are being updated, so any previously-measured object properties may be invalid (i.e. they are not updated).  To update such measurements it's necessary to re-run the relevant measurement modules.<\/li><\/ul>"
                  },
                  {
                    "name": "Output overlapping objects",
                    "description": "If outputting new merged objects (as opposed to updating the parent), objects will be stored with this reference name."
                  },
                  {
                    "name": "Merge mode",
                    "description": "When in \"Create new object\" mode, this parameter controls what coordinates are added to the new merged objects:<ul><li>\"Merge children only\" Only coordinates from child objects are added to the merged object.  In this mode, coordinates for the parent are ignored.<\/li><li>\"Merge parents and children\" Coordinates from both the parent and child objects are added to the new merged object.<\/li><\/ul>"
                  }
                ],
                "slug": "merge-related-objects"
              },
              {
                "path": "/modules/objects/relate/merge/merge-single-class",
                "deprecated": false,
                "name": "Merge single class",
                "shortDescription": "Combines all objects at a single timepoint from a specific object collection into a single object.",
                "fullDescription": "Combines all objects at a single timepoint from a specific object collection into a single object.  Due to the fact objects are stored in 3D, there are still separate objects for each timepoint.",
                "version": "1.0.0",
                "parameters": [
                  {
                    "name": "Input objects",
                    "description": "Input object collection that will have all objects from each timepoint merged into a single object."
                  },
                  {
                    "name": "Output merged objects",
                    "description": "Output merged objects (one per input timepoint).  These objects will be stored in the workspace and accessible via this name."
                  }
                ],
                "slug": "merge-single-class"
              }
            ]
          }],
          "modules": [
            {
              "path": "/modules/objects/relate/relate-many-to-many",
              "deprecated": false,
              "name": "Relate many-to-many",
              "shortDescription": "Relate objects of two classes based on spatial proximity or overlap.",
              "fullDescription": "Relate objects of two classes based on spatial proximity or overlap.  With this module, each object from a collection can be linked to an unlimited number of other objects (see \"Relate many-to-one\" and \"Relate one-to-one\" modules for alternatives).  As such, the assigned relationships can form a network of relationships, with each object connected to multiple others.  Related objects are assigned partner relationships and can optionally also be related by a common cluster (parent) object.  Measurements associated with these relationship (e.g. a record of whether each object was linked) are stored as measurements of the relevant object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Object source mode",
                  "description": "Controls whether the objects from the same class should be related to each other, or whether objects from two different classes should be related."
                },
                {
                  "name": "Input objects 1",
                  "description": "First objection collection from the workspace to relate objects for.  If \"Object source mode\" is set to \"Different classes\", these objects will be related to the objects from the collection specified by \"Input objects 2\"; however, if set to \"Same class\", the objects from this collection will be related to each other.  Related objects will be given partner relationships."
                },
                {
                  "name": "Input objects 2",
                  "description": "Second object collection from the workspace to relate objects for.  This object collection will only be used if \"Object source mode\" is set to \"Different classes\", in which case these objects will be related to those from the collection specified by \"Input objects 1\".  Related objects will be given partner relationships."
                },
                {
                  "name": "Create cluster objects",
                  "description": "When selected, new \"cluster\" objects will be created and added to the workspace.  These objects contain no spatial information, but act as links between all objects that were related.  All objects identified as relating to each other are stored as children of the same cluster object."
                },
                {
                  "name": "Output cluster objects",
                  "description": "If storing cluster objects (when \"Create cluster objects\" is selected), the output cluster objects will be added to the workspace with this name."
                },
                {
                  "name": "Spatial separation mode",
                  "description": "Controls the type of calculation used when determining which objects are related:<br><ul><li>\"Centroid separation\" Distances are calculated from object centroid to object centroid.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Spatial overlap\" The percentage of each object, which overlaps with another object is calculated.<\/li><li>\"Surface separation\" Distances are calculated between the closest points on the object surfaces.  These distances increase in magnitude the greater the minimum object-object surface distance is; however, they are assigned a negative value if the one of the closest surface points is inside the other object (this should only occur if one object is entirely enclosed by the other) or a positive value otherwise (i.e. if the objects are separate).  Note: Any instances where the object surfaces overlap will be recorded as \"0px\" distance.<\/li><\/ul>"
                },
                {
                  "name": "Maximum separation",
                  "description": "If \"Spatial separation mode\" is set to \"Centroid separation\" or \"Surface separation\", this is the maximum separation two objects can have and still be related."
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                },
                {
                  "name": "Accept all fully enclosed objects",
                  "description": "When selected and \"Spatial separation mode\" is set to \"Surface separation\", any instances of objects fully enclosed within another are accepted as being related.  Otherwise, the absolute distance between object surfaces will be used."
                },
                {
                  "name": "Threshold mode",
                  "description": ""
                },
                {
                  "name": "Minimum overlap of object 1 (%)",
                  "description": "If \"Spatial separation mode\" is set to \"Spatial overlap\", this is the minimum percentage overlap the first object must have with the other object for the two objects to be related."
                },
                {
                  "name": "Minimum overlap of object 2 (%)",
                  "description": "If \"Spatial separation mode\" is set to \"Spatial overlap\", this is the minimum percentage overlap the second object must have with the other object for the two objects to be related."
                },
                {
                  "name": "Higher overlap threshold (%)",
                  "description": ""
                },
                {
                  "name": "Lower overlap threshold (%)",
                  "description": ""
                },
                {
                  "name": "Add measurement",
                  "subParameters": [
                    {
                      "name": "Measurement 1",
                      "description": "Measurement associated with objects from the first collection that will be used for this test."
                    },
                    {
                      "name": "Measurement 2",
                      "description": "Measurement associated with objects from the second collection that will be used for this test."
                    },
                    {
                      "name": "Calculation",
                      "description": "Controls the calculation used to compare the measurements values for the two objects being tested.  The two measurements can either be summed together or the difference between them taken."
                    },
                    {
                      "name": "Measurement limit",
                      "description": "The combined measurement (summed or difference, based on \"Calculation\" parameter) must be smaller than this value for the two objects to be linked."
                    }
                  ],
                  "description": "Add additional measurement criteria the two objects must satisfy in order to be related."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, child and parent objects must be in the same time frame for them to be linked."
                }
              ],
              "slug": "relate-many-to-many"
            },
            {
              "path": "/modules/objects/relate/relate-many-to-one",
              "deprecated": false,
              "name": "Relate many-to-one",
              "shortDescription": "Relate objects of two classes based on a variety of metrics (e.",
              "fullDescription": "Relate objects of two classes based on a variety of metrics (e.g. spatial overlap or proximity).  The assigned relationships are of the form many-to-one, where many input \"child\" objects can be related to at most, one \"parent\" object (see \"Relate many-to-many\" and \"Relate one-to-one\" modules for alternatives).  Measurements associated with this relationship (e.g. distance from child to parent surface) are stored as measurements of the relevant child object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Parent (larger) objects",
                  "description": "Input reference objects.  The \"Child (smaller) objects\" will be related to these, with children assigned one parent based on, for example, closest proximity or maximum spatial overlap.  There's no guarantee of each parent object being assigned at least one child."
                },
                {
                  "name": "Child (smaller) objects",
                  "description": "Objects to relate to the parents.  Each child will be assigned at most one parent.  There's no guarantee a child will be assigned any parent, especially when using options such as \"Limit linking by distance\"."
                },
                {
                  "name": "Method to relate objects",
                  "description": "The metric by which parent and child objects will be related:<br><ul><li>\"Matching IDs\" Parents and children will be related if they have the same ID number.  Since object ID numbers are unique within an object collection there will always be no more than one parent to a child.<\/li><li>\"Proximity\" Children are related to the spatially-closest object from the parent collection.  The exact distances used (e.g. centroid to centroid or surface to surface) are controlled by the \"Reference mode\" parameter.<\/li><li>\"Spatial overlap\" Children are related to the object from the parent collection they have the greatest spatial overlap with.  Spatial overlap is defined as the number of coincident object coordinates.<\/li><\/ul>"
                },
                {
                  "name": "Reference mode",
                  "description": "Controls the method used for determining proximity-based relationships:<br><ul><li>\"Centroid\" Distances are from child object centroids to parent object centroids.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Child centroid to parent surface\" Distances are from child object centroids to the closest point on parent object surfaces.  These distances increase in magnitude the further from the parent surface a child centroid is; however, they are assigned a positive value if the child is outside the parent and a negative value if the child is inside the parent.  For example, a centroid 5px outside the object will be simply \"5px\", whereas a centroid 5px from the surface, but contained within the parent object will be recorded as \"-5px\".<\/li><li>\"Surface\" Distances are between the closest points on the child and parent surfaces.  These distances increase in magnitude the greater the minimum parent-child surface distance is; however, they are assigned a positive value if the closest child surface point is outside the parent and a negative value if the closest child surface point is inside the parent.  For example, a closest child surface point 5px outside the object will be simply \"5px\", whereas a closest child surface point 5px from the surface, but contained within the parent object will be recorded as \"-5px\".  Note: Any instances where the child and parent surfaces overlap will be recorded as \"0px\" distance.<\/li><\/ul>"
                },
                {
                  "name": "Limit linking by distance",
                  "description": "When selected, objects will only be related if the distance between them (as calculated by the \"Reference mode\" metric) is less than or equal to the distance defined by \"Maximum linking distance (px)\"."
                },
                {
                  "name": "Maximum linking distance (px)",
                  "description": "If \"Limit linking by distance\" is selected, this is the maximum permitted distance between objects for them to be assigned a relationship."
                },
                {
                  "name": "Inside/outside mode",
                  "description": "When relating children to parent surfaces it's possible to only include children inside, outside or on the edge of the parent.This parameter controls which children are allowed to be related to the parents.  Choices are: Inside and outside (all distances), Inside only (distances < 0), Inside and on surface (distances <= 0), On surface only (distances = 0), Outside and on surface (distances >= 0), Outside only (distances > 0)."
                },
                {
                  "name": "Minimum overlap (%)",
                  "description": "Percentage of total child volume overlapping with the parent object."
                },
                {
                  "name": "Require centroid overlap",
                  "description": "When selected, child objects are only related to a parent if their centroid is inside the parent object (i.e. the child object centroid is coincident with a parent object coordinate)."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, child and parent objects must be in the same time frame for them to be linked."
                },
                {
                  "name": "Calculate fractional distance",
                  "description": "When selected, the fractional distance of the child object between the centre and surface of the parent is calculated.  This option is only available when relating children to the parent surface.  The calculation can be computationally intensive when dealing with many objects."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple object relationships simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "relate-many-to-one"
            },
            {
              "path": "/modules/objects/relate/relate-objects",
              "deprecated": true,
              "name": "Relate objects",
              "shortDescription": "Relate objects of two classes based on a variety of metrics (e.",
              "fullDescription": "Relate objects of two classes based on a variety of metrics (e.g. spatial overlap or proximity).  The assigned relationships are of the form many-to-one, where many input \"child\" objects can be related to at most, one \"parent\" object.  Measurements associated with this relationship (e.g. distance from child to parent surface) are stored as measurements of the relevant child object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Parent (larger) objects",
                  "description": "Input reference objects.  The \"Child (smaller) objects\" will be related to these, with children assigned one parent based on, for example, closest proximity or maximum spatial overlap.  There's no guarantee of each parent object being assigned at least one child."
                },
                {
                  "name": "Child (smaller) objects",
                  "description": "Objects to relate to the parents.  Each child will be assigned at most one parent.  There's no guarantee a child will be assigned any parent, especially when using options such as \"Limit linking by distance\"."
                },
                {
                  "name": "Method to relate objects",
                  "description": "The metric by which parent and child objects will be related:<br><ul><li>\"Matching IDs\" Parents and children will be related if they have the same ID number.  Since object ID numbers are unique within an object collection there will always be no more than one parent to a child.<\/li><li>\"Proximity\" Children are related to the spatially-closest object from the parent collection.  The exact distances used (e.g. centroid to centroid or surface to surface) are controlled by the \"Reference point\" parameter.<\/li><li>\"Spatial overlap\" Children are related to the object from the parent collection they have the greatest spatial overlap with.  Spatial overlap is defined as the number of coincident object coordinates.<\/li><\/ul>"
                },
                {
                  "name": "Reference point",
                  "description": "Controls the method used for determining proximity-based relationships:<br><ul><li>\"Centroid\" Distances are from child object centroids to parent object centroids.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Child centroid to parent surface\" Distances are from child object centroids to the closest point on parent object surfaces.  These distances increase in magnitude the further from the parent surface a child centroid is; however, they are assigned a positive value if the child is outside the parent and a negative value if the child is inside the parent.  For example, a centroid 5px outside the object will be simply \"5px\", whereas a centroid 5px from the surface, but contained within the parent object will be recorded as \"-5px\".<\/li><li>\"Surface\" Distances are between the closest points on the child and parent surfaces.  These distances increase in magnitude the greater the minimum parent-child surface distance is; however, they are assigned a positive value if the closest child surface point is outside the parent and a negative value if the closest child surface point is inside the parent.  For example, a closest child surface point 5px outside the object will be simply \"5px\", whereas a closest child surface point 5px from the surface, but contained within the parent object will be recorded as \"-5px\".  Note: Any instances where the child and parent surfaces overlap will be recorded as \"0px\" distance.<\/li><\/ul>"
                },
                {
                  "name": "Limit linking by distance",
                  "description": "When selected, objects will only be related if the distance between them (as calculated by the \"Reference point\" metric) is less than or equal to the distance defined by \"Maximum linking distance (px)\"."
                },
                {
                  "name": "Maximum linking distance (px)",
                  "description": "If \"Limit linking by distance\" is selected, this is the maximum permitted distance between objects for them to be assigned a relationship."
                },
                {
                  "name": "Inside/outside mode",
                  "description": "When relating children to parent surfaces it's possible to only include children inside, outside or on the edge of the parent.This parameter controls which children are allowed to be related to the parents.  Choices are: Inside and outside (all distances), Inside only (distances < 0), Inside and on surface (distances <= 0), On surface only (distances = 0), Outside and on surface (distances >= 0), Outside only (distances > 0)."
                },
                {
                  "name": "Minimum percentage overlap",
                  "description": "Percentage of total child volume overlapping with the parent object."
                },
                {
                  "name": "Require centroid overlap",
                  "description": "When selected, child objects are only related to a parent if their centroid is inside the parent object (i.e. the child object centroid is coincident with a parent object coordinate)."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, child and parent objects must be in the same time frame for them to be linked."
                },
                {
                  "name": "Merge related objects",
                  "description": "When selected, any merged children and parents will be removed from their respective object collections, combined into a single object (one merged object per parent and associated children) and stored in a new object collection."
                },
                {
                  "name": "Output overlapping objects",
                  "description": "If \"Merge related objects\" is selected, this is the name of the output related objects collection that will be stored in the workspace."
                }
              ],
              "slug": "relate-objects"
            },
            {
              "path": "/modules/objects/relate/relate-one-to-one",
              "deprecated": false,
              "name": "Relate one-to-one",
              "shortDescription": "Relate objects of two classes based on spatial proximity or overlap.",
              "fullDescription": "Relate objects of two classes based on spatial proximity or overlap.  With this module, each object from a collection can only be linked to one other object (see \"Relate many-to-many\" and \"Relate many-to-one\" modules for alternatives).  The assignments are chosen to give the optimal overall relationship connectivity.  As such, an object may not be linked to its own best match if that best match is itself closer still to another object.  Related objects are assigned partner relationships and can optionally also be related by a common cluster (parent) object.  Measurements associated with this relationship (e.g. distance to the related object) are stored as measurements of the relevant object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects 1",
                  "description": "First objection collection from the workspace to relate objects for.  These objects will be related to the objects from the collection specified by \"Input objects 2\".  Related objects will be given partner relationships and optionally (depending on the state of \"Create cluster objects\") be related by a common parent cluster object."
                },
                {
                  "name": "Input objects 2",
                  "description": "Second objection collection from the workspace to relate objects for.  These objects will be related to the objects from the collection specified by \"Input objects 1\".  Related objects will be given partner relationships and optionally (depending on the state of \"Create cluster objects\") be related by a common parent cluster object."
                },
                {
                  "name": "Create cluster objects",
                  "description": "When selected, new \"cluster\" objects will be created and added to the workspace.  These objects contain no spatial information, but act as links between all objects that were related.  Both objects identified as relating to each other are stored as children of the same cluster object."
                },
                {
                  "name": "Output cluster objects",
                  "description": "If storing cluster objects (when \"Create cluster objects\" is selected), the output cluster objects will be added to the workspace with this name."
                },
                {
                  "name": "Relationship mode",
                  "description": "Controls the type of calculation used when determining which objects are related:<br><ul><li>\"Centroid separation\" Distances are calculated from object centroid to object centroid.  These distances are always positive; increasing as the distance between centroids increases.<\/li><li>\"Spatial overlap\" The percentage of each object, which overlaps with another object is calculated.<\/li><\/ul>"
                },
                {
                  "name": "Maximum separation",
                  "description": "If \"Relationship mode\" is set to \"Centroid separation\", this is the maximum separation two objects can have and still be related."
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, spatial values are assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                },
                {
                  "name": "Minimum overlap of object 1 (%)",
                  "description": "If \"Relationship mode\" is set to \"Spatial overlap\", this is the minimum percentage overlap the first object must have with the other object for the two objects to be related."
                },
                {
                  "name": "Minimum overlap of object 2 (%)",
                  "description": "If \"Relationship mode\" is set to \"Spatial overlap\", this is the minimum percentage overlap the second object must have with the other object for the two objects to be related."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, objects must be in the same time frame for them to be linked."
                }
              ],
              "slug": "relate-one-to-one"
            },
            {
              "path": "/modules/objects/relate/single-class-cluster",
              "deprecated": false,
              "name": "Single class cluster",
              "shortDescription": "Clusters objects (based on centroid locations) using K-Means++ and/or DBSCAN algorithms.",
              "fullDescription": "Clusters objects (based on centroid locations) using K-Means++ and/or DBSCAN algorithms.  In K-Means++ [1], an optimisation of the standard K-Means algorithm, the points are assigned to a pre-determined number of clusters such that each point is assigned to its closest cluster mean position (this process is repeated until the cluster assignments stabilise or a maximum number of iterations is reached).  For DBSCAN [2], points are clustered based on a minimum number of neighbours within a specified spatial range.  As such, this algorithm doesn't require prior knowledge of the number of clusters.  Both algorithms use their respective <a href=\"https://commons.apache.org/proper/commons-math/\">Apache Commons Math implementations.<\/a><br><br>References:<br>[1] Arthur, D.; Vassilvitskii, S. (2007). \"k-means++: the advantages of careful seeding.\" <i>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics Philadelphia, PA, USA.<\/i> pp. 1027\u20131035<br>[2] Ester, M.; Kriegel, H.-P.; Sander, J.; Xu, X. (1996). \"A density-based algorithm for discovering clusters in large spatial databases with noise.\" <i>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press.<\/i> pp. 226\u2013231",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects from the workspace to be grouped into clusters.  Clusters are determined based on the centroid postions of the input objects.  Input objects will be children of their assigned clusters.  Each input object will be assigned to a single cluster."
                },
                {
                  "name": "Cluster (parent) objects",
                  "description": "Output cluster objects to be added to the workspace.  Each cluster will be a parent of the associated input objects."
                },
                {
                  "name": "Apply volume",
                  "description": "When selected, the output cluster object will be assigned volume based on the extents of all child objects."
                },
                {
                  "name": "Clustering algorithm",
                  "description": "The clustering algorithm to use:<br><ul><li>\"DBSCAN\" Points are clustered based on a minimum number of neighbours (\"Minimum number of points per cluster\") within a specified distance (\"Neighbourhood for clustering (epsilon)\").  All proximal points which satisfy these criteria are added to a common cluster.  This uses the <a href=\"https://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/stat/clustering/DBSCANClusterer.html\">Apache Commons Math3<\/a> implementation of DBSCAN, which describes the algorithm as: \"A point p is density connected to another point q, if there exists a chain of points pi, with i = 1 .. n and p1 = p and pn = q, such that each pair <pi, pi+1> is directly density-reachable. A point q is directly density-reachable from point p if it is in the -neighborhood of this point.\".<\/li><li>\"KMeans++\" Points are assigned into a pre-determined number of clusters (defined by \"Number of clusters\"), with each point assigned to the cluster with the closest centroid.  Since the cluster centroids will vary with each added point, this process is optimised iteratively.  The algorithm continues until either no points switch clusters or the maximum number of allowed iterations (\"Maximum number of iterations\") is reached.<\/li><\/ul>"
                },
                {
                  "name": "Number of clusters",
                  "description": "If \"Clustering algorithm\" is set to \"KMeans++\", this is the number of clusters the points will be assigned to."
                },
                {
                  "name": "Maximum number of iterations",
                  "description": "If \"Clustering algorithm\" is set to \"KMeans++\", this is the maximum number of optimisation iterations that will be performed.  If cluster assignment has stabilised prior to reaching this number of iterations the algorithm will terminate early."
                },
                {
                  "name": "Neighbourhood for clustering (epsilon)",
                  "description": "If \"Clustering algorithm\" is set to \"DBSCAN\", this is the minimum distance to neighbour points that must be satisfied for a point to be added to a cluster.  This distance is specified in pixel units."
                },
                {
                  "name": "Minimum number of points per cluster",
                  "description": "If \"Clustering algorithm\" is set to \"DBSCAN\", this is the minimum number of neighbour points which must be within a specified distance (\"Neighbourhood for clustering (epsilon)\") of a point for that point to be included in the cluster."
                },
                {
                  "name": "Only link objects in same frame",
                  "description": "When selected, objects must be in the same time frame for them to be assigned to a common cluster."
                }
              ],
              "slug": "single-class-cluster"
            },
            {
              "path": "/modules/objects/relate/track-objects",
              "deprecated": false,
              "name": "Track objects",
              "shortDescription": "Track objects between frames.",
              "fullDescription": "Track objects between frames.  Tracks are produced as separate \"parent\" objects to the \"child\" spots.  Track objects only serve to link different timepoint instances of objects together.  As such, track objects store no coordinate information.<br><br>Uses the <a href=\"https://imagej.net/plugins/trackmate/\">TrackMate<\/a> implementation of the Jaqaman linear assignment problem solving algorithm (Jaqaman, et al., Nature Methods, 2008).  The implementation utilises sparse matrices for calculating costs in order to minimise memory overhead.<br><br>Note: Leading point determination currently only works in 2D",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects present in individual timepoints which will be tracked across multiple frames.  These objects will become children of their assigned \"track\" parent."
                },
                {
                  "name": "Output track objects",
                  "description": "Output track objects to be stored in the workspace.  These objects will contain no spatial information, rather they act as (parent) linking  objects for the individual timepoint instances of the tracked object."
                },
                {
                  "name": "Linking method",
                  "description": "The spatial cost for linking objects together:<br><ul><li>\"Absolute overlap\" Tracks will be assigned in order to maxmimise the spatial overlap between objects in adjacent frames.  This linking method uses the full 3D volume of the objects being tracked.  Note: There is no consideration of distance between objects, so non-overlapping objects next to each other will score equally to non-overlapping objects far away (not taking additional linking weights and restrictions into account).<\/li><li>\"Centroid\" Tracks will be assigned in order to minimise the total distance between object centroids.  This linking method doesn't take object size and shape into account (unless included via volume weighting), but will work at all object separations.<\/li><\/ul>"
                },
                {
                  "name": "Minimum overlap",
                  "description": "If \"Linking method\" is set to \"Absolute overlap\", this is the minimum absolute spatial overlap (number of coincident pixels/voxels) two objects must have for them to be considered as candidates for linking."
                },
                {
                  "name": "Maximum linking distance (px)",
                  "description": "If \"Linking method\" is set to \"Centroid\", this is the minimum spatial separation (pixel units) two objects must have for them to be considered as candidates for linking."
                },
                {
                  "name": "Maximum number of missing frames",
                  "description": "Maximum number of missing frames for an object to still be tracked.  A single object undetected for longer than this would be identified as two separate tracks."
                },
                {
                  "name": "Frame gap weighting",
                  "description": "When non-zero, an additional cost will be included that penalises linking objects with large temporal separations.  The frame gap between candidate objects will be multiplied by this weight.  For example, if calculating spatial costs using centroid spatial separation, a frame gap weight of 1 will equally weight 1 frame of temporal separation to 1 pixel of spatial separation.  The larger the weight, the more this frame gap will contribute towards the total linking cost."
                },
                {
                  "name": "Favour established tracks",
                  "description": "When selected, points will be preferentially linked to tracks containing more previous points.  For example, in cases where an object was detected twice in one timepoint this will favour linking to the original track, rather than establishing the on-going track from the new point."
                },
                {
                  "name": "Track length weighting",
                  "description": "If \"Favour established tracks\" is selected this is the weight assigned to the existing track duration.  Track duration costs are calculated as 1 minus the ratio of frames in which the track was detected (up to the previous time-point)."
                },
                {
                  "name": "Use volume (minimise volume change)",
                  "description": "When enabled, the 3D volume of the objects being linked will contribute towards linking costs."
                },
                {
                  "name": "Volume weighting",
                  "description": "If \"Use volume (minimise volume change)\" is enabled, this is the weight assigned to the difference in volume of the candidate objects for linking.  The difference in volume between candidate objects is multiplied by this weight.  The larger the weight, the more this difference in volume will contribute towards the total linking cost."
                },
                {
                  "name": "Maximum volume change (px^3)",
                  "description": "If \"Use volume (minimise volume change)\" is enabled, the maximum difference in volume between candidate objects can be specified.  This maximum volume change is specified in pixel units."
                },
                {
                  "name": "Direction weighting mode",
                  "description": "Controls whether cost terms will be included based on the direction a tracked object is moving in:<br><ul><li>\"None\" No direction-based cost terms will be included.<\/li><li>\"Absolute orientation 2D\" Costs will be calculated based on the absolute orientation a candidate object would be moving in.  For example, if objects are known to be moving in one particular direction, this can favour links moving that way rather than the opposite direction.<\/li><li>\"Relative to previous step\" Costs will be calculated based on the previous trajectory of a track.  This can be used to minimise rapid changes in direction if tracked objects are expected to move smoothly.<\/li><\/ul>"
                },
                {
                  "name": "Orientation range mode",
                  "description": ""
                },
                {
                  "name": "Preferred direction",
                  "description": "If \"Direction weighting mode\" is set to \"Absolute orientation 2D\", this is the preferred direction that a track should be moving in.  Orientation is measured in degree units and is positive above the x-axis and negative below it."
                },
                {
                  "name": "Direction tolerance",
                  "description": "If using directional weighting (\"Direction weighting mode\" not set to \"None\"), this is the maximum deviation in direction from the preferred direction that a candidate object can have.  For absolute linking, this is relative to the preferred direction and for relative linking, this is relative to the previous frame."
                },
                {
                  "name": "Direction weighting",
                  "description": "If using directional weighting (\"Direction weighting mode\" not set to \"None\"), the angular difference (in degrees) between the candidate track direction and the reference direction will be muliplied by this weight.  The larger the weight, the more this angular difference will contribute towards the total linking cost."
                },
                {
                  "name": "Use measurement (minimise change)",
                  "description": "When selected, an additional cost can be included based on a measurement assigned to each object.  This allows for tracking to favour minimising variation in this measurement."
                },
                {
                  "name": "Measurement",
                  "description": "If \"Use measurement (minimise change)\" is selected, this is the measurement (associated with the input objects) for which variation within a track will be minimised."
                },
                {
                  "name": "Measurement weighting",
                  "description": "If \"Use measurement (minimise change)\" is selected, the difference in measurement associated with a candidate object and the previous instance in a target track will be multiplied by this value.  The larger the weight, the more this difference in measurement will contribute towards the total linking cost."
                },
                {
                  "name": "Maximum measurement change",
                  "description": "If \"Use measurement (minimise change)\" is selected, this is the maximum amount the measurement can change between consecutive instances in a track.  Variations greater than this will result in the track being split into two."
                }
              ],
              "slug": "track-objects"
            }
          ]
        },
        {
          "path": "/modules/objects/transform",
          "name": "Transform",
          "description": "Modules capable of updating coordinates of existing objects.  These operations can include hole filling and masking.",
          "slug": "transform",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/objects/transform/duplicate-objects-across-time",
              "deprecated": false,
              "name": "Duplicate objects across time",
              "shortDescription": "Creates a copy of objects across all frames in the specified image stack.",
              "fullDescription": "Creates a copy of objects across all frames in the specified image stack.  Duplicated objects can either have their own set of coordinates or all share the set from the input object.  While sharing coordinates across all timepoints can be much more memory efficient (no redundant duplication of data is required), any change to the coordinates in one frame will result in the change being mirrored across all timepoints, so this mode should be used with care.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input objects to duplicate across multiple timepoints."
                },
                {
                  "name": "Output objects",
                  "description": "Output duplicated objects which will be added to the workspace."
                },
                {
                  "name": "Coordinates storage mode",
                  "description": "Controls whether the duplicated objects share the same common coordinate set or each have their own.  A duplicated coordinate set will use a lot less memory (as no redundant duplication of data is required), but should be used with caution, as any change to the coordinates in one frame will result in the change being mirrored across all timepoints."
                },
                {
                  "name": "Start frame mode",
                  "description": "Specifies the source for the first timepoint the objects will be created for:<br><ul><li>\"Fixed value\" The first timepoint will be controlled by the value specified in the \"Start frame fixed value\" parameter.<\/li><li>\"Image measurement\" The first timepoint will be taken from a measurement (\"Start frame image measurement\" parameter) assigned to the image specified by \"Start frame image\".<\/li><\/ul>"
                },
                {
                  "name": "Start frame fixed value",
                  "description": "If \"Start frame mode\" is set to \"Fixed value\", this is the fixed value that will be used as the first timepoint for the output duplicated objects."
                },
                {
                  "name": "Start frame image",
                  "description": "If \"Start frame mode\" is set to \"Image measurement\", this is the image from which the measurerment \"Start frame image measurement\" will be taken."
                },
                {
                  "name": "Start frame image measurement",
                  "description": "If \"Start frame mode\" is set to \"Image measurement\", this is the measurement that will be used as the first timepoint for the duplicated objects."
                },
                {
                  "name": "Start frame offset",
                  "description": "If \"Start frame mode\" is set to \"Image measurement\", the first frame can be offset relative to the specified measurement by this number of frames.  For example, if the provided measurement is 5 and an offset of -1 is used, the first frame in the duplicated set will be 4."
                },
                {
                  "name": "End frame mode",
                  "description": "Specifies the source for the last timepoint the objects will be created for:<br><ul><li>\"Fixed value\" The last timepoint will be controlled by the value specified in the \"End frame fixed value\" parameter.<\/li><li>\"Image measurement\" The last timepoint will be taken from a measurement (\"End frame image measurement\" parameter) assigned to the image specified by \"End frame image\".<\/li><\/ul>"
                },
                {
                  "name": "End frame fixed value",
                  "description": "If \"End frame mode\" is set to \"Fixed value\", this is the fixed value that will be used as the last timepoint for the output duplicated objects."
                },
                {
                  "name": "End frame image",
                  "description": "If \"End frame mode\" is set to \"Image measurement\", this is the image from which the measurerment \"End frame image measurement\" will be taken."
                },
                {
                  "name": "End frame image measurement",
                  "description": "If \"End frame mode\" is set to \"Image measurement\", this is the measurement that will be used as the last timepoint for the duplicated objects."
                },
                {
                  "name": "End frame offset",
                  "description": "If \"End frame mode\" is set to \"Image measurement\", the end frame can be offset relative to the specified measurement by this number of frames.  For example, if the provided measurement is 5 and an offset of -1 is used, the last frame in the duplicated set will be 4."
                }
              ],
              "slug": "duplicate-objects-across-time"
            },
            {
              "path": "/modules/objects/transform/expand-and-shrink-objects",
              "deprecated": false,
              "name": "Expand and shrink objects",
              "shortDescription": "Expands or shrinks all objects in a specified object collection from the workspace.",
              "fullDescription": "Expands or shrinks all objects in a specified object collection from the workspace.  Expand and shrink operations can be performed in 2D or 3D.  These are effectively binary dilate and erode operations, respectively.  Input objects can be updated with the post-hole filling coordinates, or all output objects can be stored in the workspace as a new collection.<br><br>Note: MIA permits object overlap, so objects may share coordinates.  This is important to consider if subsequently converting objects to an image, where it's not possible to represent both objects in shared pixels.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Object collection from the workspace to apply the expand or shrink operation to."
                },
                {
                  "name": "Update input objects",
                  "description": "When selected, the post-operation objects will update the input objects in the workspace (all measurements and relationships will be retained).  Otherwise, the objects will be saved to the workspace in a new collection with the name specified by the \"Output objects\" parameter.  Note: If updating the objects, any previously-measured object properties (e.g. object volume) may become invalid.  To update such measurements it's necessary to re-run the relevant measurement modules."
                },
                {
                  "name": "Output objects",
                  "description": "If \"Update input objects\" is not selected, the post-operation objects will be saved to the workspace in a new collection with this name."
                },
                {
                  "name": "Method",
                  "description": "Controls which expand or shrink operation is applied to the input objects:<br><ul><li>\"Expand 2D\" Adds any non-object coordinates within \"Radius change\" of the object to the object.  This operates in a slice-by-slice manner, irrespective of whether a 2D or 3D object is provided.  This effectively runs a 2D binary dilation operation on each object. Uses ImageJ implementation.<\/li><li>\"Expand 3D\" Adds any non-object coordinates within \"Radius change\" of the object to the object.  This effectively runs a 3D binary dilation operation on each object.  Uses MorphoLibJ implementation.<\/li><li>\"Shrink 2D\" Removes any object coordinates within \"Radius change\" of the object boundary from the object.  This operates in a slice-by-slice manner, irrespective of whether a 2D or 3D object is provided.  This effectively runs a 2D binary erosion operation on each object.  Uses ImageJ implementation.<\/li><li>\"Shrink 3D\" Removes any object coordinates within \"Radius change\" of the object boundary from the object.  This effectively runs a 3D binary erosion operation on each object.  Uses MorphoLibJ implementation.<\/li><\/ul>"
                },
                {
                  "name": "Radius change source",
                  "description": ""
                },
                {
                  "name": "Radius change",
                  "description": "Distance from the object boundary to test for potential inclusion or removal of coordinates.  When expanding, any non-object coordinates within this distance of the object are included in the object.  While shrinking, any object coordinates within this distance of the object boundary are removed from the object.  This value is assumed specified in pixel coordinates unless \"Calibrated units\" is selected."
                },
                {
                  "name": "Measurement",
                  "description": ""
                },
                {
                  "name": "Calibrated units",
                  "description": "When selected, \"Radius change\" is assumed to be specified in calibrated units (as defined by the \"Input control\" parameter \"Spatial unit\").  Otherwise, pixel units are assumed."
                }
              ],
              "slug": "expand-and-shrink-objects"
            },
            {
              "path": "/modules/objects/transform/fill-holes-in-objects",
              "deprecated": false,
              "name": "Fill holes in objects",
              "shortDescription": "Fills holes in all objects in a collection.",
              "fullDescription": "Fills holes in all objects in a collection.  Holes are considered as non-object regions bounded on all sides by object coordinates.  This operation is performed on an object-by-object basis, so only holes bounded by coordinates of the same object will be filled.  Holes can be filled slice-by-slice in 2D (considering only coordiantes in a single XY plane using 4-way connectivity) or in full 3D (considering all surrounding coordinates using 6-way connectivity).  Input objects can be updated with the post-hole filling coordinates, or all output objects can be stored in the workspace as a new collection.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Object collection from the workspace to apply fill holes operation to."
                },
                {
                  "name": "Update input objects",
                  "description": "When selected, the post-operation objects will update the input objects in the workspace (all measurements and relationships will be retained).  Otherwise, the objects will be saved to the workspace in a new collection with the name specified by the \"Output objects\" parameter.  Note: If updating the objects, any previously-measured object properties (e.g. object volume) may become invalid.  To update such measurements it's necessary to re-run the relevant measurement modules."
                },
                {
                  "name": "Output objects",
                  "description": "If \"Update input objects\" is not selected, the post-operation objects will be saved to the workspace in a new collection with this name."
                },
                {
                  "name": "Method",
                  "description": "Controls whether the holes are filled in 2D or 3D:<br><ul><li>\"Fill holes 2D\" Holes are considered on a slice-by-slice basis (i.e. 4-way connectivity within a slice).  This can be applied to both 2D and 3D objects.<\/li><li>\"Fill holes 2D\" Holes are considered in 3D (i.e. 6-way connectivity).  Note: If a 2D object (either from a 2D or 3D source) is loaded, no holes will be filled, as the top and bottom will be considered \"open\".<\/li><\/ul>"
                }
              ],
              "slug": "fill-holes-in-objects"
            },
            {
              "path": "/modules/objects/transform/mask-objects",
              "deprecated": false,
              "name": "Mask objects",
              "shortDescription": "Applies the mask image to the specified object collection.",
              "fullDescription": "Applies the mask image to the specified object collection.  Any object coordinates coincident with black pixels (intensity 0) will be removed.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Objects to be masked."
                },
                {
                  "name": "Output object mode",
                  "description": "Controls how the masked objects will be stored:<br><ul><li>\"Create new objects\" (Default) Will add the masked objects to a new object set and store this set in the workspace.<\/li><li>\"Update input objects\" Will replace the coordinates of the input object with the masked coordinates.  All measurements associated with input objects will be transferred to the masked objects.<\/li><\/ul>"
                },
                {
                  "name": "Output objects",
                  "description": "Name for the output masked objects to be stored in workspace."
                },
                {
                  "name": "Mask mode",
                  "description": "Controls whether the input objects will be masked by an image or an object collection:<br><ul><li>\"Mask from image\" (Default) Input objects will be masked based on the image specified by \"Mask image\".  Any object regions coincident with black pixels (0 pixel intensity) will be removed.<\/li><li>\"Mask from objects (remove overlap)\" Input objects will be masked based on all objects of the collection specified by \"Mask objects\".  Any object regions coincident with any objects in the masking collection will be removed.  The masking objects will be unaffected by this process.<\/li><li>\"Mask from objects (retain overlap)\" Input objects will be masked based on all objects of the collection specified by \"Mask objects\".  Any object regions not coincident with any objects in the masking collection will be removed.  The masking objects will be unaffected by this process.<\/li><\/ul>"
                },
                {
                  "name": "Mask objects",
                  "description": "Object collection to use as mask on input objects.  Depending on which object-masking mode is selected, the input objects will either have coordinates coincident with these objects removed or retained."
                },
                {
                  "name": "Mask image",
                  "description": "Image to use as mask on input objects.  Object coordinates coincident with black pixels (pixel intensity = 0) are removed."
                },
                {
                  "name": "Remove empty objects",
                  "description": "When selected, any objects which have no volume following masking will be removed."
                }
              ],
              "slug": "mask-objects"
            },
            {
              "path": "/modules/objects/transform/reassign-enclosed-objects",
              "deprecated": false,
              "name": "Reassign enclosed objects",
              "shortDescription": "Any objects entirely enclosed by another object in the same collection are reassigned as being part of the enclosing object.",
              "fullDescription": "Any objects entirely enclosed by another object in the same collection are reassigned as being part of the enclosing object.  This operation removes the enclosed object as a separate entity.<br><br>Note: MIA objects do not permit duplication of the same coordinate within a single object, so any duplicate coordinates will be ignored (i.e. only one copy will be stored).",
              "version": "1.0.0",
              "parameters": [{
                "name": "Input objects",
                "description": "Object collection to process for enclosed objects.  Objects in this collection will be updated as a result of this module (enclosed objects will be removed as separate entities and their coordinates will be added to the enclosing object)."
              }],
              "slug": "reassign-enclosed-objects"
            },
            {
              "path": "/modules/objects/transform/track-editor",
              "deprecated": false,
              "name": "Track editor",
              "shortDescription": "Display TrackMate's TrackScheme track editor to allow tracks to be re-linked between detected objects.",
              "fullDescription": "Display TrackMate's TrackScheme track editor to allow tracks to be re-linked between detected objects.  TrackScheme also allows individual timepoint objects to be deleted.  TrackScheme represents the connectivity within a track (i.e. between instances in different timepoints) as a graph, which can be adjusted by deleting and drawing lines between each timepoint instance.  Interaction with the TrackScheme window can be directly visualised on the accompanying overlay image.<br><br>Full details on using TrackScheme can be found at <a href=\"https://imagej.net/plugins/trackmate/trackscheme\">https://imagej.net/plugins/trackmate/trackscheme<\/a>.<br><br>Note: This module will update the existing spot (individual timepoint instances) and track objects.  As such, any track-related measurements will still be available, but may no longer be valid.  They can be re-calculated by re-running the relevant measurement modules.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input track objects",
                  "description": "Track objects that will be edited.  This same object collection will be updated by this module; however, all track objects will effectively be new, so any previous measurements and relationships will be lost."
                },
                {
                  "name": "Input spot objects",
                  "description": "Spot objects for the selected tracks that will be edited.  These \"spot\" objects are the individual timepoint instances of the tracks.  This same object collection will be updated by this module and all spot objects (unless deleted during editing) will be retained; however, as their associated tracks may be different, any track-related measurements may become invalid.  Track measurements should be re-calculated using the relevant modules."
                },
                {
                  "name": "Display image",
                  "description": "In addition to the graph-based TrackScheme editor, tracks and spots will be displayed as an overlay on this image."
                },
                {
                  "name": "Show projected (3D only)",
                  "description": "When selected (and when \"Display image\" has more than one Z-slice), a separate projected image will be displayed.  This can assist understanding track connectivity in 3D."
                }
              ],
              "slug": "track-editor"
            }
          ]
        }
      ],
      "modules": []
    },
    {
      "path": "/modules/script",
      "name": "Script",
      "description": "Modules for running code directly witin a workflow.  Both single commands and full scripts (e.g. macro, Jython and Groovy) can be implemented.",
      "slug": "script",
      "subCategories": [],
      "modules": [
        {
          "path": "/modules/script/run-macro",
          "deprecated": false,
          "name": "Run macro",
          "shortDescription": "Run a specific ImageJ macro once (as opposed to the \"Run macro on objects\" module, which runs once per object).",
          "fullDescription": "Run a specific ImageJ macro once (as opposed to the \"Run macro on objects\" module, which runs once per object).  This module can optionally open an image into ImageJ for the macro to run on.  It can also intercept the output image and store it in the MIA workspace.  Variables assigned during the macro can be extracted and stored as measurements associated with the input image.<br><br>Note: ImageJ can only run one macro at a time, so by using this module the \"Simultaneous jobs\" parameter of the \"Input control\" module must be set to 1.<br><br>Note: When this module runs, all windows currently open in ImageJ will be automatically hidden, then re-opened upon macro completion.  This is to prevent accidental interference while the macro is running.  It also allows the macro to run much faster (batch mode).  To keep images open while the macro is running (for example, during debugging) start the macro with the command \"setBatchMode(false)\".",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Add variable",
              "subParameters": [
                {
                  "name": "Variable type",
                  "description": "Controls the data type of the variable that will be assigned within the macro:<br><ul><li>\"Boolean\" Variable will be assigned a true/false value depending on whether the checkbox was selected/deselected (respectively).<\/li>.<li>\"Number\" Variable will be assigned a numeric value to which mathematical operations can be applied.<\/li>.<li>\"Text\" Variable will be assigned a text value.  Irrespective of whether the value contains only numeric characters this will be interpreted as text.<\/li><\/ul>."
                },
                {
                  "name": "Variable name",
                  "description": "The variable value can be accessed from within the macro by using this variable name."
                },
                {
                  "name": "Variable checkbox",
                  "description": "Boolean (true/false) value assigned to this variable if \"Variable type\" is set to \"Boolean\"."
                },
                {
                  "name": "Variable numeric value",
                  "description": "Numeric value assigned to this variable if \"Variable type\" is set to \"Number\"."
                },
                {
                  "name": "Variable text value",
                  "description": "Text value assigned to this variable if \"Variable type\" is set to \"Text\"."
                }
              ],
              "description": "Pre-define variables, which will be immediately accessible within the macro.  These can be used to provide user-controllable values to file-based macros or to prevent the need for editing macro code via the \"Run macro\" panel."
            },
            {
              "name": "Provide input image",
              "description": "When selected, a specified image from the workspace will be opened prior to running the macro.  This image will be the \"active\" image the macro runs on."
            },
            {
              "name": "Input image",
              "description": "If \"Provide input image\" is selected, this is the image that will be loaded into the macro.  A duplicate of this image is made, so the image stored in the workspace will not be affected by any processing in the macro.  The final active image once the macro has completed can be stored in the workspace using the \"Intercept output image\" parameter."
            },
            {
              "name": "Macro mode",
              "description": "Select the source for the macro code:<br><ul><li>\"Macro file\" Load the macro from the file specified by the \"Macro file\" parameter.<\/li><li>\"Macro text\" Macro code is written directly into the \"Macro text\" box.<\/li><\/ul>"
            },
            {
              "name": "Macro text",
              "description": "Macro code to be executed.  MIA macro commands are enabled using the \"run(\"Enable MIA Extensions\");\" command which is included by default.  This should always be the first line of a macro if these commands are needed."
            },
            {
              "name": "Macro file",
              "description": "Select a macro file (.ijm) to run once, after all analysis runs have completed."
            },
            {
              "name": "Intercept output image",
              "description": "When selected, the image currently active in ImageJ at completion of the macro can be stored into the workspace.  This can either overwrite the input image in the workspace or be stored as a new image (controlled by \"Apply to input image\")."
            },
            {
              "name": "Apply to input image",
              "description": "When this and \"Intercept output image\" are selected, the image active in ImageJ at completion of the macro will update the input image in the MIA workspace.  Otherwise, the actie image will be stored as a new image in the workspace with the name specified by \"Output image\"."
            },
            {
              "name": "Output image",
              "description": "When \"Intercept output image\" is selected, but not updating the input image, the image active in ImageJ at completion of the macro will be stored in the MIA workspace with this name.  This image will be accessible to other modules using this name."
            },
            {
              "name": "Intercept variable as measurement",
              "subParameters": [{
                "name": "Variable",
                "description": "Variable assigned in the macro to be stored as a measurement associated with the input image.  This name must exactly match (including case) the name as written in the macro."
              }],
              "description": "This allows variables assigned in the macro to be stored as measurements associated with the input image."
            }
          ],
          "slug": "run-macro"
        },
        {
          "path": "/modules/script/run-macro-on-objects",
          "deprecated": false,
          "name": "Run macro on objects",
          "shortDescription": "Run a specific ImageJ macro once per object from a specified input object collection (as opposed to the \"Run macro\" module, which runs once per analysis run).",
          "fullDescription": "Run a specific ImageJ macro once per object from a specified input object collection (as opposed to the \"Run macro\" module, which runs once per analysis run).  This module can optionally open an image into ImageJ for the macro to run on.  Variables assigned during the macro can be extracted and stored as measurements associated with the current object.<br><br>Note: ImageJ can only run one macro at a time, so by using this module the \"Simultaneous jobs\" parameter of the \"Input control\" module must be set to 1.<br><br>Note: When this module runs, all windows currently open in ImageJ will be automatically hidden, then re-opened upon macro completion.  This is to prevent accidental interference while the macro is running.  It also allows the macro to run much faster (batch mode).  To keep images open while the macro is running (for example, during debugging) start the macro with the command \"setBatchMode(false)\".",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Add variable",
              "subParameters": [
                {
                  "name": "Variable type",
                  "description": "Controls the data type of the variable that will be assigned within the macro:<br><ul><li>\"Boolean\" Variable will be assigned a true/false value depending on whether the checkbox was selected/deselected (respectively).<\/li>.<li>\"Number\" Variable will be assigned a numeric value to which mathematical operations can be applied.<\/li>.<li>\"Text\" Variable will be assigned a text value.  Irrespective of whether the value contains only numeric characters this will be interpreted as text.<\/li><\/ul>."
                },
                {
                  "name": "Variable name",
                  "description": "The variable value can be accessed from within the macro by using this variable name."
                },
                {
                  "name": "Variable checkbox",
                  "description": "Boolean (true/false) value assigned to this variable if \"Variable type\" is set to \"Boolean\"."
                },
                {
                  "name": "Variable numeric value",
                  "description": "Numeric value assigned to this variable if \"Variable type\" is set to \"Number\"."
                },
                {
                  "name": "Variable text value",
                  "description": "Text value assigned to this variable if \"Variable type\" is set to \"Text\"."
                }
              ],
              "description": "Pre-define variables, which will be immediately accessible within the macro.  These can be used to provide user-controllable values to file-based macros or to prevent the need for editing macro code via the \"Run macro on objects\" panel."
            },
            {
              "name": "Input objects",
              "description": "The specified macro will be run once on each of the objects from this object collection.  No information (e.g. assigned variables) is transferred between macro runs."
            },
            {
              "name": "Provide input image",
              "description": "When selected, a specified image from the workspace will be opened prior to running the macro.  This image will be the \"active\" image the macro runs on."
            },
            {
              "name": "Input image",
              "description": "If \"Provide input image\" is selected, this is the image that will be loaded into the macro.  A duplicate of this image is made, so the image stored in the workspace will not be affected by any processing in the macro."
            },
            {
              "name": "Update image after each run",
              "description": "When selected (and \"Provide input image\" is also selected), the input image will be updated to the currently-active image at the end of each iteration.  This allows all object runs for a macro to alter the input image.."
            },
            {
              "name": "Macro mode",
              "description": "Select the source for the macro code:<br><ul><li>\"Macro file\" Load the macro from the file specified by the \"Macro file\" parameter.<\/li><li>\"Macro text\" Macro code is written directly into the \"Macro text\" box.<\/li><\/ul>"
            },
            {
              "name": "Macro text",
              "description": "Macro code to be executed.  MIA macro commands are enabled using the \"run(\"Enable MIA Extensions\");\" command which is included by default.  This should always be the first line of a macro if these commands are needed."
            },
            {
              "name": "Macro file",
              "description": "Select a macro file (.ijm) to be run by this module.  As with the \"Macro text\" parameter, this macro should start with the \"run(\"Enable MIA Extensions\");\" command."
            },
            {
              "name": "Intercept variable as measurement",
              "subParameters": [{
                "name": "Variable",
                "description": "Variable assigned in the macro to be stored as a measurement associated with the current object.  This name must exactly match (including case) the name as written in the macro."
              }],
              "description": "This allows variables assigned in the macro to be stored as measurements associated with the current object."
            }
          ],
          "slug": "run-macro-on-objects"
        },
        {
          "path": "/modules/script/run-script",
          "deprecated": false,
          "name": "Run script",
          "shortDescription": "Run Fiji-compatible scripts directly within a MIA workflow.",
          "fullDescription": "Run Fiji-compatible scripts directly within a MIA workflow.  These can be used to perform advanced actions, such as making measurements that aren't explicitly supported in MIA or running additional plugins.  Each script has access to the current workspace, thus providing a route to interact with and specify new images and objects.  Scripts also have access to this module, which in turn can be used to access all modules in the current workflow.  Scripts are run once per workflow execution.",
          "version": "1.0.1",
          "parameters": [
            {
              "name": "Script mode",
              "description": "Select the source for the script code:<br><ul><li>\"Script file\" Load the macro from the file specified by the \"Script file\" parameter.<\/li><li>\"Script text\" Script code is written directly into the \"Script text\" box.<\/li><\/ul>"
            },
            {
              "name": "Script language",
              "description": "Specify the language of the script written in the \"Script text\" box.  This parameter is not necessary when loading a script from file, since the file extension provides the language information."
            },
            {
              "name": "Script text",
              "description": "Script code to be executed.  Access to the active MIA workspace and module are provided by the first two lines of code (\"#@ io.github.mianalysis.mia.object.Workspace workspace\" and \"#@ io.github.mianalysis.mia.module.Module thisModule\"), which are included by default.  With these lines present in the script, the workspace can be accessed via the \"workspace\" variable and the current module (i.e. this script module) via the \"thisModule\" variable."
            },
            {
              "name": "Script file",
              "description": "Select a script file to be run by this module.  As with the \"Script text\" parameter, this script can start with the lines \"#@ io.github.mianalysis.mia.object.Workspace workspace\" and \"#@ io.github.mianalysis.mia.module.Module thisModule\", which provide access to the active workspace and this module."
            },
            {
              "name": "Add output",
              "subParameters": [
                {
                  "name": "Output type",
                  "description": "Specifies the type of variable that has been added to the workspace during the script.  These can either be images, new object collections or measurements associated with existing images or object collections."
                },
                {
                  "name": "Output image",
                  "description": "Name of the image that has been added to the workspace during script execution.  This name must match that assigned to the image."
                },
                {
                  "name": "Output objects",
                  "description": "Name of the object collection that has been added to the workspace during script execution.  This name must match that assigned to the object collection."
                },
                {
                  "name": "Associated image",
                  "description": "Image from the workspace (i.e. one already present before running this module) to which a measurement has been added during execution of the script."
                },
                {
                  "name": "Metadata name",
                  "description": "Name of a metadata item which has been added during execution of the script."
                },
                {
                  "name": "Associated objects",
                  "description": "Object collection from the workspace (i.e. one already present before running this module) to which a measurement has been added during execution of the script."
                },
                {
                  "name": "Measurement name",
                  "description": "Name of the measurement that has been added to either an image or objects of an object collection.  This name must exactly match that assigned in the script."
                },
                {
                  "name": "Parent name",
                  "description": "Name of a parent object already in the workspace which has been related to the objects specified by \"Children name\" during execution of the script."
                },
                {
                  "name": "Children name",
                  "description": "Name of child objects already in the workspace which have been related to the objects specified by \"Parent name\" during execution of the script."
                },
                {
                  "name": "Partners name 1",
                  "description": "Name of partner object already in the workspace which have been related to the objects specified by \"Partners name 2\" during execution of the script."
                },
                {
                  "name": "Partners name 2",
                  "description": "Name of partner object already in the workspace which have been related to the objects specified by \"Partners name 1\" during execution of the script."
                }
              ],
              "description": "If images or new object collections have been added to the workspace during script execution they must be added here, so subsequent modules are aware of their presence.  The act of adding an output via this method simply tells subsequent MIA modules the relevant images/object collections were added to the workspace; the image/object collection must be added to the workspace during script execution using the \"workspace.addImage([image])\" or \"workspace.addObjects([object collection])\" commands."
            }
          ],
          "slug": "run-script"
        },
        {
          "path": "/modules/script/run-single-command",
          "deprecated": false,
          "name": "Run single command",
          "shortDescription": "Run a single command on an image from the workspace.",
          "fullDescription": "Run a single command on an image from the workspace.   This module only runs commands of the format \"run([COMMAND], [ARGUMENTS])\".  For example, the command \"run(\"Subtract Background...\", \"rolling=50 stack\");\" would be specified with the \"Command\" parameter set to \"Subtract Background...\" and the \"Parameters\" parameter set to \"rolling=50 stack\".  For more advanced macro processing please use the \"Run macro\" module.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input image",
              "description": "Image from workspace to apply command to.  This image is duplicated prior to application of the command, so won't be updated by default.  To store any changes back onto this image, select the \"Apply to input image\" parameter."
            },
            {
              "name": "Apply to input image",
              "description": "When selected, the image returned by the command will be stored back into the MIA workspace at the same name as the input image.  This will update the input image."
            },
            {
              "name": "Output image",
              "description": "When \"Apply to input image\" is not selected this will store the command output image into the MIA workspace with the name specified by this parameter."
            },
            {
              "name": "Command",
              "description": "The command command to run.  This must be the exact name as given by the ImageJ command recorder.  Note: Only commands of the format \"run([MACRO TITLE], [ARGUMENTS])\" can be run by this module.  For more advanced command processing please use the \"Run macro\" module."
            },
            {
              "name": "Parameters",
              "description": "The options to pass to the command."
            },
            {
              "name": "Enable multithreading",
              "description": "When running a command which operates on a single slice at a time, multithreading will create a new thread for each slice.  This can provide a speed improvement when working on a computer with a multi-core CPU.  Note: Multithreading is only available for commands containing the \"stack\" argument."
            }
          ],
          "slug": "run-single-command"
        }
      ]
    },
    {
      "path": "/modules/system",
      "name": "System",
      "description": "Operations handling MIA properties such as memory and global variables.",
      "slug": "system",
      "subCategories": [],
      "modules": [
        {
          "path": "/modules/system/add-custom-metadata-item",
          "deprecated": false,
          "name": "Add custom metadata item",
          "shortDescription": "This module allows for a specific metadata item to be used.",
          "fullDescription": "This module allows for a specific metadata item to be used.  An example of this would be to add a label for generic (metadata-based) filename generation in the image loader (i.e. all images to be loaded must have the word \"phase\" in them).  Output metadata values can themselves be constructed from existing metadata values, accessed using the M{[NAME]} form (e.g. M{Filename}.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Metadata name",
              "description": "Name for metadata item to be assigned."
            },
            {
              "name": "Metadata value",
              "description": "Value of metadata item.  This will be the same for all analysis runs performed at the same time since this value doesn't update during an analysis."
            }
          ],
          "slug": "add-custom-metadata-item"
        },
        {
          "path": "/modules/system/gui-separator",
          "deprecated": false,
          "name": "GUI separator",
          "shortDescription": "GUI separators break the module list into sections, where all modules are considered to be in the \"section\" corresponding to the separator above them in the module list.",
          "fullDescription": "GUI separators break the module list into sections, where all modules are considered to be in the \"section\" corresponding to the separator above them in the module list.  All modules contained within these sections can be enabled/disabled <i>en masse<\/i> by enabling/disabling their associated separator.  In processing view, the separator can be shown as a blue line, while in editing view it appears as a normal module entry albeit with blue text.  In both views it's possible to hide/show the contents of a separator section using the blue arrows.<br><br>Separators are intended to break analyses into logical groups, thus making workflow organisation easier and tidier.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Show in processing view",
              "description": "Display this GUI separator in the processing view.  When this parameter is selected, this separator will still only be shown if at least one of the following is true:<br><ul><li>The separator (and thus all associated modules) can be enabled/disabled from the processing view.  This is done by ticking \"Can be disabled\" at the top of this module in editing view.  Enabling/disabling a GUI separator will automatically apply the same state to all modules contained within (i.e. between this separator and the next separator).  If the separator can be enabled/disabled from processing view the power icon will be shown in green when enabled or in black with a strikethrough when disabled; however, if it can't be enabled/disabled, the power icon will be greyed out.<\/li><li>At least one module associated with this separator (a module between this separator and the next) can be disabled from the processing view.  Modules can be set to allow enabling/disabling from processing view by ticking \"Can be disabled\" at the top of the relevant module parameter control in editing view.<\/li><li>At least one parameter from a module associated with this separator (a module between this separator and the next) can be set from the processing view.  Parameters can be set as visible in processing view by clicking the eyeball icon to the right of that parameter in editing view.  When a module is visible an open eye is shown, while this is a closed eye when it's not visible (default option).<\/li><\/ul>"
            },
            {
              "name": "Expanded in processing view",
              "description": "In processing view, all controls between this separator and the next can be displayed/hidden by clicking the blue arrows either side of the separator bar.  This parameter controls if this parameter is in the expanded (controls displayed) or collapsed (controls hidden) state.  When the expansion state is changed from the processing view, this parameter is automatically updated."
            },
            {
              "name": "Expanded in editing view",
              "description": "In editing view, all modules in the module list between this separator and the next can be displayed/hidden by clicking the blue arrows either side of module button.  This parameter controls if this parameter is in the expanded (modules displayed) or collapsed (modules hidden) state.  When the expansion state is changed from the editing view, this parameter is automatically updated."
            }
          ],
          "slug": "gui-separator"
        },
        {
          "path": "/modules/system/global-variables",
          "deprecated": false,
          "name": "Global variables",
          "shortDescription": "Create fixed values which can be accessed by text entry parameters from all modules.",
          "fullDescription": "Create fixed values which can be accessed by text entry parameters from all modules.  Use of global variables allows the same value to be used across multiple different modules without the need to explicitly type it.  Global variables are accessed with the notation \"V{[NAME]}\", where \"[NAME]\" is replaced with the name of the relevant variable.  Global variables can be used by any text or numeric parameter.",
          "version": "1.0.0",
          "parameters": [{
            "name": "Add new variable",
            "subParameters": [
              {
                "name": "Variable name",
                "description": "Name of this variable.  This is the name that will be used when referring to the variable in place of fixed values.  To refer to variables, use the form \"V{[VARIABLE_NAME]}\", where \"[VARIABLE_NAME]\" is replaced by the variable name.  For example, a variable called \"my_var\" would be referred to using the text \"V{my_var}\"."
              },
              {
                "name": "Variable type",
                "description": "Controls how the variable is specified:<br><ul><li>\"Boolean\" The output variable is assigned either \"True\" or \"False\" depending on whether the switch (specified with the \"Variable Boolean\" parameter) is enabled or disabled, respectively.<\/li><li>\"Choice\" Select the output variable from a pre-determined list of options (specified with the \"Variable choices\" parameter).<\/li><li>\"File\" Select a specific file on the computer for this variable using the \"Variable file\" parameter.  The variable will be set to the full path to this file.  Note: backslash characters will be escaped (i.e. \"\\\" will appear as \"\\\\\").<\/li><li>\"Folder\" Select a specific folder on the computer for this variable using the \"Variable folder\" parameter.  The variable will be set to the full path to this folder.  Note: backslash characters will be escaped (i.e. \"\\\" will appear as \"\\\\\").<\/li><li>\"Text\" Specify a fixed text value for this variable using the \"Variable value\" parameter.<\/li><\/ul>"
              },
              {
                "name": "Variable Boolean",
                "description": "Boolean value for the corresponding global variable when \"Variable type\" is in \"Boolean\" mode."
              },
              {
                "name": "Variable value",
                "description": "Fixed value for the corresponding global variable when \"Variable type\" is in \"Text\" mode."
              },
              {
                "name": "Variable file",
                "description": "Fixed value file location for the corresponding global variable when \"Variable type\" is in \"File\" mode."
              },
              {
                "name": "Variable folder",
                "description": "Fixed value folder location for the corresponding global variable when \"Variable type\" is in \"Folder\" mode."
              },
              {
                "name": "Variable choices",
                "description": "When \"Variable type\" is set to \"Choice\" mode, these are the options that will be presented by the \"Variable choice\" drop-down parameter.  Choices are specified as a comma-separated list."
              },
              {
                "name": "Variable choice",
                "description": "Pre-defined list of choices to select global variable value from when \"Variable type\" is in \"Choice\" mode."
              },
              {
                "name": "Store as metadata item",
                "description": "When selected, the variable will be stored as a metadata item.  This allows it to be exported to the final spreadsheet."
              }
            ],
            "description": "Add a new global variable.  Added variables can be removed using the \"Remove\" button."
          }],
          "slug": "global-variables"
        },
        {
          "path": "/modules/system/remove-images",
          "deprecated": false,
          "name": "Remove images",
          "shortDescription": "Removes the specified image(s) from the workspace.",
          "fullDescription": "Removes the specified image(s) from the workspace.  Doing this helps keep memory usage down.  Measurements associated with an image can be retained for further use.",
          "version": "1.0.0",
          "parameters": [{
            "name": "Remove another image",
            "subParameters": [
              {
                "name": "Input image",
                "description": "Name of the image to be removed from the workspace."
              },
              {
                "name": "Retain measurements",
                "description": "Retain measurements for this image, or remove everything.  When selected, the image intensity information will be removed, as this is typically where most memory us used, however any measurements associated with it will be retained."
              }
            ],
            "description": "Mark another image from the workspace for removal."
          }],
          "slug": "remove-images"
        },
        {
          "path": "/modules/system/remove-objects",
          "deprecated": false,
          "name": "Remove objects",
          "shortDescription": "Removes the specified object set(s) from the workspace.",
          "fullDescription": "Removes the specified object set(s) from the workspace.  Doing this helps keep memory usage down.  Measurements associated with an object set can be retained for further use.",
          "version": "1.0.0",
          "parameters": [{
            "name": "Remove another object set",
            "subParameters": [
              {
                "name": "Input objects",
                "description": "Name of the object set to be removed from the workspace."
              },
              {
                "name": "Retain measurements",
                "description": "Retain measurements for this object set, or remove everything.  When selected, the object coordinates will be removed, as this is typically where most memory us used, however any measurements associated with each object will be retained."
              }
            ],
            "description": "Mark another object set from the workspace for removal."
          }],
          "slug": "remove-objects"
        }
      ]
    },
    {
      "path": "/modules/visualisation",
      "name": "Visualisation",
      "description": "Modules primarily concerned with generating and displaying images.  These include adding overlay elements and plotting data.",
      "slug": "visualisation",
      "subCategories": [
        {
          "path": "/modules/visualisation/overlays",
          "name": "Overlays",
          "description": "Modules adding ImageJ overlay elements to images.  For example, object outlines, object ID labels or fixed text.",
          "slug": "overlays",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/visualisation/overlays/add-all-object-points",
              "deprecated": false,
              "name": "Add all object points",
              "shortDescription": "Adds an overlay to the specified input image with all points in the specified input objects rendered as small circles.",
              "fullDescription": "Adds an overlay to the specified input image with all points in the specified input objects rendered as small circles.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-all-object-points"
            },
            {
              "path": "/modules/visualisation/overlays/add-arrows",
              "deprecated": false,
              "name": "Add arrows",
              "shortDescription": "Adds an overlay to the specified input image with each object represented by an arrow.",
              "fullDescription": "Adds an overlay to the specified input image with each object represented by an arrow.  The size, colour and orientation of each arrow can be fixed or based on a measurement value.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Arrow orientation mode",
                  "description": "Source for arrow orientation values:<br><ul><li>\"Measurement\" Orientation of arrows will be based on the measurement specified by the parameter \"Measurement for orientation\" for each object.<\/li><li>\"Parent measurement\" Orientation of arrows will be based on the measurement specified by the parameter \"Measurement for orientation\" taken from a parent of each object.  The parent object providing this measurement is specified by the parameter \"Parent object for orientation\".<\/li><\/ul>"
                },
                {
                  "name": "Parent object for orientation",
                  "description": "Parent objects providing the measurements on which the orientation of the arrows are based."
                },
                {
                  "name": "Measurement for orientation",
                  "description": "Measurement that defines the orientation of each arrow.  Measurements should be supplied in degree units."
                },
                {
                  "name": "Arrow length mode",
                  "description": "Method for determining the length of arrows:<br><ul><li>\"Fixed value\" All arrows are the same length.  Length is controlled by the \"Length value (px)\" parameter.<\/li><li>\"Measurement\" Arrow length is proportional to the measurement value specified by the \"Measurement for length\" parameter.  Absolute arrow lengths are adjusted by the \"Arrow length scale\" multiplication factor.<\/li><li>\"Parent measurement\" Arrow length is proportional to a parent object measurement value.  The parent is specified by the \"Parent object for length\" parameter and the measurement value by \"Measurement for length\".  Absolute arrow lengths are adjusted by the \"Arrow length scale\" multiplication factor.<\/li><\/ul>"
                },
                {
                  "name": "Length value (px)",
                  "description": "Fixed value specifying the length of all arrows in pixel units."
                },
                {
                  "name": "Parent object for length",
                  "description": "Parent objects from which the arrow length measurements will be taken."
                },
                {
                  "name": "Measurement for length",
                  "description": "Measurement value that will be used to control the arrow length.  This value is adjusted using the \"Arrow length scale\" muliplication factor."
                },
                {
                  "name": "Arrow length scale",
                  "description": "Measurement values will be multiplied by this value prior to being used to control the arrow length.  Each arrow will be <i>MeasurementValue*LengthScale<\/i> pixels long."
                },
                {
                  "name": "Head size",
                  "description": "Size of the arrow head.  This should be an integer between 0 and 30, where 0 is the smallest possible head and 30 is the largest."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display overlay elements in all frames, irrespective of whether each object is present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-arrows"
            },
            {
              "path": "/modules/visualisation/overlays/add-contour-lines",
              "deprecated": false,
              "name": "Add contour lines",
              "shortDescription": "Adds overlay contour lines (and optional labels) to an image.",
              "fullDescription": "Adds overlay contour lines (and optional labels) to an image.  Lines correspond to contours of constant intensity in the input image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Minimum intensity",
                  "description": "Minimum contour magnitude to display."
                },
                {
                  "name": "Maximum intensity",
                  "description": "Maximum contour magnitude to display."
                },
                {
                  "name": "Number of contours",
                  "description": "Number of different contour magnitudes to display.  Contour lines are equally spaced in magnitude between \"Minimum intensity\" and \"Maximum intensity\"."
                },
                {
                  "name": "Contour colour mode",
                  "description": "Determines the colour look-up table to use when rendering the contours.  Colour corresponds to the magnitude of that contour line:<br><br>- \"Black fire\" Standard ImageJ \"Fire\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Ice\" Standard ImageJ \"Ice\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Jet\" Look-up table with colour progression blue, cyan, green, yellow, orange, red.<br><br>- \"Physics\" Standard Fiji \"Physics\" look-up table.  Values taken from \"https://github.com/fiji/fiji/tree/master/luts/physics.lut\".<br><br>- \"Random\" Random sequence of colours.<br><br>- \"Single colour\" All contour lines have the same colour, determined by the \"Contour colour\" parameter.<br><br>- \"Single colour gradient\" Single colour gradient from the colour determined by the \"Contour colour\" parameter (lowest magnitude contour line) and white (highest magnitude contour line).<br><br>- \"Spectrum\" Standard ImageJ \"Spectrum\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Thermal\" Standard Fiji \"Thermal\" look-up table.  Values taken from \"https://github.com/fiji/fiji/tree/master/luts/Thermal.lut\".<br>"
                },
                {
                  "name": "Contour colour",
                  "description": "Contour colour used when \"Contour colour mode\" is set to either \"Single colour\" or \"Single colour gradient\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Draw every N points",
                  "description": "Specifies the interval between plotted points on the contour line.  This is useful when there are a lot of contour lines in an image and where a reduction in line precision isn't problematic.  Plotting fewer points will reduce the memory required to store/display overlays."
                },
                {
                  "name": "Show labels",
                  "description": "Display magnitude values as text on each contour line.  Labels are placed randomly on each contour and are oriented in line with the local contour line."
                },
                {
                  "name": "Decimal places",
                  "description": "Number of decimal places to use when displaying numeric values."
                },
                {
                  "name": "Use scientific notation",
                  "description": "When enabled, numeric values will be displayed in the format <i>1.23E-3<\/i>.  Otherwise, the same value would appear as <i>0.00123<\/i>."
                },
                {
                  "name": "Label colour mode",
                  "description": "Determines the colour look-up table to use when rendering the contour labels.  Colour corresponds to the magnitude of that contour line:<br><br>- \"Black fire\" Standard ImageJ \"Fire\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Ice\" Standard ImageJ \"Ice\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Jet\" Look-up table with colour progression blue, cyan, green, yellow, orange, red.<br><br>- \"Physics\" Standard Fiji \"Physics\" look-up table.  Values taken from \"https://github.com/fiji/fiji/tree/master/luts/physics.lut\".<br><br>- \"Random\" Random sequence of colours.<br><br>- \"Single colour\" All contour line labels have the same colour, determined by the \"Contour colour\" parameter.<br><br>- \"Single colour gradient\" Single colour gradient from the colour determined by the \"Contour colour\" parameter (lowest magnitude contour line label) and white (highest magnitude contour line label).<br><br>- \"Spectrum\" Standard ImageJ \"Spectrum\" look-up table.  Values taken from \"https://github.com/imagej/ImageJA/blob/master/src/main/java/ij/LookUpTable.java\".<br><br>- \"Thermal\" Standard Fiji \"Thermal\" look-up table.  Values taken from \"https://github.com/fiji/fiji/tree/master/luts/Thermal.lut\".<br>"
                },
                {
                  "name": "Label colour",
                  "description": "Contour line label colour used when \"Contour colour mode\" is set to either \"Single colour\" or \"Single colour gradient\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the text label."
                }
              ],
              "slug": "add-contour-lines"
            },
            {
              "path": "/modules/visualisation/overlays/add-from-position-measurement",
              "deprecated": false,
              "name": "Add from position measurement",
              "shortDescription": "Adds an overlay to the specified input image representing each object by a single marker.",
              "fullDescription": "Adds an overlay to the specified input image representing each object by a single marker.  Unlike \"Add object centroid\" the position of the marker is determined by measurements associated with the relevant object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "X-position measurement",
                  "description": "Object measurement specifying the X-position of the overlay marker.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Y-position measurement",
                  "description": "Object measurement specifying the Y-position of the overlay marker.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Z-position measurement",
                  "description": "Object measurement specifying the Z-position (slice) of the overlay marker.  Measurement value must be specified in slice units."
                },
                {
                  "name": "Use radius measurement",
                  "description": "When selected, the radius of the overlay marker circle is controlled by the measurement specified by \"Measurement for radius\".  When not selected, marker size is controlled by the \"Point size\" parameter."
                },
                {
                  "name": "Measurement for radius",
                  "description": "Object measurement use to specify the radius of the overlay marker circle.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Point size",
                  "description": "Size of each overlay marker.  Choices are: Tiny, Small, Medium, Large, Extra large."
                },
                {
                  "name": "Point type",
                  "description": "Type of overlay marker used to represent each object.  Choices are: Circle, Cross, Dot, Hybrid."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-from-position-measurement"
            },
            {
              "path": "/modules/visualisation/overlays/add-labels",
              "deprecated": false,
              "name": "Add labels",
              "shortDescription": "Adds an overlay to the specified input image with each object represented by a text label.",
              "fullDescription": "Adds an overlay to the specified input image with each object represented by a text label.  The label can include information such as measurements, associated object counts or ID numbers.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Label mode",
                  "description": "Controls what information each label displays:<br><ul><li>\"Child count\" The number of children from a specific object collection for each object.  The children to summarise are selected using the \"Child objects for label\" parameter.<\/li><li>\"ID\" The ID number of the object.<\/li><li>\"Measurement value\" A measurement associated with the object.  The measurement is selected using the \"Measurement for label\" parameter.<\/li><li>\"Parent ID\" The ID number of a parent of the object.  The parent object is selected using the \"Parent object for label\" parameter.<\/li><li>\"Parent measurement value\" A measurement associated with a parent of the object.  The measurement is selected using the \"Measurement for label\" parameter and the parent object with the \"Parent object for label\" parameter.<\/li><li>\"Partner count\" The number of partners from a specific object collection for each object.  The partners to summarise are selected using the \"Partner objects for label\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Decimal places",
                  "description": "Number of decimal places to use when displaying numeric values."
                },
                {
                  "name": "Use scientific notation",
                  "description": "When enabled, numeric values will be displayed in the format <i>1.23E-3<\/i>.  Otherwise, the same value would appear as <i>0.00123<\/i>."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the text label."
                },
                {
                  "name": "X-offset",
                  "description": "Offset the label by this number of pixels horizontally (along the x-axis).  Increasingly positive numbers move the label right."
                },
                {
                  "name": "Y-offset",
                  "description": "Offset the label by this number of pixels vertically (along the y-axis).  Increasingly positive numbers move the label down."
                },
                {
                  "name": "Centre text",
                  "description": ""
                },
                {
                  "name": "Child objects for label",
                  "description": "If \"Label mode\" is set to \"Child count\", these are the child objects which will be counted and displayed on the label.  These objects will be children of the input objects."
                },
                {
                  "name": "Parent object for label",
                  "description": "If \"Label mode\" is set to either \"Parent ID\" or \"Parent measurement value\", these are the parent objects which will be used.  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for label",
                  "description": "If \"Label mode\" is set to \"Partner count\", these are the partner objects which will be counted and displayed on the label.  These objects will be partners of the input objects."
                },
                {
                  "name": "Measurement for label",
                  "description": "If \"Label mode\" is set to either \"Measurement value\" or \"Parent measurement value\", these are the measurements which will be used."
                },
                {
                  "name": "Prefix",
                  "description": "Text to display at beginning of every label"
                },
                {
                  "name": "Suffix",
                  "description": "Text to display at end of every label"
                },
                {
                  "name": "Label position",
                  "description": "Determines the method used for placing the label overlay for each object:<br><ul><li>\"Centre of object\" Labels will be placed at the centroid (average coordinate location) of each object.  This position won't necessarily coincide with a region corresponding to that object.  For example, the centroid of a crescent shape won't lie on the crescent itself.<\/li><li>\"Fixed value\" Labels will be placed at the x,y,z location specified by the user.<\/li><li>\"Inside largest part of object\" Labels will be placed coincident with the largest region of each object.  This ensures the label is placed directly over the relevant object.<\/li><\/ul><li>\"Object measurement\" Labels will be placed coincident with the specified measurements for each object.<\/li><\/ul>"
                },
                {
                  "name": "X-position measurement",
                  "description": "Object measurement specifying the X-position of the label.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Y-position measurement",
                  "description": "Object measurement specifying the Y-position of the label.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Z-position measurement",
                  "description": "Object measurement specifying the Z-position (slice) of the label.  Measurement value must be specified in slice units."
                },
                {
                  "name": "X-position",
                  "description": "Fixed value specifying the X-position of the label.  Specified in pixel units."
                },
                {
                  "name": "Y-position",
                  "description": "Fixed value specifying the Y-position of the label.  Specified in pixel units."
                },
                {
                  "name": "Z-position",
                  "description": "Fixed value specifying the Z-position of the label.  Specified in pixel units."
                },
                {
                  "name": "Render in all object slices",
                  "description": "Display overlay elements in all slices corresponding to that object."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display overlay elements in all frames, irrespective of whether each object is present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-labels"
            },
            {
              "path": "/modules/visualisation/overlays/add-line",
              "deprecated": false,
              "name": "Add line",
              "shortDescription": "Draws an overlay line between two specified points.",
              "fullDescription": "Draws an overlay line between two specified points.<br><br>Note: This currently only works in 2D.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Reference mode 1",
                  "description": "Controls for the location of the first end of the line to be drawn:<br><br>- \"Object centroid\" This end of the line is coincident with the centre of the input object.<br><br>- \"Image measurement\" This end of the line is coincident with a pair of XY measurements taken from an image.  The image to be used is selected with the \"Reference image 1\" parameter and the measurements by the \"X-pos. image meas. 1 (px)\" and \"Y-pos. image meas. 1 (px)\" parameters.<br><br>- \"Object measurement\" This end of the line is coincident with a pair of XY measurements taken from the current object.  The measurements are selected with the \"X-pos. object meas. 1 (px)\" and \"Y-pos. object meas. 1 (px)\" parameters.<br>"
                },
                {
                  "name": "Reference image 1",
                  "description": "Image providing the XY coordinate measurements to be used as the location of the first end of the line."
                },
                {
                  "name": "X-pos. image meas. 1 (px)",
                  "description": "Measurement used as X-coordinate for the first end of the line.  This measurement will be taken from the image specified by \"Reference image 1\" when \"Reference mode 1\" is set to \"Image measurement\".  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Y-pos. image meas. 1 (px)",
                  "description": "Measurement used as Y-coordinate for the first end of the line.  This measurement will be taken from the image specified by \"Reference image 1\" when \"Reference mode 1\" is set to \"Image measurement\".  The coordinate value is specified in pixel units."
                },
                {
                  "name": "X-pos. object meas. 1 (px)",
                  "description": "Measurement used as X-coordinate for the first end of the line.  This measurement will be taken from the current object.  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Y-pos. object meas. 1 (px)",
                  "description": "Measurement used as Y-coordinate for the first end of the line.  This measurement will be taken from the current object.  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Reference mode 2",
                  "description": "Controls for the location of the second end of the line to be drawn:<br><br>- \"Object centroid\" This end of the line is coincident with the centre of the input object.<br><br>- \"Image measurement\" This end of the line is coincident with a pair of XY measurements taken from an image.  The image to be used is selected with the \"Reference image 2\" parameter and the measurements by the \"X-pos. image meas. 2 (px)\" and \"Y-pos. image meas. 2 (px)\" parameters.<br><br>- \"Object measurement\" This end of the line is coincident with a pair of XY measurements taken from the current object.  The measurements are selected with the \"X-pos. object meas. 2 (px)\" and \"Y-pos. object meas. 2 (px)\" parameters.<br>"
                },
                {
                  "name": "Reference image 2",
                  "description": "Image providing the XY coordinate measurements to be used as the location of the second end of the line."
                },
                {
                  "name": "X-pos. image meas. 2 (px)",
                  "description": "Measurement used as X-coordinate for the second end of the line.  This measurement will be taken from the image specified by \"Reference image 2\" when \"Reference mode 2\" is set to \"Image measurement\".  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Y-pos. image meas. 2 (px)",
                  "description": "Measurement used as Y-coordinate for the second end of the line.  This measurement will be taken from the image specified by \"Reference image 2\" when \"Reference mode 2\" is set to \"Image measurement\".  The coordinate value is specified in pixel units."
                },
                {
                  "name": "X-pos. object meas. 2 (px)",
                  "description": "Measurement used as X-coordinate for the second end of the line.  This measurement will be taken from the current object.  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Y-pos. object meas. 2 (px)",
                  "description": "Measurement used as Y-coordinate for the second end of the line.  This measurement will be taken from the current object.  The coordinate value is specified in pixel units."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-line"
            },
            {
              "path": "/modules/visualisation/overlays/add-object-centroid",
              "deprecated": false,
              "name": "Add object centroid",
              "shortDescription": "Adds an overlay to the specified input image representing each object by a single marker placed at the centroid of that object.",
              "fullDescription": "Adds an overlay to the specified input image representing each object by a single marker placed at the centroid of that object.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Point size",
                  "description": "Size of each overlay marker.  Choices are: Tiny, Small, Medium, Large, Extra large."
                },
                {
                  "name": "Point type",
                  "description": "Type of overlay marker used to represent each object.  Choices are: Circle, Cross, Dot, Hybrid."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-object-centroid"
            },
            {
              "path": "/modules/visualisation/overlays/add-object-fill",
              "deprecated": false,
              "name": "Add object fill",
              "shortDescription": "Adds an overlay to the specified input image showing the extent of each specified input object as a filled shape.",
              "fullDescription": "Adds an overlay to the specified input image showing the extent of each specified input object as a filled shape.  The opacity of the filled shape can be varied to see the image underneath.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-object-fill"
            },
            {
              "path": "/modules/visualisation/overlays/add-object-outline",
              "deprecated": false,
              "name": "Add object outline",
              "shortDescription": "Adds an overlay to the specified input image showing the outline of each specified input object.",
              "fullDescription": "Adds an overlay to the specified input image showing the outline of each specified input object.",
              "version": "1.0.1",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Reduce line complexity",
                  "description": "When enabled the contour can be plotted using a reduced number of points.  This is useful for simplifying outlines for large objects, where a reduction in line precision isn't problematic.  Higher interpolation values will reduce the memory required to store/display overlays."
                },
                {
                  "name": "Line interpolation",
                  "description": "Specifies the interval between plotted points on the contour line."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-object-outline"
            },
            {
              "path": "/modules/visualisation/overlays/add-overlay",
              "deprecated": true,
              "name": "Add overlay",
              "shortDescription": "DEPRECATED - Please use individual overlay modules (e.",
              "fullDescription": "DEPRECATED - Please use individual overlay modules (e.g. \"Add labels\", \"Add object outline\", etc.).<br><br>Adds an overlay to the specified input image which can represent each specified input object.  This module can render many different types of overlay; options include: All points, Arrows, Centroid, Label only, Outline, Position measurements, Tracks",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Objects to represent as overlays."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Label mode",
                  "description": "Controls what information each label displays:<br><ul><li>\"ID\" The ID number of the object.<\/li><li>\"Measurement value\" A measurement associated with the object.  The measurement is selected using the \"Measurement for label\" parameter.<\/li><li>\"Parent ID\" The ID number of a parent of the object.  The parent object is selected using the \"Parent object for label\" parameter.<\/li><li>\"Parent measurement value\" A measurement associated with a parent of the object.  The measurement is selected using the \"Measurement for label\" parameter and the parent object with the \"Parent object for label\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Decimal places",
                  "description": "Number of decimal places to use when displaying numeric values."
                },
                {
                  "name": "Use scientific notation",
                  "description": "When enabled, numeric values will be displayed in the format <i>1.23E-3<\/i>.  Otherwise, the same value would appear as <i>0.00123<\/i>."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the text label."
                },
                {
                  "name": "Parent object for label",
                  "description": "If \"Label mode\" is set to either \"Parent ID\" or \"Parent measurement value\", these are the parent objects which will be used.  These objects will be parents of the input objects."
                },
                {
                  "name": "Measurement for label",
                  "description": "If \"Label mode\" is set to either \"Measurement value\" or \"Parent measurement value\", these are the measurements which will be used."
                },
                {
                  "name": "Position mode",
                  "description": "Controls the sort of overlay to be rendered:<br><ul><li>\"All points\" All points in each object are rendered as small circles.<\/li><li>\"Arrows\" Each object is represented by an arrow.  The size, colour and orientation of each arrow can be fixed or based on a measurement value..<\/li><li>\"Centroid\" Each object is represented by a single small circle positioned at the centre of the object (mean XYZ location).<\/li><li>\"Label only\" Displays a text string at the centroid location of each object.  Text can include object ID numbers, measurements or similar values for parent objects.<\/li><li>\"Outline\" Each object is represented by an outline.<\/li><li>\"Position measurements\" Each object is represented by a single circle positioned at the XYZ location specified by three position measurements.<\/li><li>\"Tracks\" The trajectory of a track (time-linked objects) is rendered for all track objects.  The line passes between the centre of each timepoint instance of that track and appears as the object moves )(i.e. it only shows the trajectory from previous frames).<\/li><\/ul>"
                },
                {
                  "name": "Arrow orientation mode",
                  "description": "Source for arrow orientation values:<br><ul><li>\"Measurement\" Orientation of arrows will be based on the measurement specified by the parameter \"Measurement for orientation\" for each object.<\/li><li>\"Parent measurement\" Orientation of arrows will be based on the measurement specified by the parameter \"Measurement for orientation\" taken from a parent of each object.  The parent object providing this measurement is specified by the parameter \"Parent object for orientation\".<\/li><\/ul>"
                },
                {
                  "name": "Parent object for orientation",
                  "description": "Parent objects providing the measurements on which the orientation of the arrows are based."
                },
                {
                  "name": "Measurement for orientation",
                  "description": "Measurement that defines the orientation of each arrow.  Measurements should be supplied in degree units."
                },
                {
                  "name": "Arrow length mode",
                  "description": "Method for determining the length of arrows:<br><ul><li>\"Fixed value\" All arrows are the same length.  Length is controlled by the \"Length value (px)\" parameter.<\/li><li>\"Measurement\" Arrow length is proportional to the measurement value specified by the \"Measurement for length\" parameter.  Absolute arrow lengths are adjusted by the \"Arrow length scale\" multiplication factor.<\/li><li>\"Parent measurement\" Arrow length is proportional to a parent object measurement value.  The parent is specified by the \"Parent object for length\" parameter and the measurement value by \"Measurement for length\".  Absolute arrow lengths are adjusted by the \"Arrow length scale\" multiplication factor.<\/li><\/ul>"
                },
                {
                  "name": "Length value (px)",
                  "description": "Fixed value specifying the length of all arrows in pixel units."
                },
                {
                  "name": "Parent object for length",
                  "description": "Parent objects from which the arrow length measurements will be taken."
                },
                {
                  "name": "Measurement for length",
                  "description": "Measurement value that will be used to control the arrow length.  This value is adjusted using the \"Arrow length scale\" muliplication factor."
                },
                {
                  "name": "Arrow length scale",
                  "description": "Measurement values will be multiplied by this value prior to being used to control the arrow length.  Each arrow will be <i>MeasurementValue*LengthScale<\/i> pixels long."
                },
                {
                  "name": "Head size",
                  "description": "Size of the arrow head.  This should be an integer between 0 and 30, where 0 is the smallest possible head and 30 is the largest."
                },
                {
                  "name": "X-position measurement",
                  "description": "Object measurement specifying the X-position of the overlay marker.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Y-position measurement",
                  "description": "Object measurement specifying the Y-position of the overlay marker.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Z-position measurement",
                  "description": "Object measurement specifying the Z-position (slice) of the overlay marker.  Measurement value must be specified in slice units."
                },
                {
                  "name": "Use radius measurement",
                  "description": "When selected, the radius of the overlay marker circle is controlled by the measurement specified by \"Measurement for radius\".  When not selected, point is represented by a single spot of fixed size."
                },
                {
                  "name": "Measurement for radius",
                  "description": "Object measurement use to specify the radius of the overlay marker circle.  Measurement value must be specified in pixel units."
                },
                {
                  "name": "Colour mode",
                  "description": "Method for determining colour of each object's corresponding overlay:<br><ul><li>\"ID\" Overlay colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>value = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Overlay colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Overlay colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>value = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Overlay colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Random colour\" Overlay colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Overlay colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour of the overlay when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour of the overlay based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Spot objects",
                  "description": "Objects present in each frame of this track.  These are children of the \"Input objects\" and provide the coordinate information for each frame."
                },
                {
                  "name": "Limit track history",
                  "description": "When enabled, segments of a track will only be displayed for a finite number of frames after the timepoint they correspond to.  This gives the effect of a moving tail behind the object and can be use to prevent the overlay image becoming too cluttered for long/dense videos.  The duration of the track history is specified by the \"Track history (frames)\" parameter."
                },
                {
                  "name": "Track history (frames)",
                  "description": "Number of frames a track segment will be displayed for after the timepoint to which it corresponds."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-overlay"
            },
            {
              "path": "/modules/visualisation/overlays/add-relationship-connection",
              "deprecated": false,
              "name": "Add relationship connection",
              "shortDescription": "Adds a series of line overlays, representing various relationship scenarios.",
              "fullDescription": "Adds a series of line overlays, representing various relationship scenarios.  Lines extend between centroids of two objects.  Depicted relationships can be between mutual child of the same parent object, between partner objects of between children and parents.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Line mode",
                  "description": "Controls what object-object relationships will be represented by the lines.  In all cases, lines are drawn between object centroids, although the line start and end points can be offset along the line when the \"Offset by measurement\" setting is selected:<br><br>- \"Between children\" Draw lines between all objects in two child object sets of a common parent object (\"Parent objects\").  This will result in all permutations of lines between one child object set and the other  The child object sets are selected with the \"Child objects 1\" and \"Child objects 2\" parameters.<br><br>- \"Between partners\" Draw lines between all partner objects.  The two partner object sets are selected using the \"Partner objects 1\" and \"Partner objects 2\" parameters.<br><br>- \"Parent to child\" Draw lines between parent objects and all their children.  Parent objects are selected using the \"Parent objects\" parameter and children using the \"Child objects 1\" parameter..<br>"
                },
                {
                  "name": "Parent objects",
                  "description": "Used to select the parent object of the two child object sets when in \"Between children\" mode, or to select the parent objects in \"Parent to child\" mode."
                },
                {
                  "name": "Child objects 1",
                  "description": "Selects the first child objects set when in \"Between children\" mode, or the (only) child set when in \"Parent to child\" mode."
                },
                {
                  "name": "Child objects 2",
                  "description": "Selects the second child objects set when in \"Between children\" mode."
                },
                {
                  "name": "Partner objects 1",
                  "description": "Selects the first partner objects set when in \"Between partners\" mode."
                },
                {
                  "name": "Partner objects 2",
                  "description": "Selects the second partner objects set when in \"Between partners\" mode."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Render mode",
                  "description": "Controls how the line is displayed between the centroids of the relevant two objects:<br><br>- \"Full line\" Draws a complete line between the two centroids (unless \"Offset by measurement\" is selected, in which case the line won't necessarily begin at the centroid).<br><br>- \"Half line\" Draws a line between the first object (either \"Child objects 1\", \"Partner objects 1\" or \"Parent objects\" when in \"Between children\", \"Between partners\" or \"Parent to child\" modes, respectively) and the mid-point between the two relevant objects.  Any offsets applied when \"Offset by measurement\" is selected still apply.<br><br>- \"Midpoint dot\" Draws a dot half way between the two relevant objects (no line is drawn).<br>"
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Point size",
                  "description": "Size of each overlay marker.  Choices are: Tiny, Small, Medium, Large, Extra large."
                },
                {
                  "name": "Point type",
                  "description": "Type of overlay marker used to represent each object.  Choices are: Circle, Cross, Dot, Hybrid."
                },
                {
                  "name": "Offset by measurement",
                  "description": "When selected, the lines at either end can start a fraction of the way between the two relevant object centroids.  Separate offsets are applied at each end, with measurements providing the offset values selected using the \"Measurement name 1\" and \"Measurement name 2\" parameters.  For example.  This is useful when it is preferable to not have the line extend all the way between centroids."
                },
                {
                  "name": "Measurement name 1",
                  "description": "Object measurement specifying offset to be applied to the line start point.  Offsets are fractional values, specifying the proportion of the line to ignore.  For example, dual offsets of 0.25 will result in a line half the usual length."
                },
                {
                  "name": "Measurement name 2",
                  "description": "Object measurement specifying offset to be applied to the line end point.  Offsets are fractional values, specifying the proportion of the line to ignore.  For example, dual offsets of 0.25 will result in a line half the usual length."
                },
                {
                  "name": "Render in all frames",
                  "description": "Display the overlay elements in all frames (time axis) of the input image stack, irrespective of whether the object was present in that frame."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-relationship-connection"
            },
            {
              "path": "/modules/visualisation/overlays/add-text",
              "deprecated": false,
              "name": "Add text",
              "shortDescription": "Adds an overlay to the specified input image showing a fixed text label.",
              "fullDescription": "Adds an overlay to the specified input image showing a fixed text label.  Slice and frame indices can be dynamically inserted into the text using keywords.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Apply to input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Text",
                  "description": "Fixed text to be displayed.  The current slice and frame numbers can be inserted using \"D{SLICE}\" and \"D{FRAME}\".  Similarly, it's possible to insert the elapsed frame time in the form \"T{HH:mm:ss.SSS}\" (where this example would give hours:minutes:seconds.millis).  The full description of supported time values can be found <a href=\"https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html\">here<\/a>."
                },
                {
                  "name": "X-position",
                  "description": "Horizontal location of the text to be displayed.  Specified in pixel units relative to the left of the image (x=0)."
                },
                {
                  "name": "Y-position",
                  "description": "Vertical location of the text to be displayed.  Specified in pixel units relative to the top of the image (y=0)."
                },
                {
                  "name": "Z-range",
                  "description": "Z-slices on which to display the text.  This is specified as a comma-separated list of slice indices.  The keyword \"end\" is used to denote the final slice in the stack and will be interpreted automatically.  Accepted formats are \"[VALUE]\" for a single index, \"[RANGE START]-[RANGE END]\" for a complete range of indices and \"[RANGE START]-[RANGE-END]-[INTERVAL]\" for indices evenly spaced at the specified interval."
                },
                {
                  "name": "Frame-range",
                  "description": "Frames on which to display the text.  This is specified as a comma-separated list of frame indices.  The keyword \"end\" is used to denote the final frame in the stack and will be interpreted automatically.  Accepted formats are \"[VALUE]\" for a single index, \"[RANGE START]-[RANGE END]\" for a complete range of indices and \"[RANGE START]-[RANGE-END]-[INTERVAL]\" for indices evenly spaced at the specified interval."
                },
                {
                  "name": "Centre text",
                  "description": "When selected, text will be centred on the specified XY coordinate.  Otherwise, text will be based with its top-left corner at the specified coordinate."
                },
                {
                  "name": "Label size",
                  "description": "Font size of the text label."
                },
                {
                  "name": "Label colour",
                  "description": "Colour of the text label.  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                }
              ],
              "slug": "add-text"
            },
            {
              "path": "/modules/visualisation/overlays/add-tracks",
              "deprecated": false,
              "name": "Add tracks",
              "shortDescription": "Adds an overlay to the specified input image showing the path of each track object.",
              "fullDescription": "Adds an overlay to the specified input image showing the path of each track object.  The line is drawn between object centroids.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image onto which overlay will be rendered.  Input image will only be updated if \"Apply to input image\" is enabled, otherwise the image containing the overlay will be stored as a new image with name specified by \"Output image\"."
                },
                {
                  "name": "Input objects",
                  "description": "Track objects to render in the overlay.  Track objects themselves don't contain any coordinate information, they simply act as links between the different \"Spot objects\" children in each frame of the track."
                },
                {
                  "name": "Spot objects",
                  "description": "Objects present in each frame of this track.  These are children of the \"Input objects\" and provide the coordinate information for each frame."
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (added overlay elements) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                },
                {
                  "name": "Limit track history",
                  "description": "When enabled, segments of a track will only be displayed for a finite number of frames after the timepoint they correspond to.  This gives the effect of a moving tail behind the object and can be use to prevent the overlay image becoming too cluttered for long/dense videos.  The duration of the track history is specified by the \"Track history (frames)\" parameter."
                },
                {
                  "name": "Track history (frames)",
                  "description": "Number of frames a track segment will be displayed for after the timepoint to which it corresponds."
                },
                {
                  "name": "Line width",
                  "description": "Width of the rendered lines.  Specified in pixel units."
                },
                {
                  "name": "Enable multithreading",
                  "description": "Process multiple overlay elements simultaneously.  This can provide a speed improvement when working on a computer with a multi-core CPU."
                }
              ],
              "slug": "add-tracks"
            },
            {
              "path": "/modules/visualisation/overlays/clear-overlay",
              "deprecated": false,
              "name": "Clear overlay",
              "shortDescription": "Removes any overlay elements from specified image.",
              "fullDescription": "Removes any overlay elements from specified image.",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Colour mode",
                  "description": "Method for assigning colour of each object:<br><ul><li>\"Child count\" Colour is determined by the number of children each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest children is shown in red and the object with the most, in cyan.  Objects without any children are always shown in red.  Child objects used for counting are selected with the \"Child objects for colour\" parameter.<\/li><li>\"ID\" Colour is quasi-randomly selected based on the ID number of the object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.<\/li><li>\"Measurement value\" Colour is determined by a measurement value.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects missing the relevant measurement  are always shown in red.  The measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Parent ID\" Colour is quasi-randomly selected based on the ID number of a parent of this object.  The colour used for a specific ID number will always be the same and is calculated using the equation <i>hue = (ID * 1048576 % 255) / 255<\/i>.  The parent object is selected with the \"Parent object for colour\" parameter.<\/li><li>\"Parent measurement value\" Colour is determined by a measurement value of a parent of this object.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the smallest measurement is shown in red and the object with the largest, in cyan.  Objects either missing the relevant measurement or without the relevant parent are always shown in red.  The parent object is selected with the \"Parent object for colour\" parameter and the measurement value is selected with the \"Measurement for colour\" parameter.<\/li><li>\"Partner count\"  Colour is determined by the number of partners each object has.  Colour range runs across the first half of the visible spectrum (i.e. red to cyan) and is maximised, so the object with the fewest partners is shown in red and the object with the most, in cyan.  Objects without any partners are always shown in red.  Partner objects used for counting are selected with the \"Partner objects for colour\" parameter.<\/li><li>\"Random colour\" Colour is randomly selected for each object.  Unlike the \"ID\" option, the colours generated here will be different for each evaluation of the module.<\/li><li>\"Single colour\" (default option) Colour is fixed to one of a predetermined list of colours.  All objects  will be assigned the same overlay colour.  The colour is chosen using the \"Single colour\" parameter.<\/li><\/ul>"
                },
                {
                  "name": "Colour map",
                  "description": "Colourmap used for colour gradients.  This parameter is used if \"Colour mode\" is set to any mode which yields a range of colours (e.g. measurements or IDs).  Choices are: Black fire, Ice, Jet, Physics, Random, Random (vibrant), Spectrum, Thermal."
                },
                {
                  "name": "Single colour",
                  "description": "Colour for all object overlays to be rendered using.  This parameter is used if \"Colour mode\" is set to \"Single colour\".  Choices are: White, Black, Red, Orange, Yellow, Green, Cyan, Blue, Violet, Magenta."
                },
                {
                  "name": "Child objects for colour",
                  "description": "Object collection used to determine the colour based on number of children per object when \"Colour mode\" is set to \"Child count\".  These objects will be children of the input objects."
                },
                {
                  "name": "Measurement for colour",
                  "description": "Measurement used to determine the colour when \"Colour mode\" is set to either \"Measurement value\" or \"Parent measurement value\"."
                },
                {
                  "name": "Parent object for colour",
                  "description": "Object collection used to determine the colour based on either the ID or measurement value  of a parent object when \"Colour mode\" is set to either  \"Parent ID\" or \"Parent measurement value\".  These objects will be parents of the input objects."
                },
                {
                  "name": "Partner objects for colour",
                  "description": "Object collection used to determine the colour based on number of partners per object when \"Colour mode\" is set to \"Partner count\".  These objects will be partners of the input objects."
                },
                {
                  "name": "Opacity (%)",
                  "description": "Opacity of the overlay to be rendered.  This is a value between 0 (totally transparent) and 100 (totally opaque)."
                },
                {
                  "name": "Range minimum mode",
                  "description": "Controls whether the minimum range for displayed colours is set automatically to the minimum available value (e.g. the smallest measurement being rendered), or whether it is defined manually by the \"Minimum value\" parameter."
                },
                {
                  "name": "Minimum value",
                  "description": "When \"Range minimum mode\" is set to \"Manual\", this is the minimum value that will be displayed as a unique colour.  All values smaller than this will be displayed with the same colour."
                },
                {
                  "name": "Range maximum mode",
                  "description": "Controls whether the maximum range for displayed colours is set automatically to the maximum available value (e.g. the largest measurement being rendered), or whether it is defined manually by the \"Maximum value\" parameter."
                },
                {
                  "name": "Maximum value",
                  "description": "When \"Range maximum mode\" is set to \"Manual\", this is the maximum value that will be displayed as a unique colour.  All values larger than this will be displayed with the same colour."
                },
                {
                  "name": "Input image",
                  "description": "Image for which overlay will be cleared"
                },
                {
                  "name": "Apply to input image",
                  "description": "Determines if the modifications made to the input image (removed overlay) will be applied to that image or directed to a new image.  When selected, the input image will be updated."
                },
                {
                  "name": "Add output image to workspace",
                  "description": "If the modifications (overlay) aren't being applied directly to the input image, this control will determine if a separate image containing the overlay should be saved to the workspace."
                },
                {
                  "name": "Output image",
                  "description": "The name of the new image to be saved to the workspace (if not applying the changes directly to the input image)."
                }
              ],
              "slug": "clear-overlay"
            }
          ]
        },
        {
          "path": "/modules/visualisation/plots",
          "name": "Plots",
          "description": "Modules creating measurement plots",
          "slug": "plots",
          "subCategories": [],
          "modules": [
            {
              "path": "/modules/visualisation/plots/plot-measurement-timeseries",
              "deprecated": false,
              "name": "Plot measurement timeseries",
              "shortDescription": "",
              "fullDescription": "",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input tracks",
                  "description": ""
                },
                {
                  "name": "Input objects",
                  "description": ""
                },
                {
                  "name": "Output image",
                  "description": ""
                },
                {
                  "name": "Add plot",
                  "subParameters": [
                    {
                      "name": "Measurement",
                      "description": ""
                    },
                    {
                      "name": "Plot colour",
                      "description": ""
                    },
                    {
                      "name": "Plot type",
                      "description": ""
                    },
                    {
                      "name": "Line width",
                      "description": ""
                    }
                  ],
                  "description": ""
                },
                {
                  "name": "X-range mode",
                  "description": ""
                },
                {
                  "name": "X-range min",
                  "description": ""
                },
                {
                  "name": "X-range max",
                  "description": ""
                },
                {
                  "name": "Y-range mode",
                  "description": ""
                },
                {
                  "name": "Y-range min",
                  "description": ""
                },
                {
                  "name": "Y-range max",
                  "description": ""
                },
                {
                  "name": "Y axis label",
                  "description": ""
                },
                {
                  "name": "Add object ID",
                  "description": ""
                },
                {
                  "name": "Object ID colour",
                  "description": ""
                }
              ],
              "slug": "plot-measurement-timeseries"
            },
            {
              "path": "/modules/visualisation/plots/plot-measurements-as-scatter",
              "deprecated": false,
              "name": "Plot measurements as scatter",
              "shortDescription": "Creates an ImageJ scatter plot of two measurements associated with specified objects.",
              "fullDescription": "Creates an ImageJ scatter plot of two measurements associated with specified objects.  A third measurement can be encoded as point colour.  The output plot can either be displayed immediately in an interactive ImageJ plotting window or stored as an image to the MIA workspace (allowing it to subsequently be saved to file).",
              "version": "1.0.0",
              "parameters": [
                {
                  "name": "Input objects",
                  "description": "Input object collection for which object-associated measurements will be plotted."
                },
                {
                  "name": "Output image",
                  "description": "Output plot image which will be saved to the workspace with this name."
                },
                {
                  "name": "First measurement (X)",
                  "description": "Measurement associated with the input objects which will be plotted along the x-axis."
                },
                {
                  "name": "Second measurement (Y)",
                  "description": "Measurement associated with the input objects which will be plotted along the y-axis."
                },
                {
                  "name": "Add third measurement as colour",
                  "description": "When selected, a third measurement can be represented as the plot marker colour.  This colour will vary according to the colourmap set with the \"Colourmap\" parameter"
                },
                {
                  "name": "Third measurement (Colour)",
                  "description": "If \"Add third measurement as colour\" is selected, this measurement associated with the input objects will determine the plot marker colour."
                },
                {
                  "name": "Colourmap",
                  "description": "If \"Add third measurement as colour\" is selected, this is the colourmap that will control how marker colours vary in response to the magnitude of their values."
                },
                {
                  "name": "Show as interactive plot",
                  "description": "When selected, and if displaying module output in realtime (\"Show output\" button selected), the plot will be displayed as an interactive ImageJ plot (editable rendering).  Otherwise, the standard image output will be displayed (i.e. the same image added to the workspace)."
                }
              ],
              "slug": "plot-measurements-as-scatter"
            }
          ]
        }
      ],
      "modules": [
        {
          "path": "/modules/visualisation/create-measurement-map",
          "deprecated": false,
          "name": "Create measurement map",
          "shortDescription": "Creates a map of object measurements.",
          "fullDescription": "Creates a map of object measurements.  Each pixel of the output map is a combination of all object measurements at that location.  Measurements can be taken from the input objects themselves or from associated parent objects.  Multiple Z-slices and/or timepoints can be combined into a single slice.  The outmap is blurred with a Gaussian function to show smooth transitions between regions.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input objects",
              "description": "Objects from workspace for which the measurement map will be created."
            },
            {
              "name": "Output image",
              "description": "Output image showing the measurement map which will be output to the workspace.  The value at each location will be based on the measurements of nearby objects.  Any pixels too far from an object (this distance will be controlled by the blur range, \"Range\") will be assigned NaN (not a number) values."
            },
            {
              "name": "Measurement mode",
              "description": "Controls whether the measurements being rendered for each object are measurements associated with that object (\"Measurement\") or are measurements associated with a parent object (\"Parent object measurement\")."
            },
            {
              "name": "Statistic",
              "description": "Controls the statistic that will be used to combine multiple object measurements at each location.  For example, if two objects overlap at a specific location (occurs more frequently when \"Merge slices\" or \"Merge time\" are selected), the value at that location could be an average (or any other listed statistic) of the two."
            },
            {
              "name": "Parent object",
              "description": "If \"Measurement mode\" is set to \"Parent object measurement\", this is the parent object collection from which the measurement (specified by \"Measurement\") will be taken."
            },
            {
              "name": "Measurement",
              "description": "Controls the measurement for each object that will be rendered on the measurement map.  Depending on the setting for \"Measurement mode\", this can either be a measurement associated with the input object or with a parent of that object."
            },
            {
              "name": "Range",
              "description": "The measurement map can be blurred using a Gaussian distribution.  This is the sigma value for that blurring function."
            },
            {
              "name": "Merge slices",
              "description": "When selected, all measurements from different slices are combined into a single slice."
            },
            {
              "name": "Merge time",
              "description": "When selected, all measurements from different timepoints are combined into a single timepoint."
            }
          ],
          "slug": "create-measurement-map"
        },
        {
          "path": "/modules/visualisation/create-orthogonal-view",
          "deprecated": false,
          "name": "Create orthogonal view",
          "shortDescription": "Create a montage image showing orthogonal views of a specified input image from the workspace.",
          "fullDescription": "Create a montage image showing orthogonal views of a specified input image from the workspace.  Orthogonal views are taken in the XY, XZ and YZ planes and all share a common coordinate.  This common coordinate can be the centre of the image or the centre of the largest object in a specified object collection.  The output orthogonal view is stored in the workspace as a separate image.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Input image",
              "description": "Image from workspace for which orthogonal view will be generated.  This image will not be affected by the process."
            },
            {
              "name": "Output image",
              "description": "Output orthogonal image showing orthogonal views in XY, XZ and YZ planes.  The output image will be formed from three panels, showing the orthogonal views, with white pixels elsewhere.  This image will be stored in the workspace using this name."
            },
            {
              "name": "Position mode",
              "description": "Controls how the orthogonal views are generated:<br><ul><li>\"Image centre\" Orthogonal views are extracted as single slices from the input image.  The views share a common coordinate, coincident with the centre of the image.<\/li><li>\"Centroid of largest object\" Orthogonal views are extracted as single slices from the input image.  The views share a common coordinate, coincident with the centre of the largest object from the collection specified by \"Input objects\".<\/li><\/ul>"
            },
            {
              "name": "Input objects",
              "description": "If \"Position mode\" is set to \"Centroid of largest object\", the orthogonal views will be positioned coincident with the centroid of the largest object from this collection."
            }
          ],
          "slug": "create-orthogonal-view"
        },
        {
          "path": "/modules/visualisation/show-image",
          "deprecated": false,
          "name": "Show image",
          "shortDescription": "Display any image held in the current workspace.",
          "fullDescription": "Display any image held in the current workspace.  Images are displayed using the standard ImageJ image window, so can be accessed/manipulated by any ImageJ/Fiji feature.  Displayed images are duplicates of the image stored in the workspace, so modification of a displayed image won't alter the original.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Display image",
              "description": "Image to display."
            },
            {
              "name": "Title mode",
              "description": "Select what title the image window should have.<br><br>- \"Image name\" Set the image window title to the name of the image.<br><br>- \"Filename\" Set the image window title to the filename of the root file for this workspace (i.e. the file set in \"Input control\".<br><br>- \"Image and filename\" Set the image window title to a composite of the filename of the root file for this workspace and the name of the image."
            },
            {
              "name": "Quick normalisation",
              "description": "Before displaying the image, apply quick normalisation to improve contrast.  The minimum and maximum displayed intensities are simply set to the minimum and maximum pixel intensities contained within the image stack."
            },
            {
              "name": "Channel mode",
              "description": "Select whether multi-channel images should be displayed as composites (show all channels overlaid) or individually (the displayed channel is controlled by the \"C\" slider at the bottom of the image window)."
            }
          ],
          "slug": "show-image"
        },
        {
          "path": "/modules/visualisation/show-image-measurements",
          "deprecated": false,
          "name": "Show image measurements",
          "shortDescription": "Displays all measurements associated with an image.",
          "fullDescription": "Displays all measurements associated with an image.",
          "version": "1.0.0",
          "parameters": [{
            "name": "Input image",
            "description": "Image to display all measurements for."
          }],
          "slug": "show-image-measurements"
        },
        {
          "path": "/modules/visualisation/show-metadata",
          "deprecated": false,
          "name": "Show metadata",
          "shortDescription": "Displays all measurements associated with an image.",
          "fullDescription": "Displays all measurements associated with an image.",
          "version": "1.0.0",
          "parameters": [],
          "slug": "show-metadata"
        },
        {
          "path": "/modules/visualisation/show-object-measurements",
          "deprecated": false,
          "name": "Show object measurements",
          "shortDescription": "Displays all measurements associated with all objects of the specified object collection.",
          "fullDescription": "Displays all measurements associated with all objects of the specified object collection.",
          "version": "1.0.0",
          "parameters": [{
            "name": "Input objects",
            "description": "Object collection to display all measurements for."
          }],
          "slug": "show-object-measurements"
        }
      ]
    },
    {
      "path": "/modules/workflow",
      "name": "Workflow",
      "description": "Modules capable of controlling the order of module execution.  For example, skipping specific modules if certain criteria are met.",
      "slug": "workflow",
      "subCategories": [],
      "modules": [
        {
          "path": "/modules/workflow/add-pause",
          "deprecated": false,
          "name": "Add pause",
          "shortDescription": "Pauses workflow execution and displays an option dialog to continue or quit.",
          "fullDescription": "Pauses workflow execution and displays an option dialog to continue or quit.  Optionally, an image from the workspace can also be displayed.  An example usage would be during parameter optimisation, where subsequent elements of the analysis only want executing if the first steps are deemed successful (and this can't be automatically determined).",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Show image",
              "description": "When selected, an image from the workspace can be automatically displayed when this module executes."
            },
            {
              "name": "Input image",
              "description": "If \"Show image\" is selected, this image will be displayed when the module executes."
            }
          ],
          "slug": "add-pause"
        },
        {
          "path": "/modules/workflow/fixed-text-condition",
          "deprecated": false,
          "name": "Fixed text condition",
          "shortDescription": "Implement variable workflow handling outcomes based on comparison of a fixed text value against a series of fixed conditions.",
          "fullDescription": "Implement variable workflow handling outcomes based on comparison of a fixed text value against a series of fixed conditions.  If the text test value matches any of the conditions the workflow handling outcome associated with that condition will be implemented.  Outcomes can include termination of the analysis and redirection of the active module to another part of the workflow.  Redirection allows parts of the analysis to be looped, or sections of the workflow to be skipped.<br><br>An example usage case for fixed text conditions is implementing the same behaviour at multiple parts of the workflow without having to control them individually.  This can be achieved using a global variable (see \"Global variables\" module).  The global variable could be specified once, early on in the analysis, then used as \"Test value\" in this module.  As such, it's possible to only specify the value once, but refer to it in multiple \"Fixed text condition\" modules.  Note: The global variables module allows variables to be user-selected from a drop-down list, negating risk of mis-typing parameter names that will be compared in this module.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Continuation mode",
              "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\"The analysis will stop evaluating any further modules.<\/li><\/ul>"
            },
            {
              "name": "Redirect module",
              "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
            },
            {
              "name": "Show redirect message",
              "description": "Controls if a message should be displayed in the log if redirection occurs."
            },
            {
              "name": "Redirect message",
              "description": "Message to display if redirection occurs."
            },
            {
              "name": "Show termination warning",
              "description": "When selected, a warning will be displayed in the terminal if terminating a workflow early."
            },
            {
              "name": "Export terminated workspaces",
              "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
            },
            {
              "name": "Remove images from workspace",
              "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Remove objects from workspace",
              "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Test value",
              "description": "Text value that will tested.  If this matches any of the reference values listed within this module the relevant workflow operation (e.g. termination/redirection) will be implemented.  This text value could be a global variable."
            },
            {
              "name": "Add condition",
              "subParameters": [
                {
                  "name": "Reference value",
                  "description": "Value \"Test value\" will be compared against.  If the two values match, the associated handling outcome of this condition will be implemented."
                },
                {
                  "name": "Continuation mode",
                  "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\"The analysis will stop evaluating any further modules.<\/li><\/ul>"
                },
                {
                  "name": "Redirect module",
                  "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
                },
                {
                  "name": "Show redirect message",
                  "description": "Controls if a message should be displayed in the log if redirection occurs."
                },
                {
                  "name": "Redirect message",
                  "description": "Message to display if redirection occurs."
                },
                {
                  "name": "Show termination warning",
                  "description": "When selected, a warning will be displayed in the terminal if terminating a workflow early."
                },
                {
                  "name": "Export terminated workspaces",
                  "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
                },
                {
                  "name": "Remove images from workspace",
                  "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
                },
                {
                  "name": "Remove objects from workspace",
                  "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
                }
              ],
              "description": "Add another condition that \"Test value\" can be compared against.  Each condition can have its own handling outcome (e.g. termination/redirection)."
            }
          ],
          "slug": "fixed-text-condition"
        },
        {
          "path": "/modules/workflow/gui-condition",
          "deprecated": true,
          "name": "GUI condition",
          "shortDescription": "DEPRECATED: Please use \"Global variables\" module (in \"Choice\" mode) in conjunction with \"Fixed text condition\", which offer equivalent functionality.",
          "fullDescription": "DEPRECATED: Please use \"Global variables\" module (in \"Choice\" mode) in conjunction with \"Fixed text condition\", which offer equivalent functionality.<br><br>Implement variable workflow handling outcomes based on a user-selectable drop-down list of choices.  Each choice has a unique workflow outcome, which can include termination of the analysis and redirection of the active module to another part of the workflow.  Redirection allows parts of the analysis workflow to be skipped.<br><br>An example usage case for GUI conditions is providing a drop-down box on the basic control view.  With this simple control, the user can execute different blocks of the workflow without having to fundamentally understand how they are assembled.",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Continuation mode",
              "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\"The analysis will stop evaluating any further modules.<\/li><\/ul>"
            },
            {
              "name": "Redirect module",
              "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
            },
            {
              "name": "Show redirect message",
              "description": "Controls if a message should be displayed in the log if redirection occurs."
            },
            {
              "name": "Redirect message",
              "description": "Message to display if redirection occurs."
            },
            {
              "name": "Show termination warning",
              "description": "When selected, a warning will be displayed in the terminal if terminating a workflow early."
            },
            {
              "name": "Export terminated workspaces",
              "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
            },
            {
              "name": "Remove images from workspace",
              "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Remove objects from workspace",
              "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Choice",
              "description": "Currently-selected choice from the available set (all choices added via \"Add choice\" option).  The relevant workflow operation (e.g. termination/redirection) will be implemented for the selected condition.  This control can be made visible in the processing view, so users can select between a set of pre-determined outcomes."
            },
            {
              "name": "Store as metadata item",
              "description": "When selected, the selected choice will be stored as a metadata item.  This allows it to be exported to the final spreadsheet."
            },
            {
              "name": "Metadata name",
              "description": "Name for selected choice to be stored as metadata using.  The choice will be accessible via this metadata name in subsequent modules and can be exported to the final spreadsheet."
            },
            {
              "name": "Add choice",
              "subParameters": [
                {
                  "name": "Choice name",
                  "description": "Name that this choice will appear as in the \"Choice\" drop-down menu."
                },
                {
                  "name": "Continuation mode",
                  "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\"The analysis will stop evaluating any further modules.<\/li><\/ul>"
                },
                {
                  "name": "Redirect module",
                  "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
                },
                {
                  "name": "Show redirect message",
                  "description": "Controls if a message should be displayed in the log if redirection occurs."
                },
                {
                  "name": "Redirect message",
                  "description": "Message to display if redirection occurs."
                },
                {
                  "name": "Show termination warning",
                  "description": "When selected, a warning will be displayed in the terminal if terminating a workflow early."
                },
                {
                  "name": "Export terminated workspaces",
                  "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
                },
                {
                  "name": "Remove images from workspace",
                  "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
                },
                {
                  "name": "Remove objects from workspace",
                  "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
                },
                {
                  "name": "Choice",
                  "description": "Currently-selected choice from the available set (all choices added via \"Add choice\" option).  The relevant workflow operation (e.g. termination/redirection) will be implemented for the selected condition.  This control can be made visible in the processing view, so users can select between a set of pre-determined outcomes."
                },
                {
                  "name": "Store as metadata item",
                  "description": "When selected, the selected choice will be stored as a metadata item.  This allows it to be exported to the final spreadsheet."
                },
                {
                  "name": "Metadata name",
                  "description": "Name for selected choice to be stored as metadata using.  The choice will be accessible via this metadata name in subsequent modules and can be exported to the final spreadsheet."
                }
              ],
              "description": "Add another condition that \"Choice\" can select from.  Each choice can have its own handling outcome (e.g. termination/redirection)."
            }
          ],
          "slug": "gui-condition"
        },
        {
          "path": "/modules/workflow/module-is-enabled-condition",
          "deprecated": false,
          "name": "Module is enabled condition",
          "shortDescription": "Implement workflow handling outcome based on whether another module is enabled or disabled.",
          "fullDescription": "Implement workflow handling outcome based on whether another module is enabled or disabled.  Outcomes can include termination of the analysis and redirection of the active module to another part of the workflow.  Redirection allows parts of the analysis to skipped.<br><br>Note: This only applies to modules explictly enabled/disabled by the user.  It does not apply to modules that are inactive due to invalid parameters (modules highlighted in red in the module list).",
          "version": "1.0.0",
          "parameters": [
            {
              "name": "Continuation mode",
              "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\"The analysis will stop evaluating any further modules.<\/li><\/ul>"
            },
            {
              "name": "Redirect module",
              "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
            },
            {
              "name": "Show redirect message",
              "description": "Controls if a message should be displayed in the log if redirection occurs."
            },
            {
              "name": "Redirect message",
              "description": "Message to display if redirection occurs."
            },
            {
              "name": "Show termination warning",
              "description": "When selected, a warning will be displayed in the terminal if terminating a workflow early."
            },
            {
              "name": "Export terminated workspaces",
              "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
            },
            {
              "name": "Remove images from workspace",
              "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Remove objects from workspace",
              "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Test mode",
              "description": "Controls whether the specified workflow handling outcome is applied if another module is enabled or disabled:<br><ul><li>\"Module is enabled\" Execute specified outcome if another module is enabled.<\/li><li>\"Module is not enabled\" Execute specified outcome if another module is not enabled.<\/li><\/ul>"
            },
            {
              "name": "Test module",
              "description": "Module to test the enabled/disabled state of.  This doesn't necessarily need to be a module that would execute before the current module."
            }
          ],
          "slug": "module-is-enabled-condition"
        },
        {
          "path": "/modules/workflow/workflow-handling",
          "deprecated": false,
          "name": "Workflow handling",
          "shortDescription": "Implement workflow handling outcome based on a variety of metrics (e.",
          "fullDescription": "Implement workflow handling outcome based on a variety of metrics (e.g. object counts, image measurements, metadata values).  Outcomes can include termination of the analysis and redirection of the active module to another part of the workflow.  Redirection allows parts of the analysis to skipped.",
          "version": "1.0.1",
          "parameters": [
            {
              "name": "Test mode",
              "description": "Controls what condition is being tested:<br><ul><li>\"Image measurement\" Numeric filter against a measurement (specified by \"Image measurement\") associated with an image from the workspace (specified by \"Input image\").<\/li><li>\"Metadata numeric value\" Numeric filter against a metadata value (specified by \"Metadata value\") associated with the workspace.  Metadata values are stored as text, but this filter will attempt to parse any numeric values as numbers.  Text comparison can be done using \"Metadata text value\" mode.<\/li><li>\"Metadata text value\" Text filter against a metadata value (specified by \"Metadata value\") associated with the workspace.  This filter compares for exact text matches to a reference, specified by \"Reference text value\"<\/li><li>\"File exists\" Checks if a specified file exists on the accessible computer filesystem.<\/li><li>\"File doesn't exist\" Checks if a specified file doesn't exist on the accessible computer filesystem.<\/li><li>\"Fixed value\" Numeric filter against a fixed value.<\/li><li>\"Object count\" Numeric filter against the number of objects contained in an object collection stored in the workspace (specified by \"Input objects\").<\/li><\/ul>"
            },
            {
              "name": "Input image",
              "description": "If testing against an image measurement (\"Test mode\" set to \"Image measurement\"), this is the image from which that measurement will be taken."
            },
            {
              "name": "Input objects",
              "description": "If testing against an object count (\"Test mode\" set to \"Object count\"), this is the object collection which will be counted."
            },
            {
              "name": "Numeric filter mode",
              "description": "Numeric comparison used to determine whether the test passes or fails.  Choices are: Less than, Less than or equal to, Equal to, Greater than or equal to, Greater than, Not equal to."
            },
            {
              "name": "Text filter mode",
              "description": "Text comparison used to determine whether the test passes or fails.  Choices are: Contains, Does not contain, Equal to, Not equal to."
            },
            {
              "name": "Image measurement",
              "description": "If testing against an image measurement (\"Test mode\" set to \"Image measurement\"), this is the measurement from the image (specified by \"Input image\") that will be tested."
            },
            {
              "name": "Metadata value",
              "description": "If testing against a metadata value (either text or numeric) associated with the active workspace (\"Test mode\" set to \"Metadata text value\"), this is the value that will be tested."
            },
            {
              "name": "Reference numeric value",
              "description": "If testing against a numeric value, this is the reference value against which it will be tested.  What classes as a pass or fail is determined by the parameter \"Numeric filter mode\"."
            },
            {
              "name": "Reference text value",
              "description": "If testing against a text value, this is the reference value against which it will be tested.  What classes as a pass or fail is determined by the parameter \"Text filter mode\"."
            },
            {
              "name": "Fixed value",
              "description": "If testing against a fixed numeric value (\"Test mode\" set to \"Fixed value\"), this is the value that will be tested."
            },
            {
              "name": "Generic format",
              "description": "Format for a generic filename.  Plain text can be mixed with global variables or metadata values currently stored in the workspace.  Global variables are specified using the \"V{name}\" notation, where \"name\" is the name of the variable to insert.  Similarly, metadata values are specified with the \"M{name}\" notation."
            },
            {
              "name": "Continuation mode",
              "description": "Controls what happens if the termination/redirection condition is met:<br><ul><li>\"Redirect to module\" The analysis workflow will skip to the module specified by the \"Redirect module\" parameter.  Any modules between the present module and the target module will not be evaluated.<\/li><li>\"Terminate\" The analysis will stop evaluating any further modules.<\/li><\/ul>"
            },
            {
              "name": "Redirect module",
              "description": "If the condition is met, the workflow will redirect to this module.  In doing so, it will skip evaluation of any modules between the present module and this module."
            },
            {
              "name": "Show redirect message",
              "description": "Controls if a message should be displayed in the log if redirection occurs."
            },
            {
              "name": "Redirect message",
              "description": "Message to display if redirection occurs."
            },
            {
              "name": "Message level",
              "description": "Controls the logging level in which the message will be displayed.  Warnings are enabled for users by default, but debug and message aren't."
            },
            {
              "name": "Show termination warning",
              "description": ""
            },
            {
              "name": "Export terminated workspaces",
              "description": "Controls if the workspace should still be exported to the output Excel spreadsheet if termination occurs."
            },
            {
              "name": "Remove images from workspace",
              "description": "Controls if images should be completely removed from the workspace along with any associated measurements if termination occurs."
            },
            {
              "name": "Remove objects from workspace",
              "description": "Controls if objects should be completely removed from the workspace along with any associated measurements if termination occurs."
            }
          ],
          "slug": "workflow-handling"
        }
      ]
    }
  ],
  "modules": []
}